[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "mp01",
    "section": "",
    "text": "Abstract: This report analyzes data from the National Transit Database. The report explores some of the fiscal characteristics of major US public transportation systems, such as: farebox revenues, total number of trips, total number of vehicle miles traveled, and total revenues and expenses by source. One key metric to keep in mind is the farebox recovery ratio. Farebox recovery ratio represents the proportion of operating expenses that are covered just by fares alone. The ratio is total fares/total operating expenses for said time frame. This ratio becomes important as we explore various cities, their agencies for public transportation, and their different modes of public transportation and look for the most efficient and profitable form.\n\n\nThis study uses three primary datasets: 2022 Fare Revenue, 2022 Operating Expenses, and the latest Monthly Ridership data. A note to keep in mind: because the data is reported on a lag, the 2022 version of each report was used.\nAs you will find in the code below, the three datasets were downloaded, cleaned, and merged. The fare revenue data was filtered to include only relevant columns and aggregated by the columns NTD ID, Agency Name and Mode. The expenses data was similarly filtered then aggregated by NTD ID and Mode. These two datasets were then merged into a single table titled FINANCIALS.\nThe monthly ridership data was handled separately. The Unlinked Passenger Trips (UPT) and Vehicle Revenue Miles (VRM) data was extracted and combined into new table titled USAGE. This later allows for an easy calculation of farebox recovery and views of the highest/lowest VRM and UPT across different agencies and modes.\nThe data pre-processing step was necessary to clean the data, put it in a consistent format, preparing it for further analysis.\n\n# ============================================================\n# Downloading, Cleaning, and Joining Tables\n# ============================================================\ninstall.packages(\"dplyr\", repos=c(CRAN = \"https://cran.rstudio\"))\nlibrary(dplyr)\n\n#First snippet of code, provided by professor\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n  # directory.\n  download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                destfile=\"2022_fare_revenue.xlsx\", \n                quiet=FALSE, \n                method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n  select(-`State/Parent NTD ID`, \n         -`Reporter Type`,\n         -`Reporting Module`,\n         -`TOS`,\n         -`Passenger Paid Fares`,\n         -`Organization Paid Fares`) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`) |&gt;\n  group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n           `Agency Name`,  # These are direct operated and sub-contracted \n           `Mode`) |&gt;      # of the same transit modality\n  # Not a big effect in most munis (significant DO\n  # tends to get rid of sub-contractors), but we'll sum\n  # to unify different passenger experiences\n  summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n  ungroup()\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"2022_expenses.csv\" in your project\n  # directory.\n  download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                destfile=\"2022_expenses.csv\", \n                quiet=FALSE, \n                method=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n  select(`NTD ID`, \n         `Agency`,\n         `Total`, \n         `Mode`) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n#Checking financials data\nhead(FINANCIALS)\n\n#Noticed for some modes, there is an expense value of 0\n#Counting # of rows with 0 expenses\nzero_expenses_count &lt;- FINANCIALS |&gt;\n  filter(Expenses == 0) |&gt;\n  nrow()\nprint(zero_expenses_count) #There are 8 rows with zero expenses\n\n#View rows with zero expenses and print\nzero_expenses_rows &lt;- FINANCIALS |&gt;\n  filter(Expenses == 0)\nprint(zero_expenses_rows)\n\n#Noting as a potential limitation for analysis later in the project. May choose to remove blank rows when calculating averages, etc.\n# ============================================================\n# Extracting monthly transit numbers\n# ============================================================\n#Second snippet of code, provided by professor \n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"ridership.xlsx\" in your project\n  # directory.\n  download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                destfile=\"ridership.xlsx\", \n                quiet=FALSE, \n                method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"UPT\") |&gt;\n  drop_na() |&gt;\n  mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"VRM\") |&gt;\n  drop_na() |&gt;\n  group_by(`NTD ID`, `Agency`, `UZA Name`, \n           `Mode`, `3 Mode`, month) |&gt;\n  summarize(VRM = sum(VRM)) |&gt;\n  ungroup() |&gt;\n  mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`))\n\n# ============================================================\n# Creating a table\n# ============================================================\n#Third snippet of code, provided by professor\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month=as.character(month)) |&gt; \n  DT::datatable()\n\nThe table above is a peek at the USAGE table, which comes into play for several topics of analysis later in the report.\n\n\n\nBelow you will find the renaming of column UZA Name to metro_area. This variable is used multiple times across the analysis. By creating a syntatic name, the column no longer needs to be surrounded by quotes each times it is referenced, making for easier, cleaner code.\n\n# ============================================================\n# Task 1 - Creating Syntatic Names \n# ============================================================\n#renaming UZA Name to metro_area\nUSAGE &lt;- USAGE |&gt;\n  rename(metro_area = 'UZA Name')\n\nTRIPS &lt;- TRIPS |&gt;\n  rename(metro_area = 'UZA Name')\n\nMILES &lt;- MILES |&gt;\n  rename(metro_area = 'UZA Name')\n\nnames(USAGE)\nnames(TRIPS)\nnames(MILES)\n\n\n\n\nIn the code below, you will find Mode column in the USAGE table was recoded using the case_when function. The Mode columns were initially in a shorthand abbreviation and were transformed into their full terms, which provide for more descriptive, clear labels.\n\n# ============================================================\n# Task 2 - Recoding the Mode column\n# ============================================================\n#using case_when to recode the mode column \nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"SR\" ~ \"Streetcar\", \n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"RB\" ~ \"Rapid Bus\",\n    Mode == \"MG\" ~ \"Monorail\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))  # Default for any unrecognized codes\n  \n#checking result of task 2\nhead(USAGE)\nUSAGE |&gt;\n  distinct(Mode)\n\n\n#creating an attractive summary table of cleaned up USAGE table \nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month=as.character(month)) |&gt; \n  DT::datatable()\n\n\n\n\n\nThe displayed table above is a view of a random sample of 1000 from the cleaned USAGE table. The format is interactive, allowing users to sort and search through the data, making it easily accessible and usable, for even a non-technical user.\n\n\n\n\n\n\nlibrary(scales)\n# 1. What transit agency had the most total VRM in our data set?\n#grouping by agency, summing VRM by agency & removing any NA values, arranging in descending order\ntotal_VRM_by_agency &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(Total_VRM = sum(VRM, na.rm =TRUE)) |&gt;\n  arrange(desc(Total_VRM)) \n  \n\n#formatting table using gt package\ntotal_VRM_by_agency_formatted &lt;- total_VRM_by_agency |&gt;\n  mutate(Total_VRM = comma(Total_VRM, accuracy =1)) \n\n#printing formatted table \nprint(total_VRM_by_agency_formatted)\n\n# A tibble: 677 × 2\n   Agency                                                              Total_VRM\n   &lt;chr&gt;                                                               &lt;chr&gt;    \n 1 MTA New York City Transit                                           10,832,8…\n 2 New Jersey Transit Corporation                                      5,645,52…\n 3 Los Angeles County Metropolitan Transportation Authority            4,354,01…\n 4 Washington Metropolitan Area Transit Authority                      2,821,95…\n 5 Chicago Transit Authority                                           2,806,20…\n 6 Southeastern Pennsylvania Transportation Authority                  2,672,63…\n 7 Massachusetts Bay Transportation Authority                          2,383,96…\n 8 Pace, the Suburban Bus Division of the Regional Transportation Aut… 2,379,40…\n 9 Metropolitan Transit Authority of Harris County, Texas              2,272,94…\n10 Denver Regional Transportation District                             1,991,41…\n# ℹ 667 more rows\n\n#displaying agency with highest total VRM\nhighest_VRM_agency &lt;- total_VRM_by_agency |&gt;\n  filter(Total_VRM == max(Total_VRM)) |&gt;\n  mutate(Total_VRM = comma(Total_VRM, accuracy =1)) \n  \n#printing highest_VRM_agency\nprint(highest_VRM_agency)\n\n# A tibble: 1 × 2\n  Agency                    Total_VRM     \n  &lt;chr&gt;                     &lt;chr&gt;         \n1 MTA New York City Transit 10,832,855,350\n\n\nThe VRM, or Vehicle Revenue Miles, represents the total distance traveled by vehicles in revenue service.\nThe analysis starts with aggregating the VRM values for each transit agency, grouping the data by Agency, and finally summing the total VRM for each group, or agency. Any missing values are excluded from the grouping, through the na.rm = TRUE statement. The results are then arranged in descending order, with the agency with the highest total VRM as the top row.\nThe results show MTA NYC Transit to have the highest total VRM, with over 10.8B revenue miles. This tells us the MTA NYC Transit system is the most extensive when viewed with the lens of # of miles covered.\n\n\n\n\n# 2. What transit mode had the most total VRM in our data set?\n#grouping by mode, summing VRM by mode & removing any NA values, arranging in descending order\ntotal_VRM_by_mode &lt;- USAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_VRM = sum(VRM, na.rm =TRUE)) |&gt;\n  arrange(desc(Total_VRM))\n#displaying summed VRM by mode table\nprint(total_VRM_by_mode)\n\n# A tibble: 18 × 2\n   Mode              Total_VRM\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 Bus             49444494088\n 2 Demand Response 17955073508\n 3 Heavy Rail      14620362107\n 4 Commuter Rail    6970644241\n 5 Vanpool          3015783362\n 6 Light Rail       2090094714\n 7 Commuter Bus     1380948975\n 8 Publico          1021270808\n 9 Trolleybus        236840288\n10 Rapid Bus         118425283\n11 Ferry Boat         65589783\n12 Streetcar          63389725\n13 Monorail           37879729\n14 Hybrid Rail        37787608\n15 Alaska Railroad    13833261\n16 Cable Car           7386019\n17 Inclined Plane       705904\n18 Aerial Tramway       292860\n\n#displaying mode with highest total VRM\nhighest_VRM_mode &lt;- total_VRM_by_mode |&gt;\n  filter(Total_VRM == max(Total_VRM)) |&gt;\n  mutate(Total_VRM = comma(Total_VRM, accuracy =1)) \n#printing highest_VRM_mode\nprint(highest_VRM_mode)\n\n# A tibble: 1 × 2\n  Mode  Total_VRM     \n  &lt;chr&gt; &lt;chr&gt;         \n1 Bus   49,444,494,088\n\n\nThis section once again reviews the total Vehicle Revenue Miles, but instead grouped by transit mode.\nThe code aggregates the VRM in a similar fashion to the first question, but instead groups by the Mode column.\nThe result reveals the mode of Bus has the highest VRM amongst the modes in the dataset, with over $49.4B vehicle revenue miles.\n\n\n\nThis question hones in on the NYC subway system specifically. To get our data into that granular level, the code, as shown below, filters the Mode to Heavy Rail.\n\n# 3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\n#first filtering the condensed_usage data frame to only heavy rail to see what else needs to be filtered out\nheavy_rail_data &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\")\n#viewing first 10 rows of filtered data\nhead(heavy_rail_data)\n#displaying unique values of metro_area\nunique_metro_areas &lt;- USAGE |&gt;\n  distinct(metro_area)\nprint(unique_metro_areas)\n\nThe code then goes on to filter further, for the month of May and for the Agency being MTA New York City Transit.\n\nnyc_subway_trips_count &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\",\n         month == \"2024-05-01\",\n         Agency == \"MTA New York City Transit\") |&gt;\n  summarize(Total_Trips = sum(UPT, na.rm = TRUE)) |&gt;\n  mutate(Total_Trips = comma(Total_Trips, accuracy =1)) \n\n# Print the total trips taken on the NYC Subway in May 2024\nprint(nyc_subway_trips_count)\n\n# A tibble: 1 × 1\n  Total_Trips\n  &lt;chr&gt;      \n1 180,458,819\n\n\nIn May 2024, over 180M trips were taken on NYC subways.\n\n\n\nThe below code aims to quantify the impact the COVID-19 pandemic had on NYC’s public transit system, specifically focusing on the subways.\nThe calculation is for ridership in April 2019, pre-pandemic, compared to ridership April 2020, at the height of the pandemic.\nThe code calculates the ridership for each of these time periods separately, then takes a difference between the two.\n\n# 5. How much did NYC subway ridership fall between April 2019 and April 2020?\n#finding ridership for April 2019 first\nnyc_subway_trips_april_2019 &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\",\n         month == \"2019-04-01\",\n         Agency == \"MTA New York City Transit\") |&gt;\n  summarize(Total_Trips = sum(UPT, na.rm = TRUE))\n\n# Print the total trips taken on the NYC Subway in April 2019\nprint(nyc_subway_trips_april_2019)\n\n# A tibble: 1 × 1\n  Total_Trips\n        &lt;dbl&gt;\n1   232223929\n\n#ridership for april 2020 second \nnyc_subway_trips_april_2020 &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\",\n         month == \"2020-04-01\",\n         Agency == \"MTA New York City Transit\") |&gt;\n  summarize(Total_Trips = sum(UPT, na.rm = TRUE))\n\n# Print the total trips taken on the NYC Subway in April 2020\nprint(nyc_subway_trips_april_2020)\n\n# A tibble: 1 × 1\n  Total_Trips\n        &lt;dbl&gt;\n1    20254269\n\n#calculating the difference \nnyc_ridership_change &lt;- abs(nyc_subway_trips_april_2020 - nyc_subway_trips_april_2019)\n#print(nyc_ridership_change)\n#Statement defining ridership change. Using paste() function to allow for string + variable to print together\nprint(paste(\"NYC Ridership fell by \", nyc_ridership_change, \" riders from April 2019 to April 2020.\"))\n\n[1] \"NYC Ridership fell by  211969660  riders from April 2019 to April 2020.\"\n\n\nAs shown in the output above, the ridership fell drastically from April 2019 to April 2020, by over 2.1B riders.\n\n\n\n\nNext, the study continues in performing exploratory data analysis. The theme of this analysis is uncovering additional insights about the MTA New York City Transit system.\nBelow, we explore NYC ridership across different transit modes. First, the focus is on breaking down the total ridership for each mode of transit available via MTA New York City Transit. The code first filters on MTA NYC Transit, groups by Mode and sums the Unlinked Passenger Trips (UPT) for each.\nThe code reveals certain modes are much more used compared to others. In NYC, the subway, or heavy rail, surpasses all other modes of transportation. One can imagine how frequently riders use the NYC subway when commuting.\n\n# ============================================================\n# Task 4 - Explore and Analyze\n# ============================================================\n\n# Three more interesting transit facts \n# Additional fact 1 - ridership for each mode by MTA NYC Transit \nmta_ridership_by_mode &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_Ridership = sum(UPT, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_Ridership)) |&gt;\n  mutate(Total_Ridership = comma(Total_Ridership, accuracy =1))\n\nprint(mta_ridership_by_mode) #table outlining different modes of transportation and their total rides \n\n# A tibble: 5 × 2\n  Mode            Total_Ridership\n  &lt;chr&gt;           &lt;chr&gt;          \n1 Heavy Rail      51,672,094,135 \n2 Bus             16,889,723,939 \n3 Rapid Bus       315,432,467    \n4 Commuter Bus    133,304,773    \n5 Demand Response 91,175,466     \n\n\nThe table above provides a readable summary of the study - one can clearly see the Heavy Rail has the highest Total Ridership, followed by the Bus.\nBelow is another analysis of the MTA NYC Transit system - comparing subway ridership pre and post pandemic. The data is segmented into two time periods: the year of 2019 (1/1/19 - 12/31/19) for pre-pandemic and the year of 2022 (1/1/22 - 12/31/22) for post pandemic.\nWe find the average monthly ridership decreased by nearly 20M trips/per month from 2019 to 2022. This is a drop in subway ridership by about 34%.\n\n# Additional fact 2 - Comparing NYC ridership pre and post pandemic \n#pre-pandemic \npre_pandemic_ridership &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\",\n         month &gt;= \"2019-01-01\",\n         month &lt;= \"2019-12-31\") |&gt;\n  summarize(avg_ridership = mean(UPT, na.rm = TRUE))\n\nprint(pre_pandemic_ridership)\n\n# A tibble: 1 × 1\n  avg_ridership\n          &lt;dbl&gt;\n1     57709634.\n\n#post-pandemic \npost_pandemic_ridership &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\",\n         month &gt;= \"2022-01-01\",\n         month &lt;= \"2022-12-31\") |&gt;\n  summarize(avg_ridership = mean(UPT, na.rm = TRUE))\n\nprint(post_pandemic_ridership)\n\n# A tibble: 1 × 1\n  avg_ridership\n          &lt;dbl&gt;\n1     37980885.\n\nprint(paste(\"The average ridership for the year of 2019, pre-pandemic, was \", round(pre_pandemic_ridership,0), \" and the average ridership for the year of 2022, post pandemic, was \", round(post_pandemic_ridership,0)))\n\n[1] \"The average ridership for the year of 2019, pre-pandemic, was  57709634  and the average ridership for the year of 2022, post pandemic, was  37980885\"\n\nround((1 - post_pandemic_ridership/pre_pandemic_ridership) * 100,0)\n\n  avg_ridership\n1            34\n\n\nThe analysis below explores the ridership efficiency of the different NYC transit modes. Efficiency is measured as the average number of Unlinked Passenger Trips (UPT) per Vehicle Revenue Mile (VRM), which indicates how effectively each mode transports passengers against the distance covered.\n\n# Additional fact 3\n#MTA NYC Transit ridership efficiency by mode \n#where efficiency represents the avg # of unlinked passenger trips per vehicle revenue mile\nridership_efficiency &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(Efficiency = round(sum(UPT, na.rm = TRUE) / sum(VRM, na.rm = TRUE),2))\nprint(ridership_efficiency) #efficiency = for every mile traveled, x# of passengers carried \n\n# A tibble: 5 × 2\n  Mode            Efficiency\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Bus                   8.05\n2 Commuter Bus          1.15\n3 Demand Response       0.11\n4 Heavy Rail            6.68\n5 Rapid Bus             9.17\n\n\nLooking at the table above, the following may be concluded. The Rapid Bus it the most efficient mode of transport in NYC. With an efficiency score of 9.17, this translates into the Rapid Bus carrying over 9 passengers per mile traveled.\nNot surprisingly, the Demand Response as the lowest efficiency at 0.11. Demand Response modes of transportation are typically used for specialized transportation needs and are available upon request.\n\n\n\nIn the task below, a comprehensive dataset is created via merging annual transit usage and financial data for 2022.\nFirst, the USAGE table is filtered on the year 2022, then aggregated each UPT and VRM for each transit agency and mode of transportation. This summarized data is stored in the new table titled USAGE_2022_ANNUAL.\nThen, the FINANCIALS table is updated to match the Mode labels in USAGE. Once again, the abbreviations are changed to their full names. This standardizes the Mode naming convention.\nLastly, the USAGE_2022_ANNUAL table is merged with the updated FINANCIALS table, joined on NTD ID and Mode. The new table is titled USAGE_AND_FINANCIALS. This table becomes very useful when evaluating farebox recovery in the following section.\n\n# ============================================================\n# Task 5 - Table Summarization\n# ============================================================\n#creating a new table from USAGE that has annual total (sum) UPT and VRM for 2022\nnames(USAGE)\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == 2022) |&gt;\n  group_by(`NTD ID`, Agency, metro_area, Mode) |&gt;\n  summarize(\n    UPT = sum(UPT, na.rm=TRUE),\n    VRM = sum(VRM, na.rm=TRUE)\n  ) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"SR\" ~ \"Streetcar\", \n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"RB\" ~ \"Rapid Bus\",\n    Mode == \"MG\" ~ \"Monorail\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))  # Default for any unrecognized codes\n\n\n#Merging USAGE_2022_ANNUAL table to the FINANCIALS table \nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n                                  FINANCIALS, \n                                  join_by(`NTD ID`, Mode)) |&gt;\n  drop_na()\n#Viewing first few rows of merged table \nhead(USAGE_AND_FINANCIALS)\n\n\n\n\n\n\nFor analysis, first the total UPT for each combination of agency and mode is summarized. The USAGE_AND_FINANCIALS table is used for this analysis.\n\n# 1. Which transit system (agency and mode) had the most UPT in 2022?\n#table summarizing the total UPT for each agency and mode \nUPT_summary_agency_mode &lt;- USAGE_AND_FINANCIALS |&gt; \n  group_by(Agency, Mode) |&gt;\n  summarize(Total_UPT = sum(UPT, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_UPT)) \n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\n#printing summary table \nprint(UPT_summary_agency_mode)\n\n# A tibble: 1,129 × 3\n# Groups:   Agency [525]\n   Agency                                                   Mode       Total_UPT\n   &lt;chr&gt;                                                    &lt;chr&gt;          &lt;dbl&gt;\n 1 MTA New York City Transit                                Heavy Rail    1.79e9\n 2 MTA New York City Transit                                Bus           4.59e8\n 3 Los Angeles County Metropolitan Transportation Authority Bus           1.94e8\n 4 Chicago Transit Authority                                Bus           1.40e8\n 5 New Jersey Transit Corporation                           Bus           1.13e8\n 6 Chicago Transit Authority                                Heavy Rail    1.04e8\n 7 MTA Bus Company                                          Bus           1.00e8\n 8 Washington Metropolitan Area Transit Authority           Heavy Rail    9.84e7\n 9 Southeastern Pennsylvania Transportation Authority       Bus           9.66e7\n10 Washington Metropolitan Area Transit Authority           Bus           8.99e7\n# ℹ 1,119 more rows\n\n#max UPT from table \nmax_UPT &lt;- UPT_summary_agency_mode |&gt; \n  filter(Total_UPT == max(Total_UPT))\nmax_UPT_system &lt;- head(max_UPT,1)\nprint(max_UPT_system)\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                    Mode        Total_UPT\n  &lt;chr&gt;                     &lt;chr&gt;           &lt;dbl&gt;\n1 MTA New York City Transit Heavy Rail 1793073801\n\nmost_UPT_agency &lt;- max_UPT_system$Agency #grabs the value from the column after the $ sign\n#print(most_UPT_agency)\nmost_UPT_mode &lt;- max_UPT_system$Mode\n#print(most_UPT_mode)\nprint(paste(\"The Agency with the highest UPT is \", most_UPT_agency, \" with a mode of \", most_UPT_mode))\n\n[1] \"The Agency with the highest UPT is  MTA New York City Transit  with a mode of  Heavy Rail\"\n\n\nThe top performing agency and its most with the highest UPT is displayed above. We find the MTA NYC Transit with its Heavy Rail mode to have the highest UPT, with ~1.8B unlinked passenger trips in 2022.\n\n\n\nTo reiterate, the Farebox Recovery Ratio is the proportion of operating expenses coverd by fare revenue, and is calculated as the ratio of Total Fares to Expenses. The higher the Farebox Recovery Ratio, the greater financial stability through passenger fare revenue.\n\n# 2. Which transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nhighest_farebox_recovery &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Farebox_Recovery = `Total Fares` / Expenses) |&gt;\n  filter(Farebox_Recovery == max(Farebox_Recovery, na.rm = TRUE)) |&gt;\n  arrange(desc(Farebox_Recovery)) |&gt;\n  head(1) #returns top row\n#print(highest_farebox_recovery)\nmost_farebox_agency &lt;- highest_farebox_recovery$Agency\nmost_farebox_mode &lt;- highest_farebox_recovery$Mode\ntop_recovery &lt;- highest_farebox_recovery$Farebox_Recovery\n\nprint(paste(\"The Agency with the highest Farebox Recovery is\", most_farebox_agency, \"and its mode is\", most_farebox_mode, \"with a farebox recovery ratio of\", round(top_recovery,2)))\n\n[1] \"The Agency with the highest Farebox Recovery is Transit Authority of Central Kentucky and its mode is Vanpool with a farebox recovery ratio of 2.38\"\n\n\nThe Vanpool mode of transportation within the Transit Authority of Central Kentucky performed with the highest farebox recovery ratio. One caution to keep in mind is this report did not filter on only the major systems, as the programmer chose to include all transportation systems (Major and Minor).\n\n\n\nThe next analysis looks at the lowest operating expenses per UPT, which is an indicator of cost efficiency. The lower the expenses per UPT, the more cost-effective a transportation system is in transporting passengers.\n\n# 3. Which transit system (agency and mode) has the lowest expenses per UPT?\nlowest_expenses_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Expenses_per_UPT = Expenses /UPT) |&gt;\n  arrange(Expenses_per_UPT) |&gt; #arranges in ascending order \n  filter(Expenses_per_UPT == min(Expenses_per_UPT, na.rm = TRUE)) |&gt;\n  head(1) #selects top row, which will be lowest, since in ascending order\n\nlowest_exp_agency &lt;- lowest_expenses_per_UPT$Agency\nlowest_exp_mode &lt;- lowest_expenses_per_UPT$Mode\nlowest_exp_per_UPT &lt;- lowest_expenses_per_UPT$Expenses_per_UPT\n\nprint(paste(\"The transit system with the lowest expenses per UPT is\", lowest_exp_agency, \"and its mode is\", lowest_exp_mode, \"with an expenses per UPT ratio of\", round(lowest_exp_per_UPT,2)))\n\n[1] \"The transit system with the lowest expenses per UPT is North Carolina State University and its mode is Bus with an expenses per UPT ratio of 1.18\"\n\n\n\n\n\n\n# 4. Which transit system (agency and mode) has the highest total fares per UPT?\nhighest_fares_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Fares_per_UPT = `Total Fares` / UPT) |&gt;\n  arrange(desc(Fares_per_UPT)) |&gt;\n  head(1)\n\nhighest_fares_agency &lt;- highest_fares_per_UPT$Agency\nhighest_fares_mode &lt;- highest_fares_per_UPT$Mode\nhighest_fares_per_UPT_amt &lt;- highest_fares_per_UPT$Fares_per_UPT\n\nprint(paste(\"The transit system with the highest total fares per UPT is\", highest_fares_agency,\"and its mode is\", highest_fares_mode, \"with a fares per UPT ratio of\", round(highest_fares_per_UPT_amt,2)))\n\n[1] \"The transit system with the highest total fares per UPT is Altoona Metro Transit and its mode is Demand Response with a fares per UPT ratio of 660.12\"\n\n\n\n\n\nThis study focuses on identifying the transit system with the lowest operating expenses per Vehicle Revenue Mile (VRM). This is another important metric in evaluating cost efficiency. In this case, we look at cost efficiency in relation to the distance covered by transit services.\n\n# 5. Which transit system (agency and mode) has the lowest expenses per VRM?\n#very similar to code for #3 \nlowest_expenses_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Expenses_per_VRM = Expenses / VRM) |&gt;\n  arrange(Expenses_per_VRM) |&gt;\n  head(1)\n\nlowest_exp_VRM_agency &lt;- lowest_expenses_per_VRM$Agency\nlowest_exp_VRM_mode &lt;- lowest_expenses_per_VRM$Mode\nlowest_exp_per_VRM_amt &lt;- lowest_expenses_per_VRM$Expenses_per_VRM\n\nprint(paste(\"The transit system with the lowest expenses per VRM is\", lowest_exp_VRM_agency, \"and its mode is\", lowest_exp_VRM_mode, \"with an expenses per VRM ratio of\", round(lowest_exp_per_VRM_amt,2)))\n\n[1] \"The transit system with the lowest expenses per VRM is New Mexico Department of Transportation and its mode is Vanpool with an expenses per VRM ratio of 0.34\"\n\n\nThe output above tells us the New Mexico Department of Transportation, when operating the Vanpool mode, has the lowest expenses per VRM, at $0.34 per mile. This portrays the efficient use of resources within this mode and agency.\n\n\n\n\n# 6. Which transit system (agency and mode) has the highest total fares per VRM?\n# very similar to code for #4\nhighest_fares_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Fares_per_VRM = `Total Fares` / VRM) |&gt;\n  arrange(desc(Fares_per_VRM)) |&gt;\n  head(1)\n\nhighest_fares_agency &lt;- highest_fares_per_VRM$Agency\nhighest_fares_mode &lt;- highest_fares_per_VRM$Mode\nhighest_fares_per_VRM_amt &lt;- highest_fares_per_VRM$Fares_per_VRM\n\nprint(paste(\"The transit system with the highest total fares per VRM is\", highest_fares_agency,\"and its mode is\", highest_fares_mode, \"with a fares per VRM ratio of\", round(highest_fares_per_VRM_amt,2)))\n\n[1] \"The transit system with the highest total fares per VRM is Chicago Water Taxi (Wendella) and its mode is Ferry Boat with a fares per VRM ratio of 237.46\"\n\n\nWe find the Chicago Water Taxi has the highest fares per VRM, at ~$237. This translates to for every mile traveled by the ferry, the system generates over $237 in revenue.\n\n\n\nTo conclude, the MTA New York City Transit Heavy Rail Mode is the most efficient transportation system in the country. The subway had $1.8B unlinked trips in 2022 alone. With the highest ridership and UPT, surpassing all other transportation systems in the country, the NYC subway provides the best service efficiency. The scale and service of the MTA NYC Transit system in unparalleled across the country."
  },
  {
    "objectID": "mp01.html#task-1",
    "href": "mp01.html#task-1",
    "title": "mp01",
    "section": "",
    "text": "Below you will find the renaming of column UZA Name to metro_area. This variable is used multiple times across the analysis. By creating a syntatic name, the column no longer needs to be surrounded by quotes each times it is referenced, making for easier, cleaner code.\n\n# ============================================================\n# Task 1 - Creating Syntatic Names \n# ============================================================\n#renaming UZA Name to metro_area\nUSAGE &lt;- USAGE |&gt;\n  rename(metro_area = 'UZA Name')\n\nTRIPS &lt;- TRIPS |&gt;\n  rename(metro_area = 'UZA Name')\n\nMILES &lt;- MILES |&gt;\n  rename(metro_area = 'UZA Name')\n\nnames(USAGE)\nnames(TRIPS)\nnames(MILES)"
  },
  {
    "objectID": "mp01.html#task-2",
    "href": "mp01.html#task-2",
    "title": "mp01",
    "section": "",
    "text": "In the code below, you will find Mode column in the USAGE table was recoded using the case_when function. The Mode columns were initially in a shorthand abbreviation and were transformed into their full terms, which provide for more descriptive, clear labels.\n\n# ============================================================\n# Task 2 - Recoding the Mode column\n# ============================================================\n#using case_when to recode the mode column \nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"SR\" ~ \"Streetcar\", \n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"RB\" ~ \"Rapid Bus\",\n    Mode == \"MG\" ~ \"Monorail\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))  # Default for any unrecognized codes\n  \n#checking result of task 2\nhead(USAGE)\nUSAGE |&gt;\n  distinct(Mode)\n\n\n#creating an attractive summary table of cleaned up USAGE table \nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month=as.character(month)) |&gt; \n  DT::datatable()\n\n\n\n\n\nThe displayed table above is a view of a random sample of 1000 from the cleaned USAGE table. The format is interactive, allowing users to sort and search through the data, making it easily accessible and usable, for even a non-technical user."
  },
  {
    "objectID": "mp01.html#task-3",
    "href": "mp01.html#task-3",
    "title": "mp01",
    "section": "",
    "text": "library(scales)\n# 1. What transit agency had the most total VRM in our data set?\n#grouping by agency, summing VRM by agency & removing any NA values, arranging in descending order\ntotal_VRM_by_agency &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(Total_VRM = sum(VRM, na.rm =TRUE)) |&gt;\n  arrange(desc(Total_VRM)) \n  \n\n#formatting table using gt package\ntotal_VRM_by_agency_formatted &lt;- total_VRM_by_agency |&gt;\n  mutate(Total_VRM = comma(Total_VRM, accuracy =1)) \n\n#printing formatted table \nprint(total_VRM_by_agency_formatted)\n\n# A tibble: 677 × 2\n   Agency                                                              Total_VRM\n   &lt;chr&gt;                                                               &lt;chr&gt;    \n 1 MTA New York City Transit                                           10,832,8…\n 2 New Jersey Transit Corporation                                      5,645,52…\n 3 Los Angeles County Metropolitan Transportation Authority            4,354,01…\n 4 Washington Metropolitan Area Transit Authority                      2,821,95…\n 5 Chicago Transit Authority                                           2,806,20…\n 6 Southeastern Pennsylvania Transportation Authority                  2,672,63…\n 7 Massachusetts Bay Transportation Authority                          2,383,96…\n 8 Pace, the Suburban Bus Division of the Regional Transportation Aut… 2,379,40…\n 9 Metropolitan Transit Authority of Harris County, Texas              2,272,94…\n10 Denver Regional Transportation District                             1,991,41…\n# ℹ 667 more rows\n\n#displaying agency with highest total VRM\nhighest_VRM_agency &lt;- total_VRM_by_agency |&gt;\n  filter(Total_VRM == max(Total_VRM)) |&gt;\n  mutate(Total_VRM = comma(Total_VRM, accuracy =1)) \n  \n#printing highest_VRM_agency\nprint(highest_VRM_agency)\n\n# A tibble: 1 × 2\n  Agency                    Total_VRM     \n  &lt;chr&gt;                     &lt;chr&gt;         \n1 MTA New York City Transit 10,832,855,350\n\n\nThe VRM, or Vehicle Revenue Miles, represents the total distance traveled by vehicles in revenue service.\nThe analysis starts with aggregating the VRM values for each transit agency, grouping the data by Agency, and finally summing the total VRM for each group, or agency. Any missing values are excluded from the grouping, through the na.rm = TRUE statement. The results are then arranged in descending order, with the agency with the highest total VRM as the top row.\nThe results show MTA NYC Transit to have the highest total VRM, with over 10.8B revenue miles. This tells us the MTA NYC Transit system is the most extensive when viewed with the lens of # of miles covered.\n\n\n\n\n# 2. What transit mode had the most total VRM in our data set?\n#grouping by mode, summing VRM by mode & removing any NA values, arranging in descending order\ntotal_VRM_by_mode &lt;- USAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_VRM = sum(VRM, na.rm =TRUE)) |&gt;\n  arrange(desc(Total_VRM))\n#displaying summed VRM by mode table\nprint(total_VRM_by_mode)\n\n# A tibble: 18 × 2\n   Mode              Total_VRM\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 Bus             49444494088\n 2 Demand Response 17955073508\n 3 Heavy Rail      14620362107\n 4 Commuter Rail    6970644241\n 5 Vanpool          3015783362\n 6 Light Rail       2090094714\n 7 Commuter Bus     1380948975\n 8 Publico          1021270808\n 9 Trolleybus        236840288\n10 Rapid Bus         118425283\n11 Ferry Boat         65589783\n12 Streetcar          63389725\n13 Monorail           37879729\n14 Hybrid Rail        37787608\n15 Alaska Railroad    13833261\n16 Cable Car           7386019\n17 Inclined Plane       705904\n18 Aerial Tramway       292860\n\n#displaying mode with highest total VRM\nhighest_VRM_mode &lt;- total_VRM_by_mode |&gt;\n  filter(Total_VRM == max(Total_VRM)) |&gt;\n  mutate(Total_VRM = comma(Total_VRM, accuracy =1)) \n#printing highest_VRM_mode\nprint(highest_VRM_mode)\n\n# A tibble: 1 × 2\n  Mode  Total_VRM     \n  &lt;chr&gt; &lt;chr&gt;         \n1 Bus   49,444,494,088\n\n\nThis section once again reviews the total Vehicle Revenue Miles, but instead grouped by transit mode.\nThe code aggregates the VRM in a similar fashion to the first question, but instead groups by the Mode column.\nThe result reveals the mode of Bus has the highest VRM amongst the modes in the dataset, with over $49.4B vehicle revenue miles.\n\n\n\nThis question hones in on the NYC subway system specifically. To get our data into that granular level, the code, as shown below, filters the Mode to Heavy Rail.\n\n# 3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\n#first filtering the condensed_usage data frame to only heavy rail to see what else needs to be filtered out\nheavy_rail_data &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\")\n#viewing first 10 rows of filtered data\nhead(heavy_rail_data)\n#displaying unique values of metro_area\nunique_metro_areas &lt;- USAGE |&gt;\n  distinct(metro_area)\nprint(unique_metro_areas)\n\nThe code then goes on to filter further, for the month of May and for the Agency being MTA New York City Transit.\n\nnyc_subway_trips_count &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\",\n         month == \"2024-05-01\",\n         Agency == \"MTA New York City Transit\") |&gt;\n  summarize(Total_Trips = sum(UPT, na.rm = TRUE)) |&gt;\n  mutate(Total_Trips = comma(Total_Trips, accuracy =1)) \n\n# Print the total trips taken on the NYC Subway in May 2024\nprint(nyc_subway_trips_count)\n\n# A tibble: 1 × 1\n  Total_Trips\n  &lt;chr&gt;      \n1 180,458,819\n\n\nIn May 2024, over 180M trips were taken on NYC subways.\n\n\n\nThe below code aims to quantify the impact the COVID-19 pandemic had on NYC’s public transit system, specifically focusing on the subways.\nThe calculation is for ridership in April 2019, pre-pandemic, compared to ridership April 2020, at the height of the pandemic.\nThe code calculates the ridership for each of these time periods separately, then takes a difference between the two.\n\n# 5. How much did NYC subway ridership fall between April 2019 and April 2020?\n#finding ridership for April 2019 first\nnyc_subway_trips_april_2019 &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\",\n         month == \"2019-04-01\",\n         Agency == \"MTA New York City Transit\") |&gt;\n  summarize(Total_Trips = sum(UPT, na.rm = TRUE))\n\n# Print the total trips taken on the NYC Subway in April 2019\nprint(nyc_subway_trips_april_2019)\n\n# A tibble: 1 × 1\n  Total_Trips\n        &lt;dbl&gt;\n1   232223929\n\n#ridership for april 2020 second \nnyc_subway_trips_april_2020 &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\",\n         month == \"2020-04-01\",\n         Agency == \"MTA New York City Transit\") |&gt;\n  summarize(Total_Trips = sum(UPT, na.rm = TRUE))\n\n# Print the total trips taken on the NYC Subway in April 2020\nprint(nyc_subway_trips_april_2020)\n\n# A tibble: 1 × 1\n  Total_Trips\n        &lt;dbl&gt;\n1    20254269\n\n#calculating the difference \nnyc_ridership_change &lt;- abs(nyc_subway_trips_april_2020 - nyc_subway_trips_april_2019)\n#print(nyc_ridership_change)\n#Statement defining ridership change. Using paste() function to allow for string + variable to print together\nprint(paste(\"NYC Ridership fell by \", nyc_ridership_change, \" riders from April 2019 to April 2020.\"))\n\n[1] \"NYC Ridership fell by  211969660  riders from April 2019 to April 2020.\"\n\n\nAs shown in the output above, the ridership fell drastically from April 2019 to April 2020, by over 2.1B riders."
  },
  {
    "objectID": "mp01.html#exploratory-data-analysis",
    "href": "mp01.html#exploratory-data-analysis",
    "title": "mp01",
    "section": "",
    "text": "Next, the study continues in performing exploratory data analysis. The theme of this analysis is uncovering additional insights about the MTA New York City Transit system.\nBelow, we explore NYC ridership across different transit modes. First, the focus is on breaking down the total ridership for each mode of transit available via MTA New York City Transit. The code first filters on MTA NYC Transit, groups by Mode and sums the Unlinked Passenger Trips (UPT) for each.\nThe code reveals certain modes are much more used compared to others. In NYC, the subway, or heavy rail, surpasses all other modes of transportation. One can imagine how frequently riders use the NYC subway when commuting.\n\n# ============================================================\n# Task 4 - Explore and Analyze\n# ============================================================\n\n# Three more interesting transit facts \n# Additional fact 1 - ridership for each mode by MTA NYC Transit \nmta_ridership_by_mode &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_Ridership = sum(UPT, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_Ridership)) |&gt;\n  mutate(Total_Ridership = comma(Total_Ridership, accuracy =1))\n\nprint(mta_ridership_by_mode) #table outlining different modes of transportation and their total rides \n\n# A tibble: 5 × 2\n  Mode            Total_Ridership\n  &lt;chr&gt;           &lt;chr&gt;          \n1 Heavy Rail      51,672,094,135 \n2 Bus             16,889,723,939 \n3 Rapid Bus       315,432,467    \n4 Commuter Bus    133,304,773    \n5 Demand Response 91,175,466     \n\n\nThe table above provides a readable summary of the study - one can clearly see the Heavy Rail has the highest Total Ridership, followed by the Bus.\nBelow is another analysis of the MTA NYC Transit system - comparing subway ridership pre and post pandemic. The data is segmented into two time periods: the year of 2019 (1/1/19 - 12/31/19) for pre-pandemic and the year of 2022 (1/1/22 - 12/31/22) for post pandemic.\nWe find the average monthly ridership decreased by nearly 20M trips/per month from 2019 to 2022. This is a drop in subway ridership by about 34%.\n\n# Additional fact 2 - Comparing NYC ridership pre and post pandemic \n#pre-pandemic \npre_pandemic_ridership &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\",\n         month &gt;= \"2019-01-01\",\n         month &lt;= \"2019-12-31\") |&gt;\n  summarize(avg_ridership = mean(UPT, na.rm = TRUE))\n\nprint(pre_pandemic_ridership)\n\n# A tibble: 1 × 1\n  avg_ridership\n          &lt;dbl&gt;\n1     57709634.\n\n#post-pandemic \npost_pandemic_ridership &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\",\n         month &gt;= \"2022-01-01\",\n         month &lt;= \"2022-12-31\") |&gt;\n  summarize(avg_ridership = mean(UPT, na.rm = TRUE))\n\nprint(post_pandemic_ridership)\n\n# A tibble: 1 × 1\n  avg_ridership\n          &lt;dbl&gt;\n1     37980885.\n\nprint(paste(\"The average ridership for the year of 2019, pre-pandemic, was \", round(pre_pandemic_ridership,0), \" and the average ridership for the year of 2022, post pandemic, was \", round(post_pandemic_ridership,0)))\n\n[1] \"The average ridership for the year of 2019, pre-pandemic, was  57709634  and the average ridership for the year of 2022, post pandemic, was  37980885\"\n\nround((1 - post_pandemic_ridership/pre_pandemic_ridership) * 100,0)\n\n  avg_ridership\n1            34\n\n\nThe analysis below explores the ridership efficiency of the different NYC transit modes. Efficiency is measured as the average number of Unlinked Passenger Trips (UPT) per Vehicle Revenue Mile (VRM), which indicates how effectively each mode transports passengers against the distance covered.\n\n# Additional fact 3\n#MTA NYC Transit ridership efficiency by mode \n#where efficiency represents the avg # of unlinked passenger trips per vehicle revenue mile\nridership_efficiency &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(Efficiency = round(sum(UPT, na.rm = TRUE) / sum(VRM, na.rm = TRUE),2))\nprint(ridership_efficiency) #efficiency = for every mile traveled, x# of passengers carried \n\n# A tibble: 5 × 2\n  Mode            Efficiency\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Bus                   8.05\n2 Commuter Bus          1.15\n3 Demand Response       0.11\n4 Heavy Rail            6.68\n5 Rapid Bus             9.17\n\n\nLooking at the table above, the following may be concluded. The Rapid Bus it the most efficient mode of transport in NYC. With an efficiency score of 9.17, this translates into the Rapid Bus carrying over 9 passengers per mile traveled.\nNot surprisingly, the Demand Response as the lowest efficiency at 0.11. Demand Response modes of transportation are typically used for specialized transportation needs and are available upon request."
  },
  {
    "objectID": "mp01.html#table-summarization",
    "href": "mp01.html#table-summarization",
    "title": "mp01",
    "section": "",
    "text": "In the task below, a comprehensive dataset is created via merging annual transit usage and financial data for 2022.\nFirst, the USAGE table is filtered on the year 2022, then aggregated each UPT and VRM for each transit agency and mode of transportation. This summarized data is stored in the new table titled USAGE_2022_ANNUAL.\nThen, the FINANCIALS table is updated to match the Mode labels in USAGE. Once again, the abbreviations are changed to their full names. This standardizes the Mode naming convention.\nLastly, the USAGE_2022_ANNUAL table is merged with the updated FINANCIALS table, joined on NTD ID and Mode. The new table is titled USAGE_AND_FINANCIALS. This table becomes very useful when evaluating farebox recovery in the following section.\n\n# ============================================================\n# Task 5 - Table Summarization\n# ============================================================\n#creating a new table from USAGE that has annual total (sum) UPT and VRM for 2022\nnames(USAGE)\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == 2022) |&gt;\n  group_by(`NTD ID`, Agency, metro_area, Mode) |&gt;\n  summarize(\n    UPT = sum(UPT, na.rm=TRUE),\n    VRM = sum(VRM, na.rm=TRUE)\n  ) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"SR\" ~ \"Streetcar\", \n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"RB\" ~ \"Rapid Bus\",\n    Mode == \"MG\" ~ \"Monorail\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))  # Default for any unrecognized codes\n\n\n#Merging USAGE_2022_ANNUAL table to the FINANCIALS table \nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n                                  FINANCIALS, \n                                  join_by(`NTD ID`, Mode)) |&gt;\n  drop_na()\n#Viewing first few rows of merged table \nhead(USAGE_AND_FINANCIALS)"
  },
  {
    "objectID": "mp01.html#farebox-recovery-among-major-systems",
    "href": "mp01.html#farebox-recovery-among-major-systems",
    "title": "mp01",
    "section": "",
    "text": "For analysis, first the total UPT for each combination of agency and mode is summarized. The USAGE_AND_FINANCIALS table is used for this analysis.\n\n# 1. Which transit system (agency and mode) had the most UPT in 2022?\n#table summarizing the total UPT for each agency and mode \nUPT_summary_agency_mode &lt;- USAGE_AND_FINANCIALS |&gt; \n  group_by(Agency, Mode) |&gt;\n  summarize(Total_UPT = sum(UPT, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_UPT)) \n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\n#printing summary table \nprint(UPT_summary_agency_mode)\n\n# A tibble: 1,129 × 3\n# Groups:   Agency [525]\n   Agency                                                   Mode       Total_UPT\n   &lt;chr&gt;                                                    &lt;chr&gt;          &lt;dbl&gt;\n 1 MTA New York City Transit                                Heavy Rail    1.79e9\n 2 MTA New York City Transit                                Bus           4.59e8\n 3 Los Angeles County Metropolitan Transportation Authority Bus           1.94e8\n 4 Chicago Transit Authority                                Bus           1.40e8\n 5 New Jersey Transit Corporation                           Bus           1.13e8\n 6 Chicago Transit Authority                                Heavy Rail    1.04e8\n 7 MTA Bus Company                                          Bus           1.00e8\n 8 Washington Metropolitan Area Transit Authority           Heavy Rail    9.84e7\n 9 Southeastern Pennsylvania Transportation Authority       Bus           9.66e7\n10 Washington Metropolitan Area Transit Authority           Bus           8.99e7\n# ℹ 1,119 more rows\n\n#max UPT from table \nmax_UPT &lt;- UPT_summary_agency_mode |&gt; \n  filter(Total_UPT == max(Total_UPT))\nmax_UPT_system &lt;- head(max_UPT,1)\nprint(max_UPT_system)\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                    Mode        Total_UPT\n  &lt;chr&gt;                     &lt;chr&gt;           &lt;dbl&gt;\n1 MTA New York City Transit Heavy Rail 1793073801\n\nmost_UPT_agency &lt;- max_UPT_system$Agency #grabs the value from the column after the $ sign\n#print(most_UPT_agency)\nmost_UPT_mode &lt;- max_UPT_system$Mode\n#print(most_UPT_mode)\nprint(paste(\"The Agency with the highest UPT is \", most_UPT_agency, \" with a mode of \", most_UPT_mode))\n\n[1] \"The Agency with the highest UPT is  MTA New York City Transit  with a mode of  Heavy Rail\"\n\n\nThe top performing agency and its most with the highest UPT is displayed above. We find the MTA NYC Transit with its Heavy Rail mode to have the highest UPT, with ~1.8B unlinked passenger trips in 2022.\n\n\n\nTo reiterate, the Farebox Recovery Ratio is the proportion of operating expenses coverd by fare revenue, and is calculated as the ratio of Total Fares to Expenses. The higher the Farebox Recovery Ratio, the greater financial stability through passenger fare revenue.\n\n# 2. Which transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nhighest_farebox_recovery &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Farebox_Recovery = `Total Fares` / Expenses) |&gt;\n  filter(Farebox_Recovery == max(Farebox_Recovery, na.rm = TRUE)) |&gt;\n  arrange(desc(Farebox_Recovery)) |&gt;\n  head(1) #returns top row\n#print(highest_farebox_recovery)\nmost_farebox_agency &lt;- highest_farebox_recovery$Agency\nmost_farebox_mode &lt;- highest_farebox_recovery$Mode\ntop_recovery &lt;- highest_farebox_recovery$Farebox_Recovery\n\nprint(paste(\"The Agency with the highest Farebox Recovery is\", most_farebox_agency, \"and its mode is\", most_farebox_mode, \"with a farebox recovery ratio of\", round(top_recovery,2)))\n\n[1] \"The Agency with the highest Farebox Recovery is Transit Authority of Central Kentucky and its mode is Vanpool with a farebox recovery ratio of 2.38\"\n\n\nThe Vanpool mode of transportation within the Transit Authority of Central Kentucky performed with the highest farebox recovery ratio. One caution to keep in mind is this report did not filter on only the major systems, as the programmer chose to include all transportation systems (Major and Minor).\n\n\n\nThe next analysis looks at the lowest operating expenses per UPT, which is an indicator of cost efficiency. The lower the expenses per UPT, the more cost-effective a transportation system is in transporting passengers.\n\n# 3. Which transit system (agency and mode) has the lowest expenses per UPT?\nlowest_expenses_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Expenses_per_UPT = Expenses /UPT) |&gt;\n  arrange(Expenses_per_UPT) |&gt; #arranges in ascending order \n  filter(Expenses_per_UPT == min(Expenses_per_UPT, na.rm = TRUE)) |&gt;\n  head(1) #selects top row, which will be lowest, since in ascending order\n\nlowest_exp_agency &lt;- lowest_expenses_per_UPT$Agency\nlowest_exp_mode &lt;- lowest_expenses_per_UPT$Mode\nlowest_exp_per_UPT &lt;- lowest_expenses_per_UPT$Expenses_per_UPT\n\nprint(paste(\"The transit system with the lowest expenses per UPT is\", lowest_exp_agency, \"and its mode is\", lowest_exp_mode, \"with an expenses per UPT ratio of\", round(lowest_exp_per_UPT,2)))\n\n[1] \"The transit system with the lowest expenses per UPT is North Carolina State University and its mode is Bus with an expenses per UPT ratio of 1.18\"\n\n\n\n\n\n\n# 4. Which transit system (agency and mode) has the highest total fares per UPT?\nhighest_fares_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Fares_per_UPT = `Total Fares` / UPT) |&gt;\n  arrange(desc(Fares_per_UPT)) |&gt;\n  head(1)\n\nhighest_fares_agency &lt;- highest_fares_per_UPT$Agency\nhighest_fares_mode &lt;- highest_fares_per_UPT$Mode\nhighest_fares_per_UPT_amt &lt;- highest_fares_per_UPT$Fares_per_UPT\n\nprint(paste(\"The transit system with the highest total fares per UPT is\", highest_fares_agency,\"and its mode is\", highest_fares_mode, \"with a fares per UPT ratio of\", round(highest_fares_per_UPT_amt,2)))\n\n[1] \"The transit system with the highest total fares per UPT is Altoona Metro Transit and its mode is Demand Response with a fares per UPT ratio of 660.12\"\n\n\n\n\n\nThis study focuses on identifying the transit system with the lowest operating expenses per Vehicle Revenue Mile (VRM). This is another important metric in evaluating cost efficiency. In this case, we look at cost efficiency in relation to the distance covered by transit services.\n\n# 5. Which transit system (agency and mode) has the lowest expenses per VRM?\n#very similar to code for #3 \nlowest_expenses_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Expenses_per_VRM = Expenses / VRM) |&gt;\n  arrange(Expenses_per_VRM) |&gt;\n  head(1)\n\nlowest_exp_VRM_agency &lt;- lowest_expenses_per_VRM$Agency\nlowest_exp_VRM_mode &lt;- lowest_expenses_per_VRM$Mode\nlowest_exp_per_VRM_amt &lt;- lowest_expenses_per_VRM$Expenses_per_VRM\n\nprint(paste(\"The transit system with the lowest expenses per VRM is\", lowest_exp_VRM_agency, \"and its mode is\", lowest_exp_VRM_mode, \"with an expenses per VRM ratio of\", round(lowest_exp_per_VRM_amt,2)))\n\n[1] \"The transit system with the lowest expenses per VRM is New Mexico Department of Transportation and its mode is Vanpool with an expenses per VRM ratio of 0.34\"\n\n\nThe output above tells us the New Mexico Department of Transportation, when operating the Vanpool mode, has the lowest expenses per VRM, at $0.34 per mile. This portrays the efficient use of resources within this mode and agency.\n\n\n\n\n# 6. Which transit system (agency and mode) has the highest total fares per VRM?\n# very similar to code for #4\nhighest_fares_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Fares_per_VRM = `Total Fares` / VRM) |&gt;\n  arrange(desc(Fares_per_VRM)) |&gt;\n  head(1)\n\nhighest_fares_agency &lt;- highest_fares_per_VRM$Agency\nhighest_fares_mode &lt;- highest_fares_per_VRM$Mode\nhighest_fares_per_VRM_amt &lt;- highest_fares_per_VRM$Fares_per_VRM\n\nprint(paste(\"The transit system with the highest total fares per VRM is\", highest_fares_agency,\"and its mode is\", highest_fares_mode, \"with a fares per VRM ratio of\", round(highest_fares_per_VRM_amt,2)))\n\n[1] \"The transit system with the highest total fares per VRM is Chicago Water Taxi (Wendella) and its mode is Ferry Boat with a fares per VRM ratio of 237.46\"\n\n\nWe find the Chicago Water Taxi has the highest fares per VRM, at ~$237. This translates to for every mile traveled by the ferry, the system generates over $237 in revenue.\n\n\n\nTo conclude, the MTA New York City Transit Heavy Rail Mode is the most efficient transportation system in the country. The subway had $1.8B unlinked trips in 2022 alone. With the highest ridership and UPT, surpassing all other transportation systems in the country, the NYC subway provides the best service efficiency. The scale and service of the MTA NYC Transit system in unparalleled across the country."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "mp02",
    "section": "",
    "text": "Abstract: The modeler of this project plays the role of a Hollywood development executive; the executive in charge of coming up with new movie ideas. The project aims to develop a set of data-driven ideas for new movies. The process involves diving into Hollywood history to: identify key characteristics of successful movies, identify successful filmmakers and actors, and examine some of Hollywood’s most famous flops.\n\n\nCode\n#Installing necessary packages\npackages &lt;- c(\"dplyr\", \"ggplot2\", \"tidyr\", \"readr\", \"lintr\", \"gt\" )\ninstall.packages(packages)\n\n# Check for missing packages and install them\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n\n\n\nCode\n#Loading necessary libraries \nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(lintr)\nlibrary(gt)\n\n\nInstalled necessary packages and loaded libraries successfully.\nNote: Due to technical difficulties, a pre-processed data set was loaded.\n\n\nCode\nget_imdb_file &lt;- function(fname){\n    # Set the base URL to the professor's GitHub\n    BASE_URL &lt;- \"https://raw.githubusercontent.com/michaelweylandt/STA9750/main/miniprojects/mini02_preprocessed/\"\n    fname_ext &lt;- paste0(fname, \"_small.csv.zip\") # Use the correct .csv.zip extension\n    \n    # Check if the file exists locally, if not download it\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, destfile = fname_ext)\n        message(paste(\"Downloaded:\", fname_ext))\n    }\n    \n    # Read the .csv file directly from the .zip\n    data &lt;- readr::read_csv(fname_ext)\n    \n    return(as.data.frame(data))\n}\n\n# Load the pre-processed datasets\nNAME_BASICS      &lt;- get_imdb_file(\"name_basics\")\nTITLE_BASICS     &lt;- get_imdb_file(\"title_basics\")\nTITLE_EPISODES   &lt;- get_imdb_file(\"title_episodes\")\nTITLE_RATINGS    &lt;- get_imdb_file(\"title_ratings\")\nTITLE_CREW       &lt;- get_imdb_file(\"title_crew\")\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title_principals\")\n\n\nPre-Processed Data has been loaded successfully.\n\n\nTo better understand the structure of the data, we will use the glimpse function from the dplyr package to examine each table. This will provide insight into the number of columns, column names, and their respective data types for each dataset.\n\n\nCode\nglimpse(NAME_BASICS)\n\n\nRows: 2,460,608\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;chr&gt; \"1899\", \"1924\", \"1934\", \"1949\", \"1918\", \"1915\", \"189…\n$ deathYear         &lt;chr&gt; \"1987\", \"2014\", \"\\\\N\", \"1982\", \"2007\", \"1982\", \"1957…\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\nCode\nglimpse(TITLE_BASICS)\n\n\nRows: 372,198\nColumns: 9\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt…\n$ titleType      &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"…\n$ primaryTitle   &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ originalTitle  &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ isAdult        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ startYear      &lt;dbl&gt; 1894, 1892, 1892, 1892, 1893, 1894, 1894, 1894, 1894, 1…\n$ endYear        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\",…\n$ runtimeMinutes &lt;chr&gt; \"1\", \"5\", \"5\", \"12\", \"1\", \"1\", \"1\", \"1\", \"45\", \"1\", \"1\"…\n$ genres         &lt;chr&gt; \"Documentary,Short\", \"Animation,Short\", \"Animation,Come…\n\n\nCode\nglimpse(TITLE_EPISODES)\n\n\nRows: 3,007,178\nColumns: 4\n$ tconst        &lt;chr&gt; \"tt0045960\", \"tt0046855\", \"tt0048378\", \"tt0048562\", \"tt0…\n$ parentTconst  &lt;chr&gt; \"tt0044284\", \"tt0046643\", \"tt0047702\", \"tt0047768\", \"tt0…\n$ seasonNumber  &lt;chr&gt; \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"3\", \"3\", \"…\n$ episodeNumber &lt;chr&gt; \"3\", \"4\", \"6\", \"10\", \"4\", \"20\", \"5\", \"2\", \"20\", \"6\", \"2\"…\n\n\nCode\nglimpse(TITLE_RATINGS)\n\n\nRows: 372,198\nColumns: 3\n$ tconst        &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt0…\n$ averageRating &lt;dbl&gt; 5.7, 5.6, 6.5, 5.4, 6.2, 5.0, 5.4, 5.4, 5.4, 6.8, 5.2, 7…\n$ numVotes      &lt;dbl&gt; 2090, 283, 2094, 184, 2828, 196, 889, 2233, 214, 7699, 3…\n\n\nCode\nglimpse(TITLE_CREW)\n\n\nRows: 371,902\nColumns: 3\n$ tconst    &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt00000…\n$ directors &lt;chr&gt; \"nm0005690\", \"nm0721526\", \"nm0721526\", \"nm0721526\", \"nm00056…\n$ writers   &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"nm0…\n\n\nCode\nglimpse(TITLE_PRINCIPALS)\n\n\nRows: 6,586,689\nColumns: 6\n$ tconst     &lt;chr&gt; \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000…\n$ ordering   &lt;dbl&gt; 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 1, 2, 3, 4,…\n$ nconst     &lt;chr&gt; \"nm1588970\", \"nm0005690\", \"nm0005690\", \"nm0374658\", \"nm0721…\n$ category   &lt;chr&gt; \"self\", \"director\", \"producer\", \"cinematographer\", \"directo…\n$ job        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"producer\", \"director of photography\", \"\\\\N\",…\n$ characters &lt;chr&gt; \"[\\\"Self\\\"]\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\…\n\n\n\n\n\nMost columns appear to be read in as character(string) vectors, but should be numeric. The “null” values are represented as \\N, which R does not recognize as NA values.\nUsing the mutate and the as.numeric commands to change th types of columns.\nStarting with the NAME_BASICS table - taking character columns and converting to numeric, where needed.\n\n\nCode\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  mutate(birthYear = as.numeric(replace(birthYear, birthYear == \"\\\\N\", NA)),\n         deathYear = as.numeric(replace(deathYear, deathYear == \"\\\\N\", NA)))\n\n\nMoving on to the TITLE_BASICS table. For this one, we adjust two columns from character to numeric. One new fix we are making it taking the isAdult column and converting it from numeric (with values 0 or 1) and converting it to logical (TRUE/FALSE values). This way we will not have to remember how 0 and 1 are defined as TRUE/FALSE values.\n\n\nCode\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(\n    endYear = as.numeric(replace(endYear, endYear == \"\\\\N\", NA)),\n    runtimeMinutes = as.numeric(replace(runtimeMinutes, runtimeMinutes == \"\\\\N\", NA)),\n    isAdult = as.logical(isAdult) \n  )\n\n\nCleaning the TITLE_EPISODES table.\n\n\nCode\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n  mutate(\n    seasonNumber = as.numeric(replace(seasonNumber, seasonNumber == \"\\\\N\", NA)),\n    episodeNumber = as.numeric(replace(episodeNumber, episodeNumber == \"\\\\N\", NA))\n  )\n\n\nThe next table to clean would be the TITLE_RATINGS table. The three columns have the correct datatypes so this table is skipped.\nThe writers column in the TITLE_CREW table has several \\N values which need to be replaced with NA. We will maintain the writer column’s type. This is done in the step below.\n\n\nCode\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    mutate(writers = na_if(writers, \"\\\\N\"))\n\n\nThe last table left for cleaning is the TITLE_PRINCIPALS table. We will replace the \\N values in the job and characters column with NA, while maintaining the columns’ data type.\n\n\nCode\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    mutate(job = na_if(job, \"\\\\N\"),\n           characters = na_if(characters, \"\\\\N\"))\n\n\n\n\n\nThe code below gives us a summary of the number of movies, TV series, and TV episodes in our dataset. (Q1)\nThe values are calculated using dplyr functionality, and the summary table is created using the previously downloaded gt package.\n\n\nCode\nnum_movies &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\") |&gt;\n  nrow()\n\nnum_tv_series &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  nrow()\n\nnum_tv_episodes &lt;- TITLE_EPISODES |&gt;\n  nrow()\n\n\n\n\nCode\n# Creating summary table\nsummary_table &lt;- data.frame(\n  'Movies' = num_movies,\n  'TV Series' = num_tv_series, \n  'TV Episodes' = num_tv_episodes\n)\n\n# Formatting summary table with gt\nsummary_gt &lt;- summary_table |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Total Movies, TV Series and Episodes\"\n  ) |&gt;\n  fmt_number(\n    columns = everything(),\n    decimals = 0\n  )\n\n# Display the table\nsummary_gt\n\n\n\n\n\n\n\n\nTotal Movies, TV Series and Episodes\n\n\nMovies\nTV.Series\nTV.Episodes\n\n\n\n\n131,662\n29,789\n3,007,178\n\n\n\n\n\n\n\nThe code below answers the question of who the oldest living person in our data set is. We first access the oldest living person from the NAME_BASICS dataset and then format the result into a table using gt. (Q2)\nTo access the oldest living person in the NAME_BASICS dataset, the code below filters to find the person with the maximum birthYear where deathYear is NA.\n\nCode\noldest_living_person &lt;- NAME_BASICS |&gt;\n  filter(!is.na(birthYear) & is.na(deathYear) & birthYear &gt;= 1900) |&gt;\n  arrange(birthYear) |&gt;\n  slice(1) |&gt;\n  select(primaryName, birthYear)\n\noldest_living_person_tb &lt;- oldest_living_person |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Oldest Living Person in Dataset\"\n  ) |&gt;\n  cols_label(\n    primaryName = \"Name\",\n    birthYear = \"Birth Year\"\n  ) |&gt;\n  fmt_number(\n    columns = c(birthYear),\n    decimals = 0,\n    use_seps = FALSE\n  )\n\nprint(oldest_living_person_tb)\n\n\n\n\n\n\n\nOldest Living Person in Dataset\n\n\n\n\nName\n\n\nBirth Year\n\n\n\n\n\n\nLéonide Azar\n\n\n1900\n\n\n\n\n\nNext we aim to find the TV Episode with a perfect 10/10 rating and 200,000 IMDb ratings. To do so, we will need to access the TITLE_RATINGS, TITLE_BASICS, and TITLE_EPISODES tables. (Q3)\nThe code below filters the TITLE_RATINGS table for the episode with a 10/10 rating and at least 200,000 IMDb ratings.\nIt then joins this filtered set of the TITLE_RATINGS table with the TITLE_BASICS table to access the show’s title and series. “tconst” is the unique identifier for the episode’s title we use to join. We use an inner join, so only rows that match in both tables are included.\nThe second inner_join joins the result of the first join to TITLE_EPISODES. This is to be access the series name of the episode.\nThe final inner_join with TITLE_BASICS retrieves the series name by matching the “parentTconst” of the episode to “tconst” of the series in the TITLE_BASICS table.\nWe then use the select function to select only the relevant columns for our dataframe. We finally rename these columns to allow for more descriptive column names.\n\n\nCode\nperfect_episode &lt;- TITLE_RATINGS |&gt;\n  filter(averageRating == 10, numVotes &gt;= 200000) |&gt;\n  inner_join(\n    TITLE_BASICS |&gt; filter(titleType == \"tvEpisode\"), \n    by = \"tconst\"\n  ) |&gt;\n  inner_join(TITLE_EPISODES, by = \"tconst\") |&gt;\n  inner_join(\n    TITLE_BASICS |&gt; filter(titleType == \"tvSeries\"), \n    by = c(\"parentTconst\" = \"tconst\")\n  ) |&gt;\n  select(primaryTitle.x, primaryTitle.y, titleType.x, averageRating, numVotes)\n\nperfect_episode &lt;- perfect_episode |&gt;\n  rename(\n    episodeTitle = primaryTitle.x,\n    seriesTitle = primaryTitle.y,\n    episodeType = titleType.x\n  )\n\n\nThe below code creates a summary table of our results.\n\nCode\nperfect_episode_tb &lt;- perfect_episode |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Highly Rated TV Episode\"\n  ) |&gt;\n  cols_label(\n    episodeTitle = \"Episode Title\",\n    seriesTitle = \"Series Title\",\n    episodeType = \"Type\",\n    averageRating = \"Rating\",\n    numVotes = \"# Votes\"\n  ) |&gt;\n  fmt_number(\n    columns = c(averageRating, numVotes),\n    decimals = 0\n  )\n\n\nprint(perfect_episode_tb)\n\n\n\n\n\n\n\nHighly Rated TV Episode\n\n\n\n\nEpisode Title\n\n\nSeries Title\n\n\nType\n\n\nRating\n\n\n# Votes\n\n\n\n\n\n\nOzymandias\n\n\nBreaking Bad\n\n\ntvEpisode\n\n\n10\n\n\n227,589\n\n\n\n\n\nWe find the TV episode with a perfect 10/10 rating and at least 200,000 IMDb ratings is the episode titled “Ozymandias” in the “Breaking Bad” series.\nMoving on to learning more about the actor Mark Hamill. The code below finds the 4 most popular projects Mark Hamill worked on. The code filters the NAME_BASICS table to only Mark Hamill, then splits the knownForTitles columns into separate rows for each title, then joins to the TITLE_BASICS table to get project details. We then arrange the numVotes in descending order and take the top 4 using a slice function. We select the title and type for the final output of his projects. (Q4)\n\nCode\nmark_hamill_projects &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\") |&gt;\n  separate_rows(knownForTitles, sep = \",\") |&gt;\n  inner_join(TITLE_BASICS, by = c(\"knownForTitles\" = \"tconst\")) |&gt;\n  select(primaryTitle, titleType)\n\nmark_hamill_projects_tb &lt;- mark_hamill_projects |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Mark Hamill's Most Known Projects\"\n  ) |&gt;\n  cols_label(\n    primaryTitle = \"Project Title\",\n    titleType = \"Type\"\n  )\n\n\nmark_hamill_projects_tb\n\n\n\n\n\n\n\nMark Hamill's Most Known Projects\n\n\nProject Title\nType\n\n\n\n\nStar Wars: Episode IV - A New Hope\nmovie\n\n\nStar Wars: Episode VIII - The Last Jedi\nmovie\n\n\nStar Wars: Episode V - The Empire Strikes Back\nmovie\n\n\nStar Wars: Episode VI - Return of the Jedi\nmovie\n\n\n\n\n\nNext, we move on to answer the next question of: What TV series, with more than 12 episodes, has the highest average rating? (Q5)\nFirst, we filter the TITLE_BASICS table to only get TV series:\n\n\nCode\ntv_series &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\")\n\n\nThen, filter the TITLE_EPISODES table to be grouped by series and filtered on series with more than 12 episodes:\n\n\nCode\nseries_episodes_filtered &lt;- TITLE_EPISODES |&gt;\n  group_by(parentTconst) |&gt;\n  summarize(numEpisodes = n()) |&gt;\n  filter(numEpisodes &gt; 12)\n\n\nJoining the filtered TITLE_EPISODES table with the TITLE_RATINGS table to calculate average rating by series:\n\n\nCode\nseries_avg_ratings &lt;- series_episodes_filtered |&gt;\n  inner_join(TITLE_RATINGS, by = c(\"parentTconst\" = \"tconst\")) |&gt;\n  group_by(parentTconst) |&gt;\n  summarize(avgRating = mean(averageRating, na.rm = TRUE), numEpisodes = first(numEpisodes)) |&gt;\n  arrange(desc(avgRating)) |&gt;\n  slice(1) #series with highest avg rating\n\n\nJoining the series with the highest average rating back with TITLE_BASICS to access the series name:\n\n\nCode\nhighest_rated_series &lt;- series_avg_ratings |&gt;\n  inner_join(tv_series, by = c(\"parentTconst\" = \"tconst\")) |&gt;\n  select(primaryTitle, avgRating, numEpisodes)\n\n\nCreating a table to display the results from the 4 steps of code above:\n\nCode\nhighest_rated_series_tb &lt;- highest_rated_series |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Highest Rated TV Series with More Than 12 Episodes\"\n  ) |&gt;\n  cols_label(\n    primaryTitle = \"Series Title\",\n    avgRating = \"Average Rating\",\n    numEpisodes = \"Number of Episodes\"\n  ) |&gt;\n  fmt_number(\n    columns = c(avgRating),\n    decimals = 1\n  )\n\n#displaying table \nhighest_rated_series_tb\n\n\n\n\n\n\n\nHighest Rated TV Series with More Than 12 Episodes\n\n\nSeries Title\nAverage Rating\nNumber of Episodes\n\n\n\n\nCraft Games\n9.7\n318\n\n\n\n\n\nMoving on to the final question of Task 2, regarding the TV series Happy Days (1974-1984) (Q6)\nFirst, filtering to get only the “Happy Days” TV series from the TITLE_BASICS table.\n\n\nCode\nhappy_days_series &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == \"Happy Days\" & titleType == \"tvSeries\")\n\n\nNext, joining with TITLE_EPISODES to get episode data for “Happy Days”.\n\n\nCode\nhappy_days_episodes &lt;- TITLE_EPISODES |&gt; \n  inner_join(happy_days_series, by =  c(\"parentTconst\" = \"tconst\"))\n\n\nPerforming another join with TITLE_RATINGS to get the ratings for each Happy Days episode.\n\n\nCode\nhappy_days_ratings &lt;- happy_days_episodes |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\")\n\n\nGrouping by season and calculating the average rating for each season.\n\n\nCode\nhappy_days_avg_ratings &lt;- happy_days_ratings |&gt;\n  group_by(seasonNumber) |&gt;\n  summarize(avgRating = mean(averageRating, na.rm = TRUE))\n\n\nFor this question, a bar chart will be a great way to visualize our results. The below code creates a bar chart of average ratings by season, using the previously downloaded library ggplot2.\n\n\nCode\nggplot(happy_days_avg_ratings, aes(x = as.factor(seasonNumber), y = avgRating)) + geom_bar(stat = \"identity\", fill = \"steelblue\") + labs(\n  title = \"Average Ratings by Season for Happy Days\",\n  x = \"Season Number\",\n  y = \"Average Rating\"\n) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of the following section is to define a success metric for movies, given only IMDb ratings.\n\n\nIn creating a custom success metric, important metrics to consider are both the quality and popularity of a movie. Quality could be reflected by average IMDb rating and popularity could be reflected by the number of IMDb votes.\nWe can combine these two metric into a Success Metric formula as follows:\nSuccess Metric = averageRating x log(numVotes)\nNote: We use the logarithm of the number of votes to prevent movies with a large number of votes from skewing the results due to their popularity. The logarithm serves as a method to normalize the the scale of votes.\n\nCreating success metric:\n\n\n\nCode\n#adding success metric to TITLE_RATINGS table\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt; \n  mutate(success_metric = averageRating * log1p(numVotes))\n\n# taking a peek at updated TITLE_RATINGS table \nglimpse(TITLE_RATINGS)\n\n\nRows: 372,198\nColumns: 4\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt…\n$ averageRating  &lt;dbl&gt; 5.7, 5.6, 6.5, 5.4, 6.2, 5.0, 5.4, 5.4, 5.4, 6.8, 5.2, …\n$ numVotes       &lt;dbl&gt; 2090, 283, 2094, 184, 2828, 196, 889, 2233, 214, 7699, …\n$ success_metric &lt;dbl&gt; 43.57877, 31.63426, 49.70751, 28.18992, 49.27561, 26.41…\n\n\n\nValidating success metric:\n\n\nHighest Success Metric Top 10 Movies\nUsing a bar chart to display the top 10 movies based on the custom success metric.\n\n\n\nCode\n#top 10 movies by success metric\ntop_10_movies &lt;- TITLE_RATINGS |&gt;\n  inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\") |&gt;\n  arrange(desc(success_metric)) |&gt;\n  slice(1:10) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric)\n\n#bar chart to visualize top 10 movies\nggplot(top_10_movies, aes(x = reorder(primaryTitle, success_metric), y = success_metric)) + geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Movies by Success Metric\",\n    x = \"Movie Title\",\n    y = \"Success Metric\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nMovies with High Votes but Low Success Scores\nUsing a scatter plot to display 5 movies with a large number of votes but poor success scores.\n\n\nCode\n#grabbing movies with high votes and low success scores\nlow_success_movies &lt;- TITLE_RATINGS |&gt;\n  filter(numVotes &gt; 100000) |&gt;\n  arrange(success_metric) |&gt;\n  slice(1:5) |&gt;\n  inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric)\n\n#creating scatter plot to visualize \nggplot(low_success_movies, aes(x = numVotes, y = success_metric)) + geom_point(size=3, color=\"red\") + geom_text(aes(label = primaryTitle), vjust = -1) + labs(\n  title = \"Movies with High Votes and Low Success Scores\",\n  x= \"Number of Votes\",\n  y=\"Success Metric\"\n) +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\nPrestigious Actor Check\n\nIn this validation method, the modeler selects a prestigious actor and confirms they have many projects with high success scores based on the defined success metric.\nChosen Actor: Robert De Niro\nThe code below walks us through the steps taken to get to Robert De Niro’s projects and their ratings.\nFirst, filter for Robert De Niro in the NAME_BASICS table.\nNext, join with the TITLE_BASICS on the knownForTitles column, after splitting this column.\nThen, join with TITLE_RATINGS to get ratings for the projects.\n\n\nCode\nde_niro_titles &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Robert De Niro\") |&gt;\n  separate_rows(knownForTitles, sep = \",\")\n\nprint(de_niro_titles)\n\n\n# A tibble: 4 × 6\n  nconst    primaryName    birthYear deathYear primaryProfession  knownForTitles\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;         \n1 nm0000134 Robert De Niro      1943        NA actor,producer,di… tt0101540     \n2 nm0000134 Robert De Niro      1943        NA actor,producer,di… tt0081398     \n3 nm0000134 Robert De Niro      1943        NA actor,producer,di… tt0077416     \n4 nm0000134 Robert De Niro      1943        NA actor,producer,di… tt0075314     \n\n\n\n\nCode\nde_niro_basic_join &lt;- de_niro_titles |&gt;\n  inner_join(TITLE_BASICS, by = c(\"knownForTitles\" = \"tconst\"))\n\nglimpse(de_niro_basic_join)\n\n\nRows: 4\nColumns: 14\n$ nconst            &lt;chr&gt; \"nm0000134\", \"nm0000134\", \"nm0000134\", \"nm0000134\"\n$ primaryName       &lt;chr&gt; \"Robert De Niro\", \"Robert De Niro\", \"Robert De Niro\"…\n$ birthYear         &lt;dbl&gt; 1943, 1943, 1943, 1943\n$ deathYear         &lt;dbl&gt; NA, NA, NA, NA\n$ primaryProfession &lt;chr&gt; \"actor,producer,director\", \"actor,producer,director\"…\n$ knownForTitles    &lt;chr&gt; \"tt0101540\", \"tt0081398\", \"tt0077416\", \"tt0075314\"\n$ titleType         &lt;chr&gt; \"movie\", \"movie\", \"movie\", \"movie\"\n$ primaryTitle      &lt;chr&gt; \"Cape Fear\", \"Raging Bull\", \"The Deer Hunter\", \"Taxi…\n$ originalTitle     &lt;chr&gt; \"Cape Fear\", \"Raging Bull\", \"The Deer Hunter\", \"Taxi…\n$ isAdult           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE\n$ startYear         &lt;dbl&gt; 1991, 1980, 1978, 1976\n$ endYear           &lt;dbl&gt; NA, NA, NA, NA\n$ runtimeMinutes    &lt;dbl&gt; 128, 129, 183, 114\n$ genres            &lt;chr&gt; \"Crime,Thriller\", \"Biography,Drama,Sport\", \"Drama,Wa…\n\n\n\n\nCode\nde_niro_basic_join &lt;- de_niro_basic_join |&gt;\n  rename(tconst = knownForTitles)\n\nde_niro_success &lt;- de_niro_basic_join |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  arrange(desc(success_metric)) |&gt;\n  select(primaryTitle, titleType, averageRating, numVotes, success_metric)\n\nglimpse(de_niro_success)\n\n\nRows: 4\nColumns: 5\n$ primaryTitle   &lt;chr&gt; \"Taxi Driver\", \"Raging Bull\", \"The Deer Hunter\", \"Cape …\n$ titleType      &lt;chr&gt; \"movie\", \"movie\", \"movie\", \"movie\"\n$ averageRating  &lt;dbl&gt; 8.2, 8.1, 8.1, 7.3\n$ numVotes       &lt;dbl&gt; 940583, 385456, 367022, 220649\n$ success_metric &lt;dbl&gt; 112.78490, 104.18370, 103.78676, 89.82163\n\n\n\nCode\n#creating a table to display results above\nde_niro_table &lt;- de_niro_success |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Robert De Niro's Top 4 Projects by Success Metric\"\n  ) |&gt;\n  fmt_number(\n    columns = c(averageRating, numVotes, success_metric),\n    decimals = 2\n  )\n\n#printing table\nde_niro_table\n\n\n\n\n\n\n\nRobert De Niro's Top 4 Projects by Success Metric\n\n\nprimaryTitle\ntitleType\naverageRating\nnumVotes\nsuccess_metric\n\n\n\n\nTaxi Driver\nmovie\n8.20\n940,583.00\n112.78\n\n\nRaging Bull\nmovie\n8.10\n385,456.00\n104.18\n\n\nThe Deer Hunter\nmovie\n8.10\n367,022.00\n103.79\n\n\nCape Fear\nmovie\n7.30\n220,649.00\n89.82\n\n\n\n\n\nWe use a scatterplot to display the averageRating vs Success Metric. The trend line is included to help highlight any linear relationship between these two variables.\n\n\nCode\nggplot(de_niro_success, aes(x = averageRating, y = success_metric)) +\n  geom_point(size = 3, color = \"blue\") +  # Scatter plot points\n  geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\") +  # Add a trend line (linear regression)\n  labs(\n    title = \"Robert De Niro's Projects: IMDb Rating vs Success Metric\",\n    x = \"IMDb Average Rating\",\n    y = \"Success Metric\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe find Robert De Niro’s projects with the highest success rating also have a high success score, which further validates the chosen success metric.\n\nFurther Validation\nIn an effort to continue validating our chosen success metric, we explore the use of scatterplots to analyze the relationship between IMDb rating and success metric score.\n\n\n\nCode\nall_movies_success &lt;- TITLE_RATINGS |&gt;\n  inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\")\n\nggplot(all_movies_success, aes(x = averageRating, y = success_metric)) +\n  geom_point(color = \"blue\", size = 2, alpha = 0.5) +  # All movie points\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE, linetype = \"dashed\") +  # Trend line\n  labs(x = \"IMDb Average Rating\", y = \"Success Metric\", title = \"Scatterplot of Success Metric vs IMDb Average Rating (All Movies)\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 14),\n    axis.title.y = element_text(size = 14)\n  )\n\n\n\n\n\n\n\n\n\nApplying jittering to make the scatterplot slightly more readable.\n\n\nCode\nggplot(all_movies_success, aes(x = averageRating, y = success_metric)) +\n  geom_jitter(color = \"blue\", size = 1.5, alpha = 0.3, width = 0.2, height = 0.2) +  # Jitter points\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE, linetype = \"dashed\") +  # Trend line\n  labs(x = \"IMDb Average Rating\", y = \"Success Metric\", title = \"Scatterplot of Success Metric vs IMDb Average Rating (All Movies)\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 14),\n    axis.title.y = element_text(size = 14)\n  )\n\n\n\n\n\n\n\n\n\nCleaning dataset to eliminate outliers, set a threshold for the minimum number of votes, and remove NAs. This allows for a smaller dataset to hopefully improve the visibility of our scatterplot.\n\n\nCode\nmin_votes_threshold &lt;- 1000  # You can adjust this value based on your needs\n\nall_movies_filtered &lt;- all_movies_success |&gt;\n  filter(!is.na(averageRating) & !is.na(numVotes)) |&gt;  # Remove rows with missing values\n  filter(numVotes &gt;= min_votes_threshold) |&gt;  # Remove movies with too few votes\n  filter(averageRating &gt;= 2 & averageRating &lt;= 9.5) |&gt; # Remove movies with extreme ratings\n  filter(!is.na(startYear) & genres != \"\\\\N\")\n\nggplot(all_movies_filtered, aes(x = averageRating, y = success_metric)) +\n  geom_jitter(color = \"blue\", size = 1.5, alpha = 0.3, width = 0.2, height = 0.2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE, linetype = \"dashed\") +\n  labs(x = \"IMDb Average Rating\", y = \"Success Metric\", title = \"Scatterplot of Success Metric vs IMDb Average Rating (Filtered Movies)\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 14),\n    axis.title.y = element_text(size = 14)\n  )\n\n\n\n\n\n\n\n\n\nGiven the volume of movies in our dataset, this only slightly improved the visibility of the scatterplot. In all three versions, the correlation is clear, movies with a higher average rating tend to have a higher success metric.\nTo further validate the custom success metric, we find the correlation coefficient between the IMDb average rating and the success metric. This measures the linear relationship between the two variables.\n\nCode\ncorrelation &lt;- cor(all_movies_filtered$averageRating, all_movies_filtered$success_metric, use = \"complete.obs\")\n\ncorrelation_df &lt;- data.frame(\n  x = \"IMDb Average Rating\",\n  y = \"Success Metric\",\n  Correlation_Coefficient = sprintf(\"%.2f\", correlation)\n)\n\ncorrelation_table &lt;- correlation_df |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Correlation between IMDb Average Rating and Success Metric\"\n  )\n\nprint(correlation_table)\n\n\n\n\n\n\n\nCorrelation between IMDb Average Rating and Success Metric\n\n\n\n\nx\n\n\ny\n\n\nCorrelation_Coefficient\n\n\n\n\n\n\nIMDb Average Rating\n\n\nSuccess Metric\n\n\n0.78\n\n\n\n\n\nThe calculated correlation coefficient of 0.78 indicates a strong positive correlation between the IMDb average rating and the success metric. This translates to, movies with higher IMDb ratings also tend to score higher on the success metric, which is the trend we can visualize in the scatterplots above. This further supports and validates the use of the success metric.\n\nDefine a Numerical Threshold for Successful Project\nEstablishing a numerical threshold for the custom success metric requires a few steps.\n1) Analyzing the distribution of the success metric\n\nCode\nsuccess_summary &lt;- all_movies_filtered |&gt;\n  summarise(\n    Min = round(min(success_metric),2),\n    Q1 = round(quantile(success_metric, 0.25),2),\n    Median = round(median(success_metric),2),\n    Q3 = round(quantile(success_metric, 0.75),2),\n    Max = round(max(success_metric),2),\n    Mean = round(mean(success_metric),2),\n    SD = round(sd(success_metric),2)\n  )\n\nsuccess_summary_table &lt;- success_summary |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Summary Statistics for Success Metric\"\n  )\n\nprint(success_summary_table)\n\n\n\n\n\n\n\nSummary Statistics for Success Metric\n\n\n\n\nMin\n\n\nQ1\n\n\nMedian\n\n\nQ3\n\n\nMax\n\n\nMean\n\n\nSD\n\n\n\n\n\n\n14.17\n\n\n44.38\n\n\n52.31\n\n\n62.06\n\n\n138.52\n\n\n53.91\n\n\n15.24\n\n\n\n\n\n\n\nVisualizing the Success Metric\n\n\n\nCode\nggplot(all_movies_filtered, aes(x = \"\", y = success_metric)) +\n  geom_boxplot() +\n  labs(\n    title = \"Boxplot of Success Metric for All Movies\",\n    y = \"Success Metric\"\n  )\n\n\n\n\n\n\n\n\n\nThe box plot helps visualize the distribution of the success metric and outliers above the upper quartile. A majoriy of the success metrics lie between 50 and 75, with outliers above 100.\n\n\nCode\nggplot(all_movies_filtered, aes(x = success_metric)) +\n  geom_histogram(bins = 30, fill = \"blue\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of Success Metric for All Movies\",\n    x = \"Success Metric\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\nThe histogram of the success metric shows a right-skewed distribution, with the majority of movies clustered around a success metric of 50 to 75. Another observation includes the significant number of high-performing movies in the dataset, as we can see from the long tail extending towards higher success values.\n\nDefining the Success Threshold\n\nWe can pick a threshold at the third quartile, Q3, given the top 25% of movies fall above this threshold. By setting this threshold, we ensure only movies that are significantly better than the median are classified as “solid” or better.\nOur success threshold chosen is our Q3 value of 62.06.\n\n\n\n\n\n\nThe analysis may be broken down by decades and genres.\nWe start with categorizing movies by decade. The code below creates a new column that groups movies by the decade they were released, through the use of the startYear column. We add a decade column through the use of the mutate function.\n\n\nCode\nall_movies_filtered &lt;- all_movies_filtered |&gt; \n  mutate(decade = floor(startYear / 10) * 10)\n\n\nMoving on to grouping by genres. The code below splits the genres column into separate rows so movies can be individually analyzed by each genre.\n\n\nCode\nall_movies_filtered &lt;- all_movies_filtered |&gt;\n  separate_rows(genres, sep=\",\")\n\n\nWe then filter the movies that are considered successful. We define our success_threshold as the Q3 value, or 62.05, found in the previous task.\n\n\nCode\nsuccess_threshold &lt;- 62.05 \nsuccessful_movies &lt;- all_movies_filtered |&gt;\n  filter(success_metric &gt;= success_threshold)\n\n\nThen group by genre and decade, and count the number of successes.\n\n\nCode\nsuccess_by_genre_decade &lt;- successful_movies |&gt;\n  group_by(genres, decade) |&gt;\n  summarize(success_count=n(), .groups=\"drop\")\n\n\nLet’s take a look at few different views of our trends.\nLine Plot showing Success Over Time by Genre:\nEach line in the plot below represents the number of successful movies for a given genre in each decade, as defined by the custom success metric, filtered above the 62.05 threshold, showing only “successful” movies.\n\n\nCode\nggplot(success_by_genre_decade, aes(x = decade, y = success_count, color = genres)) +\n  geom_line(size = 1) +\n  labs(title = \"Trends in Movie Success by Genre and Decade\",\n       x = \"Decade\", y = \"Number of Successful Movies\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nObservations on the above:\n\nDocumentaries appear to dominate the successful movie space, while seeing a sharp rise from the 1980s to early 2000’s, and then a significant decline to follow\nNotable increase in biography popularity, with a peak in the early 2000’s\nSuccess in the early 2000’s, as we see most of the genres peak around 2010\n\nBar Chart showing Success by Genre since 2010:\n\n\nCode\nsuccess_2010s &lt;- success_by_genre_decade |&gt; \n  filter(decade &gt;= 2010)\n\nggplot(success_2010s, aes(x = reorder(genres, success_count), y = success_count, fill = genres)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Successful Movies by Genre (Post-2010)\",\n       x = \"Genre\", y = \"Number of Successful Movies\") +\n  coord_flip() +  # Flip the axis to make it easier to read\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhen we take a look at genre success following the year 2010, we see drama dominating the field, followed by comedy, action, then crime.\nWe find that documentaries, while extremely popular before 2010, are quite low on the list in comparison to other genres.\nWith these results, the chosen genre for the next project, which will be used in the following task, is Action.\n\n\n\n\n\n\nThe chosen genre is Action. we start with filtering only on Action movies. Rather than filtering from the TITLE_BASICS table, we filter from our cleaned dataset of movies, all_movies_filtered.\nWe join our dataset of cleaned movies data, filtered to only action movies, with the TITLE_PRINCIPALS table to get the principals, which will be actors and directors, in our case.\nThis dataset is then joined to the NAME_BASICS table, so we can later access the names of the actors and directors.\n\n\nCode\naction_movies_filtered &lt;- successful_movies |&gt;\n  separate_rows(genres, sep = \",\") |&gt;\n  filter(genres == \"Action\")\n\naction_principals &lt;- TITLE_PRINCIPALS |&gt;\n  inner_join(action_movies_filtered, by = \"tconst\")\n\naction_personnel &lt;- action_principals |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\")\n\n\nThe code below accesses directors and leads to creating and displaying a table of the top 5 directors, who have worked on action movies, in our dataset.\n\nCode\ndirectors &lt;- action_personnel |&gt;\n  filter(category == \"director\")\n\ndirector_success &lt;- directors |&gt;\n  group_by(nconst, primaryName) |&gt;\n  summarize(\n    success_count = n(),\n    avg_success_metric = round(mean(success_metric, na.rm = TRUE),2),\n    known_for_titles = first(knownForTitles)\n  ) |&gt;\n  arrange(desc(avg_success_metric)) |&gt;\n  ungroup() |&gt;\n  slice_head(n=5)\n\ndirectors_known_for &lt;- director_success |&gt;\n  separate_rows(known_for_titles, sep = \",\") |&gt;\n  inner_join(TITLE_BASICS, by = c(\"known_for_titles\" = \"tconst\")) |&gt;\n  group_by(primaryName) |&gt;\n  summarize(\n    success_count = first(success_count),\n    avg_success_metric = first(avg_success_metric),\n    known_for_titles = paste(primaryTitle, collapse = \", \")\n  )\n\ndirector_table &lt;- directors_known_for |&gt;\n  slice_head(n = 5) |&gt;\n  select(primaryName, success_count, avg_success_metric, known_for_titles) |&gt;\n  gt() |&gt;\n  tab_header(title = \"Top 5 Action Directors\") |&gt;\n  cols_label(\n    primaryName = \"Director\",\n    success_count = \"Number of Successful Movies\",\n    avg_success_metric = \"Average Success Metric\",\n    known_for_titles = \"Known For Titles\"\n  )\n\nprint(director_table)\n\n\n\n\n\n\n\nTop 5 Action Directors\n\n\n\n\nDirector\n\n\nNumber of Successful Movies\n\n\nAverage Success Metric\n\n\nKnown For Titles\n\n\n\n\n\n\nBob Persichetti\n\n\n1\n\n\n112.97\n\n\nSpider-Man: Into the Spider-Verse, Spider-Man: Across the Spider-Verse, The Little Prince, Puss in Boots: The Last Wish\n\n\n\n\nChristopher Nolan\n\n\n6\n\n\n117.52\n\n\nTenet, Interstellar, Inception, The Prestige\n\n\n\n\nPeter Jackson\n\n\n4\n\n\n120.14\n\n\nThe Lord of the Rings: The Fellowship of the Ring, Bad Taste, The Lord of the Rings: The Return of the King, The Lord of the Rings: The Two Towers\n\n\n\n\nRichard Marquand\n\n\n1\n\n\n115.74\n\n\nStar Wars: Episode VI - Return of the Jedi, Nowhere to Run, Jagged Edge\n\n\n\n\nRodney Rothman\n\n\n1\n\n\n112.97\n\n\nSpider-Man: Into the Spider-Verse, Spider-Man: Across the Spider-Verse, Forgetting Sarah Marshall, 22 Jump Street\n\n\n\n\n\nThe chosen director is Peter Jackson. We see he’s had 4 successful movies and one of the highest success metrics.\nThe code below accesses actors and leads to creating and displaying a table of the top 5 actors, who have worked on action movies, in our dataset.\n\nCode\nactors &lt;- action_personnel |&gt;\n  filter(category == \"actor\" | category == \"actress\") |&gt;\n  group_by(nconst, primaryName) |&gt;\n  summarize(success_count=n(), avg_success_metric = round(mean(success_metric),2), known_for_titles = first(knownForTitles)) |&gt;\n  arrange(desc(avg_success_metric)) |&gt;\n  ungroup() |&gt;\n  slice_head(n=5)\n\nactors_known_for &lt;- actors |&gt;\n  separate_rows(known_for_titles, sep = \",\") |&gt;\n  inner_join(TITLE_BASICS, by = c(\"known_for_titles\" = \"tconst\")) |&gt;\n  group_by(primaryName) |&gt;\n  summarize(\n    success_count = first(success_count),\n    avg_success_metric = first(avg_success_metric),\n    known_for_titles = paste(primaryTitle, collapse = \", \")\n  )\n\nactor_table &lt;- actors_known_for |&gt;\n  slice_head(n = 5) |&gt;\n  gt() |&gt;\n  tab_header(title = \"Top 5 Action Actors\") |&gt;\n  cols_label(\n    primaryName = \"Actor\",\n    success_count = \"Number of Successful Movies\",\n    avg_success_metric = \"Average Success Metric\",\n    known_for_titles = \"Known For Titles\"\n  )\n\nactor_table\n\n\n\n\n\n\n\nTop 5 Action Actors\n\n\nActor\nNumber of Successful Movies\nAverage Success Metric\nKnown For Titles\n\n\n\n\nAlan Howard\n1\n129.32\nThe Lord of the Rings: The Fellowship of the Ring, The Cook, the Thief, His Wife & Her Lover, The Lord of the Rings: The Return of the King\n\n\nAli Astin\n1\n130.64\nThe Lord of the Rings: The Return of the King, Bad Kids of Crestview Academy\n\n\nDileep Rao\n1\n129.97\nInception, Avatar, Drag Me to Hell, Avatar: The Way of Water\n\n\nMonique Gabriela Curnen\n1\n133.99\nThe Dark Knight, Half Nelson, Contagion, Fast & Furious\n\n\nNoel Appleby\n2\n129.98\nThe Lord of the Rings: The Fellowship of the Ring, The Lord of the Rings: The Return of the King, The Navigator: A Medieval Odyssey\n\n\n\n\n\nThe table above displays the top 4 actors involved in action movies, by highest average success scores.\nThe two chosen actors are Noel Appleby and Alan Howard, given they’ve worked together on a successful film, The Lord of the Rings: The Fellowship of the Ring, and have each seen success in a few other known films throughout their career. They go along well with the chosen director, who was the director of The Lord of the Rings.\nThe team has known success as we can see from their metrics displayed in the corresponding tables and visualized below. The three have worked together in the past, on a movie which was deemed successful, making this team more reliable.\n\n\nCode\nselected_personnel &lt;- bind_rows(\n  actors_known_for |&gt; select(primaryName, avg_success_metric) |&gt; mutate(type = \"Actor\"),\n  directors_known_for |&gt; select(primaryName, avg_success_metric) |&gt; mutate(type = \"Director\")\n)\n\nggplot(selected_personnel, aes(x = reorder(primaryName, avg_success_metric), y = avg_success_metric, fill = type)) +\n  geom_bar(stat = \"identity\", show.legend = TRUE) +\n  coord_flip() +\n  labs(\n    title = \"Success Metrics for Selected Actors and Directors\",\n    x = \"Personnel\",\n    y = \"Average Success Metric\"\n  ) +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"Actor\" = \"skyblue\", \"Director\" = \"orange\")) +\n  geom_text(aes(label = round(avg_success_metric, 2)), hjust = -0.1) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this task, our goal is to identify a classic action movie to remake. This movie is to have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.\nThe code below focuses on successful action movies released before 1999, sorted by the custom success metric, IMDb rating, and vote count.\n\n\nCode\nclassic_successful_action_movies &lt;- successful_movies |&gt;\n  separate_rows(genres, sep = \",\") |&gt;\n  filter(genres == \"Action\" & startYear &lt; 1999)\n\nclassic_successful_action_movies_sorted &lt;- classic_successful_action_movies |&gt;\n  arrange(desc(success_metric), desc(averageRating), desc(numVotes))\n\nclassic_successful_action_movies_sorted &lt;- classic_successful_action_movies_sorted |&gt;\n  select(primaryTitle, startYear, averageRating, numVotes, success_metric)\n\ntop_classic_successful_action_movies &lt;- classic_successful_action_movies_sorted |&gt; \n  slice_head(n = 10)\n\nprint(top_classic_successful_action_movies)\n\n\n# A tibble: 10 × 5\n   primaryTitle                  startYear averageRating numVotes success_metric\n   &lt;chr&gt;                             &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n 1 Star Wars: Episode V - The E…      1980           8.7  1401591           123.\n 2 Star Wars: Episode IV - A Ne…      1977           8.6  1471222           122.\n 3 Terminator 2: Judgment Day         1991           8.6  1194496           120.\n 4 Léon: The Professional             1994           8.5  1266394           119.\n 5 Raiders of the Lost Ark            1981           8.4  1049518           116.\n 6 Star Wars: Episode VI - Retu…      1983           8.3  1137692           116.\n 7 Aliens                             1986           8.4   784536           114.\n 8 Jurassic Park                      1993           8.2  1086595           114.\n 9 Die Hard                           1988           8.2   958730           113.\n10 Heat                               1995           8.3   731857           112.\n\n\nThe output above displays the top 10 successful classic action movies, as defined above.\nThe chosen classic movie is Aliens. This movie has not been remake in the past 25 years, has an 8.4 average IMDb rating, a 114.01 custom success rating, and over 78,000 votes.\nThese key details on the movie are pulled from the code below, and displayed in the table output.\n\nCode\n# Filter for the movie \"Aliens\"\naliens_info &lt;- top_classic_successful_action_movies |&gt;\n  filter(primaryTitle == \"Aliens\") |&gt;\n  select(primaryTitle, averageRating, numVotes, startYear, success_metric)\n\n# Create a gt table to display the details for \"Aliens\"\naliens_gt_table &lt;- aliens_info |&gt;\n  gt() |&gt;\n  tab_header(title = \"Details for the Movie: Aliens\") |&gt;\n  cols_label(\n    primaryTitle = \"Movie Title\",\n    averageRating = \"IMDb Rating\",\n    numVotes = \"Number of Votes\",\n    startYear = \"Release Year\",\n    success_metric = \"Success Metric\"\n  ) |&gt;\n  fmt_number(\n    columns = c(averageRating, success_metric),\n    decimals = 2\n  ) |&gt;\n  fmt_number(\n    columns = numVotes,\n    use_seps = TRUE,\n    decimals = 0\n  )\n\n# Print the table\nprint(aliens_gt_table)\n\n\n\n\n\n\n\nDetails for the Movie: Aliens\n\n\n\n\nMovie Title\n\n\nIMDb Rating\n\n\nNumber of Votes\n\n\nRelease Year\n\n\nSuccess Metric\n\n\n\n\n\n\nAliens\n\n\n8.40\n\n\n784,536\n\n\n1986\n\n\n114.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImagine an action-packed remake of the 1986 sci-fi classic Aliens, a film that has captivated audiences with its 8.4 IMDb rating, 784,536 votes, and a success metric of 114.01. We are breathing new life into this iconic movie with a fresh, bold vision.\nLeading the charge is Peter Jackson, the legendary director behind The Lord of the Rings series. With 4 highly successful films under his belt, and one of the highest success metrics in the industry, Peter Jackson’s vision is the perfect blend of spectacle and storytelling to make this Aliens remake a sure hit.\nWe’ve also assembled a powerhouse cast. Noel Appleby and Alan Howard, both having achieved success in The Lord of the Rings: The Fellowship of the Ring, bring an undeniable chemistry to the screen. Their performances, combined with Jackson’s directorial mastery, promise to elevate this project beyond a mere remake—it’s a revitalized epic that honors the original while pushing the boundaries of modern sci-fi action.\nWith proven success across the board—genre, director, and cast—this project is a guaranteed box office hit and a thrilling adventure for fans, old and new."
  },
  {
    "objectID": "mp02.html#initial-exploration",
    "href": "mp02.html#initial-exploration",
    "title": "mp02",
    "section": "",
    "text": "To better understand the structure of the data, we will use the glimpse function from the dplyr package to examine each table. This will provide insight into the number of columns, column names, and their respective data types for each dataset.\n\n\nCode\nglimpse(NAME_BASICS)\n\n\nRows: 2,460,608\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;chr&gt; \"1899\", \"1924\", \"1934\", \"1949\", \"1918\", \"1915\", \"189…\n$ deathYear         &lt;chr&gt; \"1987\", \"2014\", \"\\\\N\", \"1982\", \"2007\", \"1982\", \"1957…\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\nCode\nglimpse(TITLE_BASICS)\n\n\nRows: 372,198\nColumns: 9\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt…\n$ titleType      &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"…\n$ primaryTitle   &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ originalTitle  &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ isAdult        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ startYear      &lt;dbl&gt; 1894, 1892, 1892, 1892, 1893, 1894, 1894, 1894, 1894, 1…\n$ endYear        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\",…\n$ runtimeMinutes &lt;chr&gt; \"1\", \"5\", \"5\", \"12\", \"1\", \"1\", \"1\", \"1\", \"45\", \"1\", \"1\"…\n$ genres         &lt;chr&gt; \"Documentary,Short\", \"Animation,Short\", \"Animation,Come…\n\n\nCode\nglimpse(TITLE_EPISODES)\n\n\nRows: 3,007,178\nColumns: 4\n$ tconst        &lt;chr&gt; \"tt0045960\", \"tt0046855\", \"tt0048378\", \"tt0048562\", \"tt0…\n$ parentTconst  &lt;chr&gt; \"tt0044284\", \"tt0046643\", \"tt0047702\", \"tt0047768\", \"tt0…\n$ seasonNumber  &lt;chr&gt; \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"3\", \"3\", \"…\n$ episodeNumber &lt;chr&gt; \"3\", \"4\", \"6\", \"10\", \"4\", \"20\", \"5\", \"2\", \"20\", \"6\", \"2\"…\n\n\nCode\nglimpse(TITLE_RATINGS)\n\n\nRows: 372,198\nColumns: 3\n$ tconst        &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt0…\n$ averageRating &lt;dbl&gt; 5.7, 5.6, 6.5, 5.4, 6.2, 5.0, 5.4, 5.4, 5.4, 6.8, 5.2, 7…\n$ numVotes      &lt;dbl&gt; 2090, 283, 2094, 184, 2828, 196, 889, 2233, 214, 7699, 3…\n\n\nCode\nglimpse(TITLE_CREW)\n\n\nRows: 371,902\nColumns: 3\n$ tconst    &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt00000…\n$ directors &lt;chr&gt; \"nm0005690\", \"nm0721526\", \"nm0721526\", \"nm0721526\", \"nm00056…\n$ writers   &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"nm0…\n\n\nCode\nglimpse(TITLE_PRINCIPALS)\n\n\nRows: 6,586,689\nColumns: 6\n$ tconst     &lt;chr&gt; \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000…\n$ ordering   &lt;dbl&gt; 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 1, 2, 3, 4,…\n$ nconst     &lt;chr&gt; \"nm1588970\", \"nm0005690\", \"nm0005690\", \"nm0374658\", \"nm0721…\n$ category   &lt;chr&gt; \"self\", \"director\", \"producer\", \"cinematographer\", \"directo…\n$ job        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"producer\", \"director of photography\", \"\\\\N\",…\n$ characters &lt;chr&gt; \"[\\\"Self\\\"]\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\…"
  },
  {
    "objectID": "mp02.html#task-1-column-type-conversion",
    "href": "mp02.html#task-1-column-type-conversion",
    "title": "mp02",
    "section": "",
    "text": "Most columns appear to be read in as character(string) vectors, but should be numeric. The “null” values are represented as \\N, which R does not recognize as NA values.\nUsing the mutate and the as.numeric commands to change th types of columns.\nStarting with the NAME_BASICS table - taking character columns and converting to numeric, where needed.\n\n\nCode\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  mutate(birthYear = as.numeric(replace(birthYear, birthYear == \"\\\\N\", NA)),\n         deathYear = as.numeric(replace(deathYear, deathYear == \"\\\\N\", NA)))\n\n\nMoving on to the TITLE_BASICS table. For this one, we adjust two columns from character to numeric. One new fix we are making it taking the isAdult column and converting it from numeric (with values 0 or 1) and converting it to logical (TRUE/FALSE values). This way we will not have to remember how 0 and 1 are defined as TRUE/FALSE values.\n\n\nCode\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(\n    endYear = as.numeric(replace(endYear, endYear == \"\\\\N\", NA)),\n    runtimeMinutes = as.numeric(replace(runtimeMinutes, runtimeMinutes == \"\\\\N\", NA)),\n    isAdult = as.logical(isAdult) \n  )\n\n\nCleaning the TITLE_EPISODES table.\n\n\nCode\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n  mutate(\n    seasonNumber = as.numeric(replace(seasonNumber, seasonNumber == \"\\\\N\", NA)),\n    episodeNumber = as.numeric(replace(episodeNumber, episodeNumber == \"\\\\N\", NA))\n  )\n\n\nThe next table to clean would be the TITLE_RATINGS table. The three columns have the correct datatypes so this table is skipped.\nThe writers column in the TITLE_CREW table has several \\N values which need to be replaced with NA. We will maintain the writer column’s type. This is done in the step below.\n\n\nCode\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    mutate(writers = na_if(writers, \"\\\\N\"))\n\n\nThe last table left for cleaning is the TITLE_PRINCIPALS table. We will replace the \\N values in the job and characters column with NA, while maintaining the columns’ data type.\n\n\nCode\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    mutate(job = na_if(job, \"\\\\N\"),\n           characters = na_if(characters, \"\\\\N\"))"
  },
  {
    "objectID": "mp02.html#task-2-instructor-provided-questions",
    "href": "mp02.html#task-2-instructor-provided-questions",
    "title": "mp02",
    "section": "",
    "text": "The code below gives us a summary of the number of movies, TV series, and TV episodes in our dataset. (Q1)\nThe values are calculated using dplyr functionality, and the summary table is created using the previously downloaded gt package.\n\n\nCode\nnum_movies &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\") |&gt;\n  nrow()\n\nnum_tv_series &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  nrow()\n\nnum_tv_episodes &lt;- TITLE_EPISODES |&gt;\n  nrow()\n\n\n\n\nCode\n# Creating summary table\nsummary_table &lt;- data.frame(\n  'Movies' = num_movies,\n  'TV Series' = num_tv_series, \n  'TV Episodes' = num_tv_episodes\n)\n\n# Formatting summary table with gt\nsummary_gt &lt;- summary_table |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Total Movies, TV Series and Episodes\"\n  ) |&gt;\n  fmt_number(\n    columns = everything(),\n    decimals = 0\n  )\n\n# Display the table\nsummary_gt\n\n\n\n\n\n\n\n\nTotal Movies, TV Series and Episodes\n\n\nMovies\nTV.Series\nTV.Episodes\n\n\n\n\n131,662\n29,789\n3,007,178\n\n\n\n\n\n\n\nThe code below answers the question of who the oldest living person in our data set is. We first access the oldest living person from the NAME_BASICS dataset and then format the result into a table using gt. (Q2)\nTo access the oldest living person in the NAME_BASICS dataset, the code below filters to find the person with the maximum birthYear where deathYear is NA.\n\nCode\noldest_living_person &lt;- NAME_BASICS |&gt;\n  filter(!is.na(birthYear) & is.na(deathYear) & birthYear &gt;= 1900) |&gt;\n  arrange(birthYear) |&gt;\n  slice(1) |&gt;\n  select(primaryName, birthYear)\n\noldest_living_person_tb &lt;- oldest_living_person |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Oldest Living Person in Dataset\"\n  ) |&gt;\n  cols_label(\n    primaryName = \"Name\",\n    birthYear = \"Birth Year\"\n  ) |&gt;\n  fmt_number(\n    columns = c(birthYear),\n    decimals = 0,\n    use_seps = FALSE\n  )\n\nprint(oldest_living_person_tb)\n\n\n\n\n\n\n\nOldest Living Person in Dataset\n\n\n\n\nName\n\n\nBirth Year\n\n\n\n\n\n\nLéonide Azar\n\n\n1900\n\n\n\n\n\nNext we aim to find the TV Episode with a perfect 10/10 rating and 200,000 IMDb ratings. To do so, we will need to access the TITLE_RATINGS, TITLE_BASICS, and TITLE_EPISODES tables. (Q3)\nThe code below filters the TITLE_RATINGS table for the episode with a 10/10 rating and at least 200,000 IMDb ratings.\nIt then joins this filtered set of the TITLE_RATINGS table with the TITLE_BASICS table to access the show’s title and series. “tconst” is the unique identifier for the episode’s title we use to join. We use an inner join, so only rows that match in both tables are included.\nThe second inner_join joins the result of the first join to TITLE_EPISODES. This is to be access the series name of the episode.\nThe final inner_join with TITLE_BASICS retrieves the series name by matching the “parentTconst” of the episode to “tconst” of the series in the TITLE_BASICS table.\nWe then use the select function to select only the relevant columns for our dataframe. We finally rename these columns to allow for more descriptive column names.\n\n\nCode\nperfect_episode &lt;- TITLE_RATINGS |&gt;\n  filter(averageRating == 10, numVotes &gt;= 200000) |&gt;\n  inner_join(\n    TITLE_BASICS |&gt; filter(titleType == \"tvEpisode\"), \n    by = \"tconst\"\n  ) |&gt;\n  inner_join(TITLE_EPISODES, by = \"tconst\") |&gt;\n  inner_join(\n    TITLE_BASICS |&gt; filter(titleType == \"tvSeries\"), \n    by = c(\"parentTconst\" = \"tconst\")\n  ) |&gt;\n  select(primaryTitle.x, primaryTitle.y, titleType.x, averageRating, numVotes)\n\nperfect_episode &lt;- perfect_episode |&gt;\n  rename(\n    episodeTitle = primaryTitle.x,\n    seriesTitle = primaryTitle.y,\n    episodeType = titleType.x\n  )\n\n\nThe below code creates a summary table of our results.\n\nCode\nperfect_episode_tb &lt;- perfect_episode |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Highly Rated TV Episode\"\n  ) |&gt;\n  cols_label(\n    episodeTitle = \"Episode Title\",\n    seriesTitle = \"Series Title\",\n    episodeType = \"Type\",\n    averageRating = \"Rating\",\n    numVotes = \"# Votes\"\n  ) |&gt;\n  fmt_number(\n    columns = c(averageRating, numVotes),\n    decimals = 0\n  )\n\n\nprint(perfect_episode_tb)\n\n\n\n\n\n\n\nHighly Rated TV Episode\n\n\n\n\nEpisode Title\n\n\nSeries Title\n\n\nType\n\n\nRating\n\n\n# Votes\n\n\n\n\n\n\nOzymandias\n\n\nBreaking Bad\n\n\ntvEpisode\n\n\n10\n\n\n227,589\n\n\n\n\n\nWe find the TV episode with a perfect 10/10 rating and at least 200,000 IMDb ratings is the episode titled “Ozymandias” in the “Breaking Bad” series.\nMoving on to learning more about the actor Mark Hamill. The code below finds the 4 most popular projects Mark Hamill worked on. The code filters the NAME_BASICS table to only Mark Hamill, then splits the knownForTitles columns into separate rows for each title, then joins to the TITLE_BASICS table to get project details. We then arrange the numVotes in descending order and take the top 4 using a slice function. We select the title and type for the final output of his projects. (Q4)\n\nCode\nmark_hamill_projects &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\") |&gt;\n  separate_rows(knownForTitles, sep = \",\") |&gt;\n  inner_join(TITLE_BASICS, by = c(\"knownForTitles\" = \"tconst\")) |&gt;\n  select(primaryTitle, titleType)\n\nmark_hamill_projects_tb &lt;- mark_hamill_projects |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Mark Hamill's Most Known Projects\"\n  ) |&gt;\n  cols_label(\n    primaryTitle = \"Project Title\",\n    titleType = \"Type\"\n  )\n\n\nmark_hamill_projects_tb\n\n\n\n\n\n\n\nMark Hamill's Most Known Projects\n\n\nProject Title\nType\n\n\n\n\nStar Wars: Episode IV - A New Hope\nmovie\n\n\nStar Wars: Episode VIII - The Last Jedi\nmovie\n\n\nStar Wars: Episode V - The Empire Strikes Back\nmovie\n\n\nStar Wars: Episode VI - Return of the Jedi\nmovie\n\n\n\n\n\nNext, we move on to answer the next question of: What TV series, with more than 12 episodes, has the highest average rating? (Q5)\nFirst, we filter the TITLE_BASICS table to only get TV series:\n\n\nCode\ntv_series &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\")\n\n\nThen, filter the TITLE_EPISODES table to be grouped by series and filtered on series with more than 12 episodes:\n\n\nCode\nseries_episodes_filtered &lt;- TITLE_EPISODES |&gt;\n  group_by(parentTconst) |&gt;\n  summarize(numEpisodes = n()) |&gt;\n  filter(numEpisodes &gt; 12)\n\n\nJoining the filtered TITLE_EPISODES table with the TITLE_RATINGS table to calculate average rating by series:\n\n\nCode\nseries_avg_ratings &lt;- series_episodes_filtered |&gt;\n  inner_join(TITLE_RATINGS, by = c(\"parentTconst\" = \"tconst\")) |&gt;\n  group_by(parentTconst) |&gt;\n  summarize(avgRating = mean(averageRating, na.rm = TRUE), numEpisodes = first(numEpisodes)) |&gt;\n  arrange(desc(avgRating)) |&gt;\n  slice(1) #series with highest avg rating\n\n\nJoining the series with the highest average rating back with TITLE_BASICS to access the series name:\n\n\nCode\nhighest_rated_series &lt;- series_avg_ratings |&gt;\n  inner_join(tv_series, by = c(\"parentTconst\" = \"tconst\")) |&gt;\n  select(primaryTitle, avgRating, numEpisodes)\n\n\nCreating a table to display the results from the 4 steps of code above:\n\nCode\nhighest_rated_series_tb &lt;- highest_rated_series |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Highest Rated TV Series with More Than 12 Episodes\"\n  ) |&gt;\n  cols_label(\n    primaryTitle = \"Series Title\",\n    avgRating = \"Average Rating\",\n    numEpisodes = \"Number of Episodes\"\n  ) |&gt;\n  fmt_number(\n    columns = c(avgRating),\n    decimals = 1\n  )\n\n#displaying table \nhighest_rated_series_tb\n\n\n\n\n\n\n\nHighest Rated TV Series with More Than 12 Episodes\n\n\nSeries Title\nAverage Rating\nNumber of Episodes\n\n\n\n\nCraft Games\n9.7\n318\n\n\n\n\n\nMoving on to the final question of Task 2, regarding the TV series Happy Days (1974-1984) (Q6)\nFirst, filtering to get only the “Happy Days” TV series from the TITLE_BASICS table.\n\n\nCode\nhappy_days_series &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == \"Happy Days\" & titleType == \"tvSeries\")\n\n\nNext, joining with TITLE_EPISODES to get episode data for “Happy Days”.\n\n\nCode\nhappy_days_episodes &lt;- TITLE_EPISODES |&gt; \n  inner_join(happy_days_series, by =  c(\"parentTconst\" = \"tconst\"))\n\n\nPerforming another join with TITLE_RATINGS to get the ratings for each Happy Days episode.\n\n\nCode\nhappy_days_ratings &lt;- happy_days_episodes |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\")\n\n\nGrouping by season and calculating the average rating for each season.\n\n\nCode\nhappy_days_avg_ratings &lt;- happy_days_ratings |&gt;\n  group_by(seasonNumber) |&gt;\n  summarize(avgRating = mean(averageRating, na.rm = TRUE))\n\n\nFor this question, a bar chart will be a great way to visualize our results. The below code creates a bar chart of average ratings by season, using the previously downloaded library ggplot2.\n\n\nCode\nggplot(happy_days_avg_ratings, aes(x = as.factor(seasonNumber), y = avgRating)) + geom_bar(stat = \"identity\", fill = \"steelblue\") + labs(\n  title = \"Average Ratings by Season for Happy Days\",\n  x = \"Season Number\",\n  y = \"Average Rating\"\n) +\n  theme_minimal()"
  },
  {
    "objectID": "mp02.html#quantifying-success",
    "href": "mp02.html#quantifying-success",
    "title": "mp02",
    "section": "",
    "text": "The goal of the following section is to define a success metric for movies, given only IMDb ratings.\n\n\nIn creating a custom success metric, important metrics to consider are both the quality and popularity of a movie. Quality could be reflected by average IMDb rating and popularity could be reflected by the number of IMDb votes.\nWe can combine these two metric into a Success Metric formula as follows:\nSuccess Metric = averageRating x log(numVotes)\nNote: We use the logarithm of the number of votes to prevent movies with a large number of votes from skewing the results due to their popularity. The logarithm serves as a method to normalize the the scale of votes.\n\nCreating success metric:\n\n\n\nCode\n#adding success metric to TITLE_RATINGS table\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt; \n  mutate(success_metric = averageRating * log1p(numVotes))\n\n# taking a peek at updated TITLE_RATINGS table \nglimpse(TITLE_RATINGS)\n\n\nRows: 372,198\nColumns: 4\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt…\n$ averageRating  &lt;dbl&gt; 5.7, 5.6, 6.5, 5.4, 6.2, 5.0, 5.4, 5.4, 5.4, 6.8, 5.2, …\n$ numVotes       &lt;dbl&gt; 2090, 283, 2094, 184, 2828, 196, 889, 2233, 214, 7699, …\n$ success_metric &lt;dbl&gt; 43.57877, 31.63426, 49.70751, 28.18992, 49.27561, 26.41…\n\n\n\nValidating success metric:\n\n\nHighest Success Metric Top 10 Movies\nUsing a bar chart to display the top 10 movies based on the custom success metric.\n\n\n\nCode\n#top 10 movies by success metric\ntop_10_movies &lt;- TITLE_RATINGS |&gt;\n  inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\") |&gt;\n  arrange(desc(success_metric)) |&gt;\n  slice(1:10) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric)\n\n#bar chart to visualize top 10 movies\nggplot(top_10_movies, aes(x = reorder(primaryTitle, success_metric), y = success_metric)) + geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Movies by Success Metric\",\n    x = \"Movie Title\",\n    y = \"Success Metric\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nMovies with High Votes but Low Success Scores\nUsing a scatter plot to display 5 movies with a large number of votes but poor success scores.\n\n\nCode\n#grabbing movies with high votes and low success scores\nlow_success_movies &lt;- TITLE_RATINGS |&gt;\n  filter(numVotes &gt; 100000) |&gt;\n  arrange(success_metric) |&gt;\n  slice(1:5) |&gt;\n  inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric)\n\n#creating scatter plot to visualize \nggplot(low_success_movies, aes(x = numVotes, y = success_metric)) + geom_point(size=3, color=\"red\") + geom_text(aes(label = primaryTitle), vjust = -1) + labs(\n  title = \"Movies with High Votes and Low Success Scores\",\n  x= \"Number of Votes\",\n  y=\"Success Metric\"\n) +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\nPrestigious Actor Check\n\nIn this validation method, the modeler selects a prestigious actor and confirms they have many projects with high success scores based on the defined success metric.\nChosen Actor: Robert De Niro\nThe code below walks us through the steps taken to get to Robert De Niro’s projects and their ratings.\nFirst, filter for Robert De Niro in the NAME_BASICS table.\nNext, join with the TITLE_BASICS on the knownForTitles column, after splitting this column.\nThen, join with TITLE_RATINGS to get ratings for the projects.\n\n\nCode\nde_niro_titles &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Robert De Niro\") |&gt;\n  separate_rows(knownForTitles, sep = \",\")\n\nprint(de_niro_titles)\n\n\n# A tibble: 4 × 6\n  nconst    primaryName    birthYear deathYear primaryProfession  knownForTitles\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;         \n1 nm0000134 Robert De Niro      1943        NA actor,producer,di… tt0101540     \n2 nm0000134 Robert De Niro      1943        NA actor,producer,di… tt0081398     \n3 nm0000134 Robert De Niro      1943        NA actor,producer,di… tt0077416     \n4 nm0000134 Robert De Niro      1943        NA actor,producer,di… tt0075314     \n\n\n\n\nCode\nde_niro_basic_join &lt;- de_niro_titles |&gt;\n  inner_join(TITLE_BASICS, by = c(\"knownForTitles\" = \"tconst\"))\n\nglimpse(de_niro_basic_join)\n\n\nRows: 4\nColumns: 14\n$ nconst            &lt;chr&gt; \"nm0000134\", \"nm0000134\", \"nm0000134\", \"nm0000134\"\n$ primaryName       &lt;chr&gt; \"Robert De Niro\", \"Robert De Niro\", \"Robert De Niro\"…\n$ birthYear         &lt;dbl&gt; 1943, 1943, 1943, 1943\n$ deathYear         &lt;dbl&gt; NA, NA, NA, NA\n$ primaryProfession &lt;chr&gt; \"actor,producer,director\", \"actor,producer,director\"…\n$ knownForTitles    &lt;chr&gt; \"tt0101540\", \"tt0081398\", \"tt0077416\", \"tt0075314\"\n$ titleType         &lt;chr&gt; \"movie\", \"movie\", \"movie\", \"movie\"\n$ primaryTitle      &lt;chr&gt; \"Cape Fear\", \"Raging Bull\", \"The Deer Hunter\", \"Taxi…\n$ originalTitle     &lt;chr&gt; \"Cape Fear\", \"Raging Bull\", \"The Deer Hunter\", \"Taxi…\n$ isAdult           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE\n$ startYear         &lt;dbl&gt; 1991, 1980, 1978, 1976\n$ endYear           &lt;dbl&gt; NA, NA, NA, NA\n$ runtimeMinutes    &lt;dbl&gt; 128, 129, 183, 114\n$ genres            &lt;chr&gt; \"Crime,Thriller\", \"Biography,Drama,Sport\", \"Drama,Wa…\n\n\n\n\nCode\nde_niro_basic_join &lt;- de_niro_basic_join |&gt;\n  rename(tconst = knownForTitles)\n\nde_niro_success &lt;- de_niro_basic_join |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  arrange(desc(success_metric)) |&gt;\n  select(primaryTitle, titleType, averageRating, numVotes, success_metric)\n\nglimpse(de_niro_success)\n\n\nRows: 4\nColumns: 5\n$ primaryTitle   &lt;chr&gt; \"Taxi Driver\", \"Raging Bull\", \"The Deer Hunter\", \"Cape …\n$ titleType      &lt;chr&gt; \"movie\", \"movie\", \"movie\", \"movie\"\n$ averageRating  &lt;dbl&gt; 8.2, 8.1, 8.1, 7.3\n$ numVotes       &lt;dbl&gt; 940583, 385456, 367022, 220649\n$ success_metric &lt;dbl&gt; 112.78490, 104.18370, 103.78676, 89.82163\n\n\n\nCode\n#creating a table to display results above\nde_niro_table &lt;- de_niro_success |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Robert De Niro's Top 4 Projects by Success Metric\"\n  ) |&gt;\n  fmt_number(\n    columns = c(averageRating, numVotes, success_metric),\n    decimals = 2\n  )\n\n#printing table\nde_niro_table\n\n\n\n\n\n\n\nRobert De Niro's Top 4 Projects by Success Metric\n\n\nprimaryTitle\ntitleType\naverageRating\nnumVotes\nsuccess_metric\n\n\n\n\nTaxi Driver\nmovie\n8.20\n940,583.00\n112.78\n\n\nRaging Bull\nmovie\n8.10\n385,456.00\n104.18\n\n\nThe Deer Hunter\nmovie\n8.10\n367,022.00\n103.79\n\n\nCape Fear\nmovie\n7.30\n220,649.00\n89.82\n\n\n\n\n\nWe use a scatterplot to display the averageRating vs Success Metric. The trend line is included to help highlight any linear relationship between these two variables.\n\n\nCode\nggplot(de_niro_success, aes(x = averageRating, y = success_metric)) +\n  geom_point(size = 3, color = \"blue\") +  # Scatter plot points\n  geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\") +  # Add a trend line (linear regression)\n  labs(\n    title = \"Robert De Niro's Projects: IMDb Rating vs Success Metric\",\n    x = \"IMDb Average Rating\",\n    y = \"Success Metric\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe find Robert De Niro’s projects with the highest success rating also have a high success score, which further validates the chosen success metric.\n\nFurther Validation\nIn an effort to continue validating our chosen success metric, we explore the use of scatterplots to analyze the relationship between IMDb rating and success metric score.\n\n\n\nCode\nall_movies_success &lt;- TITLE_RATINGS |&gt;\n  inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\")\n\nggplot(all_movies_success, aes(x = averageRating, y = success_metric)) +\n  geom_point(color = \"blue\", size = 2, alpha = 0.5) +  # All movie points\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE, linetype = \"dashed\") +  # Trend line\n  labs(x = \"IMDb Average Rating\", y = \"Success Metric\", title = \"Scatterplot of Success Metric vs IMDb Average Rating (All Movies)\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 14),\n    axis.title.y = element_text(size = 14)\n  )\n\n\n\n\n\n\n\n\n\nApplying jittering to make the scatterplot slightly more readable.\n\n\nCode\nggplot(all_movies_success, aes(x = averageRating, y = success_metric)) +\n  geom_jitter(color = \"blue\", size = 1.5, alpha = 0.3, width = 0.2, height = 0.2) +  # Jitter points\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE, linetype = \"dashed\") +  # Trend line\n  labs(x = \"IMDb Average Rating\", y = \"Success Metric\", title = \"Scatterplot of Success Metric vs IMDb Average Rating (All Movies)\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 14),\n    axis.title.y = element_text(size = 14)\n  )\n\n\n\n\n\n\n\n\n\nCleaning dataset to eliminate outliers, set a threshold for the minimum number of votes, and remove NAs. This allows for a smaller dataset to hopefully improve the visibility of our scatterplot.\n\n\nCode\nmin_votes_threshold &lt;- 1000  # You can adjust this value based on your needs\n\nall_movies_filtered &lt;- all_movies_success |&gt;\n  filter(!is.na(averageRating) & !is.na(numVotes)) |&gt;  # Remove rows with missing values\n  filter(numVotes &gt;= min_votes_threshold) |&gt;  # Remove movies with too few votes\n  filter(averageRating &gt;= 2 & averageRating &lt;= 9.5) |&gt; # Remove movies with extreme ratings\n  filter(!is.na(startYear) & genres != \"\\\\N\")\n\nggplot(all_movies_filtered, aes(x = averageRating, y = success_metric)) +\n  geom_jitter(color = \"blue\", size = 1.5, alpha = 0.3, width = 0.2, height = 0.2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE, linetype = \"dashed\") +\n  labs(x = \"IMDb Average Rating\", y = \"Success Metric\", title = \"Scatterplot of Success Metric vs IMDb Average Rating (Filtered Movies)\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 14),\n    axis.title.y = element_text(size = 14)\n  )\n\n\n\n\n\n\n\n\n\nGiven the volume of movies in our dataset, this only slightly improved the visibility of the scatterplot. In all three versions, the correlation is clear, movies with a higher average rating tend to have a higher success metric.\nTo further validate the custom success metric, we find the correlation coefficient between the IMDb average rating and the success metric. This measures the linear relationship between the two variables.\n\nCode\ncorrelation &lt;- cor(all_movies_filtered$averageRating, all_movies_filtered$success_metric, use = \"complete.obs\")\n\ncorrelation_df &lt;- data.frame(\n  x = \"IMDb Average Rating\",\n  y = \"Success Metric\",\n  Correlation_Coefficient = sprintf(\"%.2f\", correlation)\n)\n\ncorrelation_table &lt;- correlation_df |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Correlation between IMDb Average Rating and Success Metric\"\n  )\n\nprint(correlation_table)\n\n\n\n\n\n\n\nCorrelation between IMDb Average Rating and Success Metric\n\n\n\n\nx\n\n\ny\n\n\nCorrelation_Coefficient\n\n\n\n\n\n\nIMDb Average Rating\n\n\nSuccess Metric\n\n\n0.78\n\n\n\n\n\nThe calculated correlation coefficient of 0.78 indicates a strong positive correlation between the IMDb average rating and the success metric. This translates to, movies with higher IMDb ratings also tend to score higher on the success metric, which is the trend we can visualize in the scatterplots above. This further supports and validates the use of the success metric.\n\nDefine a Numerical Threshold for Successful Project\nEstablishing a numerical threshold for the custom success metric requires a few steps.\n1) Analyzing the distribution of the success metric\n\nCode\nsuccess_summary &lt;- all_movies_filtered |&gt;\n  summarise(\n    Min = round(min(success_metric),2),\n    Q1 = round(quantile(success_metric, 0.25),2),\n    Median = round(median(success_metric),2),\n    Q3 = round(quantile(success_metric, 0.75),2),\n    Max = round(max(success_metric),2),\n    Mean = round(mean(success_metric),2),\n    SD = round(sd(success_metric),2)\n  )\n\nsuccess_summary_table &lt;- success_summary |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Summary Statistics for Success Metric\"\n  )\n\nprint(success_summary_table)\n\n\n\n\n\n\n\nSummary Statistics for Success Metric\n\n\n\n\nMin\n\n\nQ1\n\n\nMedian\n\n\nQ3\n\n\nMax\n\n\nMean\n\n\nSD\n\n\n\n\n\n\n14.17\n\n\n44.38\n\n\n52.31\n\n\n62.06\n\n\n138.52\n\n\n53.91\n\n\n15.24\n\n\n\n\n\n\n\nVisualizing the Success Metric\n\n\n\nCode\nggplot(all_movies_filtered, aes(x = \"\", y = success_metric)) +\n  geom_boxplot() +\n  labs(\n    title = \"Boxplot of Success Metric for All Movies\",\n    y = \"Success Metric\"\n  )\n\n\n\n\n\n\n\n\n\nThe box plot helps visualize the distribution of the success metric and outliers above the upper quartile. A majoriy of the success metrics lie between 50 and 75, with outliers above 100.\n\n\nCode\nggplot(all_movies_filtered, aes(x = success_metric)) +\n  geom_histogram(bins = 30, fill = \"blue\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of Success Metric for All Movies\",\n    x = \"Success Metric\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\nThe histogram of the success metric shows a right-skewed distribution, with the majority of movies clustered around a success metric of 50 to 75. Another observation includes the significant number of high-performing movies in the dataset, as we can see from the long tail extending towards higher success values.\n\nDefining the Success Threshold\n\nWe can pick a threshold at the third quartile, Q3, given the top 25% of movies fall above this threshold. By setting this threshold, we ensure only movies that are significantly better than the median are classified as “solid” or better.\nOur success threshold chosen is our Q3 value of 62.06."
  },
  {
    "objectID": "MP01_files/mp01.html",
    "href": "MP01_files/mp01.html",
    "title": "mp01",
    "section": "",
    "text": "Abstract: This report analyzes data from the National Transit Database. The report explores some of the fiscal characteristics of major US public transportation systems, such as: farebox revenues, total number of trips, total number of vehicle miles traveled, and total revenues and expenses by source. One key metric to keep in mind is the farebox recovery ratio. Farebox recovery ratio represents the proportion of operating expenses that are covered just by fares alone. The ratio is total fares/total operating expenses for said time frame. This ratio becomes important as we explore various cities, their agencies for public transportation, and their different modes of public transportation and look for the most efficient and profitable form.\n\n\nThis study uses three primary datasets: 2022 Fare Revenue, 2022 Operating Expenses, and the latest Monthly Ridership data. A note to keep in mind: because the data is reported on a lag, the 2022 version of each report was used.\nAs you will find in the code below, the three datasets were downloaded, cleaned, and merged. The fare revenue data was filtered to include only relevant columns and aggregated by the columns NTD ID, Agency Name and Mode. The expenses data was similarly filtered then aggregated by NTD ID and Mode. These two datasets were then merged into a single table titled FINANCIALS.\nThe monthly ridership data was handled separately. The Unlinked Passenger Trips (UPT) and Vehicle Revenue Miles (VRM) data was extracted and combined into new table titled USAGE. This later allows for an easy calculation of farebox recovery and views of the highest/lowest VRM and UPT across different agencies and modes.\nThe data pre-processing step was necessary to clean the data, put it in a consistent format, preparing it for further analysis.\n\n# ============================================================\n# Downloading, Cleaning, and Joining Tables\n# ============================================================\ninstall.packages(\"dplyr\", repos=c(CRAN = \"https://cran.rstudio\"))\nlibrary(dplyr)\n\n#First snippet of code, provided by professor\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n  # directory.\n  download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                destfile=\"2022_fare_revenue.xlsx\", \n                quiet=FALSE, \n                method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n  select(-`State/Parent NTD ID`, \n         -`Reporter Type`,\n         -`Reporting Module`,\n         -`TOS`,\n         -`Passenger Paid Fares`,\n         -`Organization Paid Fares`) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`) |&gt;\n  group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n           `Agency Name`,  # These are direct operated and sub-contracted \n           `Mode`) |&gt;      # of the same transit modality\n  # Not a big effect in most munis (significant DO\n  # tends to get rid of sub-contractors), but we'll sum\n  # to unify different passenger experiences\n  summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n  ungroup()\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"2022_expenses.csv\" in your project\n  # directory.\n  download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                destfile=\"2022_expenses.csv\", \n                quiet=FALSE, \n                method=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n  select(`NTD ID`, \n         `Agency`,\n         `Total`, \n         `Mode`) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n#Checking financials data\nhead(FINANCIALS)\n\n#Noticed for some modes, there is an expense value of 0\n#Counting # of rows with 0 expenses\nzero_expenses_count &lt;- FINANCIALS |&gt;\n  filter(Expenses == 0) |&gt;\n  nrow()\nprint(zero_expenses_count) #There are 8 rows with zero expenses\n\n#View rows with zero expenses and print\nzero_expenses_rows &lt;- FINANCIALS |&gt;\n  filter(Expenses == 0)\nprint(zero_expenses_rows)\n\n#Noting as a potential limitation for analysis later in the project. May choose to remove blank rows when calculating averages, etc.\n# ============================================================\n# Extracting monthly transit numbers\n# ============================================================\n#Second snippet of code, provided by professor \n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"ridership.xlsx\" in your project\n  # directory.\n  download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                destfile=\"ridership.xlsx\", \n                quiet=FALSE, \n                method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"UPT\") |&gt;\n  drop_na() |&gt;\n  mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"VRM\") |&gt;\n  drop_na() |&gt;\n  group_by(`NTD ID`, `Agency`, `UZA Name`, \n           `Mode`, `3 Mode`, month) |&gt;\n  summarize(VRM = sum(VRM)) |&gt;\n  ungroup() |&gt;\n  mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`))\n\n# ============================================================\n# Creating a table\n# ============================================================\n#Third snippet of code, provided by professor\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month=as.character(month)) |&gt; \n  DT::datatable()\n\nThe table above is a peek at the USAGE table, which comes into play for several topics of analysis later in the report.\n\n\n\nBelow you will find the renaming of column UZA Name to metro_area. This variable is used multiple times across the analysis. By creating a syntatic name, the column no longer needs to be surrounded by quotes each times it is referenced, making for easier, cleaner code.\n\n# ============================================================\n# Task 1 - Creating Syntatic Names \n# ============================================================\n#renaming UZA Name to metro_area\nUSAGE &lt;- USAGE |&gt;\n  rename(metro_area = 'UZA Name')\n\nTRIPS &lt;- TRIPS |&gt;\n  rename(metro_area = 'UZA Name')\n\nMILES &lt;- MILES |&gt;\n  rename(metro_area = 'UZA Name')\n\nnames(USAGE)\nnames(TRIPS)\nnames(MILES)\n\n\n\n\nIn the code below, you will find Mode column in the USAGE table was recoded using the case_when function. The Mode columns were initially in a shorthand abbreviation and were transformed into their full terms, which provide for more descriptive, clear labels.\n\n# ============================================================\n# Task 2 - Recoding the Mode column\n# ============================================================\n#using case_when to recode the mode column \nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"SR\" ~ \"Streetcar\", \n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"RB\" ~ \"Rapid Bus\",\n    Mode == \"MG\" ~ \"Monorail\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))  # Default for any unrecognized codes\n  \n#checking result of task 2\nhead(USAGE)\nUSAGE |&gt;\n  distinct(Mode)\n\n\n#creating an attractive summary table of cleaned up USAGE table \nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month=as.character(month)) |&gt; \n  DT::datatable()\n\n\n\n\n\nThe displayed table above is a view of a random sample of 1000 from the cleaned USAGE table. The format is interactive, allowing users to sort and search through the data, making it easily accessible and usable, for even a non-technical user.\n\n\n\n\n\n\nlibrary(scales)\n# 1. What transit agency had the most total VRM in our data set?\n#grouping by agency, summing VRM by agency & removing any NA values, arranging in descending order\ntotal_VRM_by_agency &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(Total_VRM = sum(VRM, na.rm =TRUE)) |&gt;\n  arrange(desc(Total_VRM)) \n  \n\n#formatting table using gt package\ntotal_VRM_by_agency_formatted &lt;- total_VRM_by_agency |&gt;\n  mutate(Total_VRM = comma(Total_VRM, accuracy =1)) \n\n#printing formatted table \nprint(total_VRM_by_agency_formatted)\n\n# A tibble: 677 × 2\n   Agency                                                              Total_VRM\n   &lt;chr&gt;                                                               &lt;chr&gt;    \n 1 MTA New York City Transit                                           10,832,8…\n 2 New Jersey Transit Corporation                                      5,645,52…\n 3 Los Angeles County Metropolitan Transportation Authority            4,354,01…\n 4 Washington Metropolitan Area Transit Authority                      2,821,95…\n 5 Chicago Transit Authority                                           2,806,20…\n 6 Southeastern Pennsylvania Transportation Authority                  2,672,63…\n 7 Massachusetts Bay Transportation Authority                          2,383,96…\n 8 Pace, the Suburban Bus Division of the Regional Transportation Aut… 2,379,40…\n 9 Metropolitan Transit Authority of Harris County, Texas              2,272,94…\n10 Denver Regional Transportation District                             1,991,41…\n# ℹ 667 more rows\n\n#displaying agency with highest total VRM\nhighest_VRM_agency &lt;- total_VRM_by_agency |&gt;\n  filter(Total_VRM == max(Total_VRM)) |&gt;\n  mutate(Total_VRM = comma(Total_VRM, accuracy =1)) \n  \n#printing highest_VRM_agency\nprint(highest_VRM_agency)\n\n# A tibble: 1 × 2\n  Agency                    Total_VRM     \n  &lt;chr&gt;                     &lt;chr&gt;         \n1 MTA New York City Transit 10,832,855,350\n\n\nThe VRM, or Vehicle Revenue Miles, represents the total distance traveled by vehicles in revenue service.\nThe analysis starts with aggregating the VRM values for each transit agency, grouping the data by Agency, and finally summing the total VRM for each group, or agency. Any missing values are excluded from the grouping, through the na.rm = TRUE statement. The results are then arranged in descending order, with the agency with the highest total VRM as the top row.\nThe results show MTA NYC Transit to have the highest total VRM, with over 10.8B revenue miles. This tells us the MTA NYC Transit system is the most extensive when viewed with the lens of # of miles covered.\n\n\n\n\n# 2. What transit mode had the most total VRM in our data set?\n#grouping by mode, summing VRM by mode & removing any NA values, arranging in descending order\ntotal_VRM_by_mode &lt;- USAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_VRM = sum(VRM, na.rm =TRUE)) |&gt;\n  arrange(desc(Total_VRM))\n#displaying summed VRM by mode table\nprint(total_VRM_by_mode)\n\n# A tibble: 18 × 2\n   Mode              Total_VRM\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 Bus             49444494088\n 2 Demand Response 17955073508\n 3 Heavy Rail      14620362107\n 4 Commuter Rail    6970644241\n 5 Vanpool          3015783362\n 6 Light Rail       2090094714\n 7 Commuter Bus     1380948975\n 8 Publico          1021270808\n 9 Trolleybus        236840288\n10 Rapid Bus         118425283\n11 Ferry Boat         65589783\n12 Streetcar          63389725\n13 Monorail           37879729\n14 Hybrid Rail        37787608\n15 Alaska Railroad    13833261\n16 Cable Car           7386019\n17 Inclined Plane       705904\n18 Aerial Tramway       292860\n\n#displaying mode with highest total VRM\nhighest_VRM_mode &lt;- total_VRM_by_mode |&gt;\n  filter(Total_VRM == max(Total_VRM)) |&gt;\n  mutate(Total_VRM = comma(Total_VRM, accuracy =1)) \n#printing highest_VRM_mode\nprint(highest_VRM_mode)\n\n# A tibble: 1 × 2\n  Mode  Total_VRM     \n  &lt;chr&gt; &lt;chr&gt;         \n1 Bus   49,444,494,088\n\n\nThis section once again reviews the total Vehicle Revenue Miles, but instead grouped by transit mode.\nThe code aggregates the VRM in a similar fashion to the first question, but instead groups by the Mode column.\nThe result reveals the mode of Bus has the highest VRM amongst the modes in the dataset, with over $49.4B vehicle revenue miles.\n\n\n\nThis question hones in on the NYC subway system specifically. To get our data into that granular level, the code, as shown below, filters the Mode to Heavy Rail.\n\n# 3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\n#first filtering the condensed_usage data frame to only heavy rail to see what else needs to be filtered out\nheavy_rail_data &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\")\n#viewing first 10 rows of filtered data\nhead(heavy_rail_data)\n#displaying unique values of metro_area\nunique_metro_areas &lt;- USAGE |&gt;\n  distinct(metro_area)\nprint(unique_metro_areas)\n\nThe code then goes on to filter further, for the month of May and for the Agency being MTA New York City Transit.\n\nnyc_subway_trips_count &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\",\n         month == \"2024-05-01\",\n         Agency == \"MTA New York City Transit\") |&gt;\n  summarize(Total_Trips = sum(UPT, na.rm = TRUE)) |&gt;\n  mutate(Total_Trips = comma(Total_Trips, accuracy =1)) \n\n# Print the total trips taken on the NYC Subway in May 2024\nprint(nyc_subway_trips_count)\n\n# A tibble: 1 × 1\n  Total_Trips\n  &lt;chr&gt;      \n1 180,458,819\n\n\nIn May 2024, over 180M trips were taken on NYC subways.\n\n\n\nThe below code aims to quantify the impact the COVID-19 pandemic had on NYC’s public transit system, specifically focusing on the subways.\nThe calculation is for ridership in April 2019, pre-pandemic, compared to ridership April 2020, at the height of the pandemic.\nThe code calculates the ridership for each of these time periods separately, then takes a difference between the two.\n\n# 5. How much did NYC subway ridership fall between April 2019 and April 2020?\n#finding ridership for April 2019 first\nnyc_subway_trips_april_2019 &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\",\n         month == \"2019-04-01\",\n         Agency == \"MTA New York City Transit\") |&gt;\n  summarize(Total_Trips = sum(UPT, na.rm = TRUE))\n\n# Print the total trips taken on the NYC Subway in April 2019\nprint(nyc_subway_trips_april_2019)\n\n# A tibble: 1 × 1\n  Total_Trips\n        &lt;dbl&gt;\n1   232223929\n\n#ridership for april 2020 second \nnyc_subway_trips_april_2020 &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\",\n         month == \"2020-04-01\",\n         Agency == \"MTA New York City Transit\") |&gt;\n  summarize(Total_Trips = sum(UPT, na.rm = TRUE))\n\n# Print the total trips taken on the NYC Subway in April 2020\nprint(nyc_subway_trips_april_2020)\n\n# A tibble: 1 × 1\n  Total_Trips\n        &lt;dbl&gt;\n1    20254269\n\n#calculating the difference \nnyc_ridership_change &lt;- abs(nyc_subway_trips_april_2020 - nyc_subway_trips_april_2019)\n#print(nyc_ridership_change)\n#Statement defining ridership change. Using paste() function to allow for string + variable to print together\nprint(paste(\"NYC Ridership fell by \", nyc_ridership_change, \" riders from April 2019 to April 2020.\"))\n\n[1] \"NYC Ridership fell by  211969660  riders from April 2019 to April 2020.\"\n\n\nAs shown in the output above, the ridership fell drastically from April 2019 to April 2020, by over 2.1B riders.\n\n\n\n\nNext, the study continues in performing exploratory data analysis. The theme of this analysis is uncovering additional insights about the MTA New York City Transit system.\nBelow, we explore NYC ridership across different transit modes. First, the focus is on breaking down the total ridership for each mode of transit available via MTA New York City Transit. The code first filters on MTA NYC Transit, groups by Mode and sums the Unlinked Passenger Trips (UPT) for each.\nThe code reveals certain modes are much more used compared to others. In NYC, the subway, or heavy rail, surpasses all other modes of transportation. One can imagine how frequently riders use the NYC subway when commuting.\n\n# ============================================================\n# Task 4 - Explore and Analyze\n# ============================================================\n\n# Three more interesting transit facts \n# Additional fact 1 - ridership for each mode by MTA NYC Transit \nmta_ridership_by_mode &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_Ridership = sum(UPT, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_Ridership)) |&gt;\n  mutate(Total_Ridership = comma(Total_Ridership, accuracy =1))\n\nprint(mta_ridership_by_mode) #table outlining different modes of transportation and their total rides \n\n# A tibble: 5 × 2\n  Mode            Total_Ridership\n  &lt;chr&gt;           &lt;chr&gt;          \n1 Heavy Rail      51,672,094,135 \n2 Bus             16,889,723,939 \n3 Rapid Bus       315,432,467    \n4 Commuter Bus    133,304,773    \n5 Demand Response 91,175,466     \n\n\nThe table above provides a readable summary of the study - one can clearly see the Heavy Rail has the highest Total Ridership, followed by the Bus.\nBelow is another analysis of the MTA NYC Transit system - comparing subway ridership pre and post pandemic. The data is segmented into two time periods: the year of 2019 (1/1/19 - 12/31/19) for pre-pandemic and the year of 2022 (1/1/22 - 12/31/22) for post pandemic.\nWe find the average monthly ridership decreased by nearly 20M trips/per month from 2019 to 2022. This is a drop in subway ridership by about 34%.\n\n# Additional fact 2 - Comparing NYC ridership pre and post pandemic \n#pre-pandemic \npre_pandemic_ridership &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\",\n         month &gt;= \"2019-01-01\",\n         month &lt;= \"2019-12-31\") |&gt;\n  summarize(avg_ridership = mean(UPT, na.rm = TRUE))\n\nprint(pre_pandemic_ridership)\n\n# A tibble: 1 × 1\n  avg_ridership\n          &lt;dbl&gt;\n1     57709634.\n\n#post-pandemic \npost_pandemic_ridership &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\",\n         month &gt;= \"2022-01-01\",\n         month &lt;= \"2022-12-31\") |&gt;\n  summarize(avg_ridership = mean(UPT, na.rm = TRUE))\n\nprint(post_pandemic_ridership)\n\n# A tibble: 1 × 1\n  avg_ridership\n          &lt;dbl&gt;\n1     37980885.\n\nprint(paste(\"The average ridership for the year of 2019, pre-pandemic, was \", round(pre_pandemic_ridership,0), \" and the average ridership for the year of 2022, post pandemic, was \", round(post_pandemic_ridership,0)))\n\n[1] \"The average ridership for the year of 2019, pre-pandemic, was  57709634  and the average ridership for the year of 2022, post pandemic, was  37980885\"\n\nround((1 - post_pandemic_ridership/pre_pandemic_ridership) * 100,0)\n\n  avg_ridership\n1            34\n\n\nThe analysis below explores the ridership efficiency of the different NYC transit modes. Efficiency is measured as the average number of Unlinked Passenger Trips (UPT) per Vehicle Revenue Mile (VRM), which indicates how effectively each mode transports passengers against the distance covered.\n\n# Additional fact 3\n#MTA NYC Transit ridership efficiency by mode \n#where efficiency represents the avg # of unlinked passenger trips per vehicle revenue mile\nridership_efficiency &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(Efficiency = round(sum(UPT, na.rm = TRUE) / sum(VRM, na.rm = TRUE),2))\nprint(ridership_efficiency) #efficiency = for every mile traveled, x# of passengers carried \n\n# A tibble: 5 × 2\n  Mode            Efficiency\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Bus                   8.05\n2 Commuter Bus          1.15\n3 Demand Response       0.11\n4 Heavy Rail            6.68\n5 Rapid Bus             9.17\n\n\nLooking at the table above, the following may be concluded. The Rapid Bus it the most efficient mode of transport in NYC. With an efficiency score of 9.17, this translates into the Rapid Bus carrying over 9 passengers per mile traveled.\nNot surprisingly, the Demand Response as the lowest efficiency at 0.11. Demand Response modes of transportation are typically used for specialized transportation needs and are available upon request.\n\n\n\nIn the task below, a comprehensive dataset is created via merging annual transit usage and financial data for 2022.\nFirst, the USAGE table is filtered on the year 2022, then aggregated each UPT and VRM for each transit agency and mode of transportation. This summarized data is stored in the new table titled USAGE_2022_ANNUAL.\nThen, the FINANCIALS table is updated to match the Mode labels in USAGE. Once again, the abbreviations are changed to their full names. This standardizes the Mode naming convention.\nLastly, the USAGE_2022_ANNUAL table is merged with the updated FINANCIALS table, joined on NTD ID and Mode. The new table is titled USAGE_AND_FINANCIALS. This table becomes very useful when evaluating farebox recovery in the following section.\n\n# ============================================================\n# Task 5 - Table Summarization\n# ============================================================\n#creating a new table from USAGE that has annual total (sum) UPT and VRM for 2022\nnames(USAGE)\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == 2022) |&gt;\n  group_by(`NTD ID`, Agency, metro_area, Mode) |&gt;\n  summarize(\n    UPT = sum(UPT, na.rm=TRUE),\n    VRM = sum(VRM, na.rm=TRUE)\n  ) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"SR\" ~ \"Streetcar\", \n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"RB\" ~ \"Rapid Bus\",\n    Mode == \"MG\" ~ \"Monorail\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))  # Default for any unrecognized codes\n\n\n#Merging USAGE_2022_ANNUAL table to the FINANCIALS table \nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n                                  FINANCIALS, \n                                  join_by(`NTD ID`, Mode)) |&gt;\n  drop_na()\n#Viewing first few rows of merged table \nhead(USAGE_AND_FINANCIALS)\n\n\n\n\n\n\nFor analysis, first the total UPT for each combination of agency and mode is summarized. The USAGE_AND_FINANCIALS table is used for this analysis.\n\n# 1. Which transit system (agency and mode) had the most UPT in 2022?\n#table summarizing the total UPT for each agency and mode \nUPT_summary_agency_mode &lt;- USAGE_AND_FINANCIALS |&gt; \n  group_by(Agency, Mode) |&gt;\n  summarize(Total_UPT = sum(UPT, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_UPT)) \n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\n#printing summary table \nprint(UPT_summary_agency_mode)\n\n# A tibble: 1,129 × 3\n# Groups:   Agency [525]\n   Agency                                                   Mode       Total_UPT\n   &lt;chr&gt;                                                    &lt;chr&gt;          &lt;dbl&gt;\n 1 MTA New York City Transit                                Heavy Rail    1.79e9\n 2 MTA New York City Transit                                Bus           4.59e8\n 3 Los Angeles County Metropolitan Transportation Authority Bus           1.94e8\n 4 Chicago Transit Authority                                Bus           1.40e8\n 5 New Jersey Transit Corporation                           Bus           1.13e8\n 6 Chicago Transit Authority                                Heavy Rail    1.04e8\n 7 MTA Bus Company                                          Bus           1.00e8\n 8 Washington Metropolitan Area Transit Authority           Heavy Rail    9.84e7\n 9 Southeastern Pennsylvania Transportation Authority       Bus           9.66e7\n10 Washington Metropolitan Area Transit Authority           Bus           8.99e7\n# ℹ 1,119 more rows\n\n#max UPT from table \nmax_UPT &lt;- UPT_summary_agency_mode |&gt; \n  filter(Total_UPT == max(Total_UPT))\nmax_UPT_system &lt;- head(max_UPT,1)\nprint(max_UPT_system)\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                    Mode        Total_UPT\n  &lt;chr&gt;                     &lt;chr&gt;           &lt;dbl&gt;\n1 MTA New York City Transit Heavy Rail 1793073801\n\nmost_UPT_agency &lt;- max_UPT_system$Agency #grabs the value from the column after the $ sign\n#print(most_UPT_agency)\nmost_UPT_mode &lt;- max_UPT_system$Mode\n#print(most_UPT_mode)\nprint(paste(\"The Agency with the highest UPT is \", most_UPT_agency, \" with a mode of \", most_UPT_mode))\n\n[1] \"The Agency with the highest UPT is  MTA New York City Transit  with a mode of  Heavy Rail\"\n\n\nThe top performing agency and its most with the highest UPT is displayed above. We find the MTA NYC Transit with its Heavy Rail mode to have the highest UPT, with ~1.8B unlinked passenger trips in 2022.\n\n\n\nTo reiterate, the Farebox Recovery Ratio is the proportion of operating expenses coverd by fare revenue, and is calculated as the ratio of Total Fares to Expenses. The higher the Farebox Recovery Ratio, the greater financial stability through passenger fare revenue.\n\n# 2. Which transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nhighest_farebox_recovery &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Farebox_Recovery = `Total Fares` / Expenses) |&gt;\n  filter(Farebox_Recovery == max(Farebox_Recovery, na.rm = TRUE)) |&gt;\n  arrange(desc(Farebox_Recovery)) |&gt;\n  head(1) #returns top row\n#print(highest_farebox_recovery)\nmost_farebox_agency &lt;- highest_farebox_recovery$Agency\nmost_farebox_mode &lt;- highest_farebox_recovery$Mode\ntop_recovery &lt;- highest_farebox_recovery$Farebox_Recovery\n\nprint(paste(\"The Agency with the highest Farebox Recovery is\", most_farebox_agency, \"and its mode is\", most_farebox_mode, \"with a farebox recovery ratio of\", round(top_recovery,2)))\n\n[1] \"The Agency with the highest Farebox Recovery is Transit Authority of Central Kentucky and its mode is Vanpool with a farebox recovery ratio of 2.38\"\n\n\nThe Vanpool mode of transportation within the Transit Authority of Central Kentucky performed with the highest farebox recovery ratio. One caution to keep in mind is this report did not filter on only the major systems, as the programmer chose to include all transportation systems (Major and Minor).\n\n\n\nThe next analysis looks at the lowest operating expenses per UPT, which is an indicator of cost efficiency. The lower the expenses per UPT, the more cost-effective a transportation system is in transporting passengers.\n\n# 3. Which transit system (agency and mode) has the lowest expenses per UPT?\nlowest_expenses_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Expenses_per_UPT = Expenses /UPT) |&gt;\n  arrange(Expenses_per_UPT) |&gt; #arranges in ascending order \n  filter(Expenses_per_UPT == min(Expenses_per_UPT, na.rm = TRUE)) |&gt;\n  head(1) #selects top row, which will be lowest, since in ascending order\n\nlowest_exp_agency &lt;- lowest_expenses_per_UPT$Agency\nlowest_exp_mode &lt;- lowest_expenses_per_UPT$Mode\nlowest_exp_per_UPT &lt;- lowest_expenses_per_UPT$Expenses_per_UPT\n\nprint(paste(\"The transit system with the lowest expenses per UPT is\", lowest_exp_agency, \"and its mode is\", lowest_exp_mode, \"with an expenses per UPT ratio of\", round(lowest_exp_per_UPT,2)))\n\n[1] \"The transit system with the lowest expenses per UPT is North Carolina State University and its mode is Bus with an expenses per UPT ratio of 1.18\"\n\n\n\n\n\n\n# 4. Which transit system (agency and mode) has the highest total fares per UPT?\nhighest_fares_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Fares_per_UPT = `Total Fares` / UPT) |&gt;\n  arrange(desc(Fares_per_UPT)) |&gt;\n  head(1)\n\nhighest_fares_agency &lt;- highest_fares_per_UPT$Agency\nhighest_fares_mode &lt;- highest_fares_per_UPT$Mode\nhighest_fares_per_UPT_amt &lt;- highest_fares_per_UPT$Fares_per_UPT\n\nprint(paste(\"The transit system with the highest total fares per UPT is\", highest_fares_agency,\"and its mode is\", highest_fares_mode, \"with a fares per UPT ratio of\", round(highest_fares_per_UPT_amt,2)))\n\n[1] \"The transit system with the highest total fares per UPT is Altoona Metro Transit and its mode is Demand Response with a fares per UPT ratio of 660.12\"\n\n\n\n\n\nThis study focuses on identifying the transit system with the lowest operating expenses per Vehicle Revenue Mile (VRM). This is another important metric in evaluating cost efficiency. In this case, we look at cost efficiency in relation to the distance covered by transit services.\n\n# 5. Which transit system (agency and mode) has the lowest expenses per VRM?\n#very similar to code for #3 \nlowest_expenses_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Expenses_per_VRM = Expenses / VRM) |&gt;\n  arrange(Expenses_per_VRM) |&gt;\n  head(1)\n\nlowest_exp_VRM_agency &lt;- lowest_expenses_per_VRM$Agency\nlowest_exp_VRM_mode &lt;- lowest_expenses_per_VRM$Mode\nlowest_exp_per_VRM_amt &lt;- lowest_expenses_per_VRM$Expenses_per_VRM\n\nprint(paste(\"The transit system with the lowest expenses per VRM is\", lowest_exp_VRM_agency, \"and its mode is\", lowest_exp_VRM_mode, \"with an expenses per VRM ratio of\", round(lowest_exp_per_VRM_amt,2)))\n\n[1] \"The transit system with the lowest expenses per VRM is New Mexico Department of Transportation and its mode is Vanpool with an expenses per VRM ratio of 0.34\"\n\n\nThe output above tells us the New Mexico Department of Transportation, when operating the Vanpool mode, has the lowest expenses per VRM, at $0.34 per mile. This portrays the efficient use of resources within this mode and agency.\n\n\n\n\n# 6. Which transit system (agency and mode) has the highest total fares per VRM?\n# very similar to code for #4\nhighest_fares_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Fares_per_VRM = `Total Fares` / VRM) |&gt;\n  arrange(desc(Fares_per_VRM)) |&gt;\n  head(1)\n\nhighest_fares_agency &lt;- highest_fares_per_VRM$Agency\nhighest_fares_mode &lt;- highest_fares_per_VRM$Mode\nhighest_fares_per_VRM_amt &lt;- highest_fares_per_VRM$Fares_per_VRM\n\nprint(paste(\"The transit system with the highest total fares per VRM is\", highest_fares_agency,\"and its mode is\", highest_fares_mode, \"with a fares per VRM ratio of\", round(highest_fares_per_VRM_amt,2)))\n\n[1] \"The transit system with the highest total fares per VRM is Chicago Water Taxi (Wendella) and its mode is Ferry Boat with a fares per VRM ratio of 237.46\"\n\n\nWe find the Chicago Water Taxi has the highest fares per VRM, at ~$237. This translates to for every mile traveled by the ferry, the system generates over $237 in revenue.\n\n\n\nTo conclude, the MTA New York City Transit Heavy Rail Mode is the most efficient transportation system in the country. The subway had $1.8B unlinked trips in 2022 alone. With the highest ridership and UPT, surpassing all other transportation systems in the country, the NYC subway provides the best service efficiency. The scale and service of the MTA NYC Transit system in unparalleled across the country."
  },
  {
    "objectID": "MP01_files/mp01.html#task-1",
    "href": "MP01_files/mp01.html#task-1",
    "title": "mp01",
    "section": "",
    "text": "Below you will find the renaming of column UZA Name to metro_area. This variable is used multiple times across the analysis. By creating a syntatic name, the column no longer needs to be surrounded by quotes each times it is referenced, making for easier, cleaner code.\n\n# ============================================================\n# Task 1 - Creating Syntatic Names \n# ============================================================\n#renaming UZA Name to metro_area\nUSAGE &lt;- USAGE |&gt;\n  rename(metro_area = 'UZA Name')\n\nTRIPS &lt;- TRIPS |&gt;\n  rename(metro_area = 'UZA Name')\n\nMILES &lt;- MILES |&gt;\n  rename(metro_area = 'UZA Name')\n\nnames(USAGE)\nnames(TRIPS)\nnames(MILES)"
  },
  {
    "objectID": "MP01_files/mp01.html#task-2",
    "href": "MP01_files/mp01.html#task-2",
    "title": "mp01",
    "section": "",
    "text": "In the code below, you will find Mode column in the USAGE table was recoded using the case_when function. The Mode columns were initially in a shorthand abbreviation and were transformed into their full terms, which provide for more descriptive, clear labels.\n\n# ============================================================\n# Task 2 - Recoding the Mode column\n# ============================================================\n#using case_when to recode the mode column \nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"SR\" ~ \"Streetcar\", \n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"RB\" ~ \"Rapid Bus\",\n    Mode == \"MG\" ~ \"Monorail\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))  # Default for any unrecognized codes\n  \n#checking result of task 2\nhead(USAGE)\nUSAGE |&gt;\n  distinct(Mode)\n\n\n#creating an attractive summary table of cleaned up USAGE table \nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month=as.character(month)) |&gt; \n  DT::datatable()\n\n\n\n\n\nThe displayed table above is a view of a random sample of 1000 from the cleaned USAGE table. The format is interactive, allowing users to sort and search through the data, making it easily accessible and usable, for even a non-technical user."
  },
  {
    "objectID": "MP01_files/mp01.html#task-3",
    "href": "MP01_files/mp01.html#task-3",
    "title": "mp01",
    "section": "",
    "text": "library(scales)\n# 1. What transit agency had the most total VRM in our data set?\n#grouping by agency, summing VRM by agency & removing any NA values, arranging in descending order\ntotal_VRM_by_agency &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(Total_VRM = sum(VRM, na.rm =TRUE)) |&gt;\n  arrange(desc(Total_VRM)) \n  \n\n#formatting table using gt package\ntotal_VRM_by_agency_formatted &lt;- total_VRM_by_agency |&gt;\n  mutate(Total_VRM = comma(Total_VRM, accuracy =1)) \n\n#printing formatted table \nprint(total_VRM_by_agency_formatted)\n\n# A tibble: 677 × 2\n   Agency                                                              Total_VRM\n   &lt;chr&gt;                                                               &lt;chr&gt;    \n 1 MTA New York City Transit                                           10,832,8…\n 2 New Jersey Transit Corporation                                      5,645,52…\n 3 Los Angeles County Metropolitan Transportation Authority            4,354,01…\n 4 Washington Metropolitan Area Transit Authority                      2,821,95…\n 5 Chicago Transit Authority                                           2,806,20…\n 6 Southeastern Pennsylvania Transportation Authority                  2,672,63…\n 7 Massachusetts Bay Transportation Authority                          2,383,96…\n 8 Pace, the Suburban Bus Division of the Regional Transportation Aut… 2,379,40…\n 9 Metropolitan Transit Authority of Harris County, Texas              2,272,94…\n10 Denver Regional Transportation District                             1,991,41…\n# ℹ 667 more rows\n\n#displaying agency with highest total VRM\nhighest_VRM_agency &lt;- total_VRM_by_agency |&gt;\n  filter(Total_VRM == max(Total_VRM)) |&gt;\n  mutate(Total_VRM = comma(Total_VRM, accuracy =1)) \n  \n#printing highest_VRM_agency\nprint(highest_VRM_agency)\n\n# A tibble: 1 × 2\n  Agency                    Total_VRM     \n  &lt;chr&gt;                     &lt;chr&gt;         \n1 MTA New York City Transit 10,832,855,350\n\n\nThe VRM, or Vehicle Revenue Miles, represents the total distance traveled by vehicles in revenue service.\nThe analysis starts with aggregating the VRM values for each transit agency, grouping the data by Agency, and finally summing the total VRM for each group, or agency. Any missing values are excluded from the grouping, through the na.rm = TRUE statement. The results are then arranged in descending order, with the agency with the highest total VRM as the top row.\nThe results show MTA NYC Transit to have the highest total VRM, with over 10.8B revenue miles. This tells us the MTA NYC Transit system is the most extensive when viewed with the lens of # of miles covered.\n\n\n\n\n# 2. What transit mode had the most total VRM in our data set?\n#grouping by mode, summing VRM by mode & removing any NA values, arranging in descending order\ntotal_VRM_by_mode &lt;- USAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_VRM = sum(VRM, na.rm =TRUE)) |&gt;\n  arrange(desc(Total_VRM))\n#displaying summed VRM by mode table\nprint(total_VRM_by_mode)\n\n# A tibble: 18 × 2\n   Mode              Total_VRM\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 Bus             49444494088\n 2 Demand Response 17955073508\n 3 Heavy Rail      14620362107\n 4 Commuter Rail    6970644241\n 5 Vanpool          3015783362\n 6 Light Rail       2090094714\n 7 Commuter Bus     1380948975\n 8 Publico          1021270808\n 9 Trolleybus        236840288\n10 Rapid Bus         118425283\n11 Ferry Boat         65589783\n12 Streetcar          63389725\n13 Monorail           37879729\n14 Hybrid Rail        37787608\n15 Alaska Railroad    13833261\n16 Cable Car           7386019\n17 Inclined Plane       705904\n18 Aerial Tramway       292860\n\n#displaying mode with highest total VRM\nhighest_VRM_mode &lt;- total_VRM_by_mode |&gt;\n  filter(Total_VRM == max(Total_VRM)) |&gt;\n  mutate(Total_VRM = comma(Total_VRM, accuracy =1)) \n#printing highest_VRM_mode\nprint(highest_VRM_mode)\n\n# A tibble: 1 × 2\n  Mode  Total_VRM     \n  &lt;chr&gt; &lt;chr&gt;         \n1 Bus   49,444,494,088\n\n\nThis section once again reviews the total Vehicle Revenue Miles, but instead grouped by transit mode.\nThe code aggregates the VRM in a similar fashion to the first question, but instead groups by the Mode column.\nThe result reveals the mode of Bus has the highest VRM amongst the modes in the dataset, with over $49.4B vehicle revenue miles.\n\n\n\nThis question hones in on the NYC subway system specifically. To get our data into that granular level, the code, as shown below, filters the Mode to Heavy Rail.\n\n# 3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\n#first filtering the condensed_usage data frame to only heavy rail to see what else needs to be filtered out\nheavy_rail_data &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\")\n#viewing first 10 rows of filtered data\nhead(heavy_rail_data)\n#displaying unique values of metro_area\nunique_metro_areas &lt;- USAGE |&gt;\n  distinct(metro_area)\nprint(unique_metro_areas)\n\nThe code then goes on to filter further, for the month of May and for the Agency being MTA New York City Transit.\n\nnyc_subway_trips_count &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\",\n         month == \"2024-05-01\",\n         Agency == \"MTA New York City Transit\") |&gt;\n  summarize(Total_Trips = sum(UPT, na.rm = TRUE)) |&gt;\n  mutate(Total_Trips = comma(Total_Trips, accuracy =1)) \n\n# Print the total trips taken on the NYC Subway in May 2024\nprint(nyc_subway_trips_count)\n\n# A tibble: 1 × 1\n  Total_Trips\n  &lt;chr&gt;      \n1 180,458,819\n\n\nIn May 2024, over 180M trips were taken on NYC subways.\n\n\n\nThe below code aims to quantify the impact the COVID-19 pandemic had on NYC’s public transit system, specifically focusing on the subways.\nThe calculation is for ridership in April 2019, pre-pandemic, compared to ridership April 2020, at the height of the pandemic.\nThe code calculates the ridership for each of these time periods separately, then takes a difference between the two.\n\n# 5. How much did NYC subway ridership fall between April 2019 and April 2020?\n#finding ridership for April 2019 first\nnyc_subway_trips_april_2019 &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\",\n         month == \"2019-04-01\",\n         Agency == \"MTA New York City Transit\") |&gt;\n  summarize(Total_Trips = sum(UPT, na.rm = TRUE))\n\n# Print the total trips taken on the NYC Subway in April 2019\nprint(nyc_subway_trips_april_2019)\n\n# A tibble: 1 × 1\n  Total_Trips\n        &lt;dbl&gt;\n1   232223929\n\n#ridership for april 2020 second \nnyc_subway_trips_april_2020 &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\",\n         month == \"2020-04-01\",\n         Agency == \"MTA New York City Transit\") |&gt;\n  summarize(Total_Trips = sum(UPT, na.rm = TRUE))\n\n# Print the total trips taken on the NYC Subway in April 2020\nprint(nyc_subway_trips_april_2020)\n\n# A tibble: 1 × 1\n  Total_Trips\n        &lt;dbl&gt;\n1    20254269\n\n#calculating the difference \nnyc_ridership_change &lt;- abs(nyc_subway_trips_april_2020 - nyc_subway_trips_april_2019)\n#print(nyc_ridership_change)\n#Statement defining ridership change. Using paste() function to allow for string + variable to print together\nprint(paste(\"NYC Ridership fell by \", nyc_ridership_change, \" riders from April 2019 to April 2020.\"))\n\n[1] \"NYC Ridership fell by  211969660  riders from April 2019 to April 2020.\"\n\n\nAs shown in the output above, the ridership fell drastically from April 2019 to April 2020, by over 2.1B riders."
  },
  {
    "objectID": "MP01_files/mp01.html#exploratory-data-analysis",
    "href": "MP01_files/mp01.html#exploratory-data-analysis",
    "title": "mp01",
    "section": "",
    "text": "Next, the study continues in performing exploratory data analysis. The theme of this analysis is uncovering additional insights about the MTA New York City Transit system.\nBelow, we explore NYC ridership across different transit modes. First, the focus is on breaking down the total ridership for each mode of transit available via MTA New York City Transit. The code first filters on MTA NYC Transit, groups by Mode and sums the Unlinked Passenger Trips (UPT) for each.\nThe code reveals certain modes are much more used compared to others. In NYC, the subway, or heavy rail, surpasses all other modes of transportation. One can imagine how frequently riders use the NYC subway when commuting.\n\n# ============================================================\n# Task 4 - Explore and Analyze\n# ============================================================\n\n# Three more interesting transit facts \n# Additional fact 1 - ridership for each mode by MTA NYC Transit \nmta_ridership_by_mode &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_Ridership = sum(UPT, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_Ridership)) |&gt;\n  mutate(Total_Ridership = comma(Total_Ridership, accuracy =1))\n\nprint(mta_ridership_by_mode) #table outlining different modes of transportation and their total rides \n\n# A tibble: 5 × 2\n  Mode            Total_Ridership\n  &lt;chr&gt;           &lt;chr&gt;          \n1 Heavy Rail      51,672,094,135 \n2 Bus             16,889,723,939 \n3 Rapid Bus       315,432,467    \n4 Commuter Bus    133,304,773    \n5 Demand Response 91,175,466     \n\n\nThe table above provides a readable summary of the study - one can clearly see the Heavy Rail has the highest Total Ridership, followed by the Bus.\nBelow is another analysis of the MTA NYC Transit system - comparing subway ridership pre and post pandemic. The data is segmented into two time periods: the year of 2019 (1/1/19 - 12/31/19) for pre-pandemic and the year of 2022 (1/1/22 - 12/31/22) for post pandemic.\nWe find the average monthly ridership decreased by nearly 20M trips/per month from 2019 to 2022. This is a drop in subway ridership by about 34%.\n\n# Additional fact 2 - Comparing NYC ridership pre and post pandemic \n#pre-pandemic \npre_pandemic_ridership &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\",\n         month &gt;= \"2019-01-01\",\n         month &lt;= \"2019-12-31\") |&gt;\n  summarize(avg_ridership = mean(UPT, na.rm = TRUE))\n\nprint(pre_pandemic_ridership)\n\n# A tibble: 1 × 1\n  avg_ridership\n          &lt;dbl&gt;\n1     57709634.\n\n#post-pandemic \npost_pandemic_ridership &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\",\n         month &gt;= \"2022-01-01\",\n         month &lt;= \"2022-12-31\") |&gt;\n  summarize(avg_ridership = mean(UPT, na.rm = TRUE))\n\nprint(post_pandemic_ridership)\n\n# A tibble: 1 × 1\n  avg_ridership\n          &lt;dbl&gt;\n1     37980885.\n\nprint(paste(\"The average ridership for the year of 2019, pre-pandemic, was \", round(pre_pandemic_ridership,0), \" and the average ridership for the year of 2022, post pandemic, was \", round(post_pandemic_ridership,0)))\n\n[1] \"The average ridership for the year of 2019, pre-pandemic, was  57709634  and the average ridership for the year of 2022, post pandemic, was  37980885\"\n\nround((1 - post_pandemic_ridership/pre_pandemic_ridership) * 100,0)\n\n  avg_ridership\n1            34\n\n\nThe analysis below explores the ridership efficiency of the different NYC transit modes. Efficiency is measured as the average number of Unlinked Passenger Trips (UPT) per Vehicle Revenue Mile (VRM), which indicates how effectively each mode transports passengers against the distance covered.\n\n# Additional fact 3\n#MTA NYC Transit ridership efficiency by mode \n#where efficiency represents the avg # of unlinked passenger trips per vehicle revenue mile\nridership_efficiency &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(Efficiency = round(sum(UPT, na.rm = TRUE) / sum(VRM, na.rm = TRUE),2))\nprint(ridership_efficiency) #efficiency = for every mile traveled, x# of passengers carried \n\n# A tibble: 5 × 2\n  Mode            Efficiency\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Bus                   8.05\n2 Commuter Bus          1.15\n3 Demand Response       0.11\n4 Heavy Rail            6.68\n5 Rapid Bus             9.17\n\n\nLooking at the table above, the following may be concluded. The Rapid Bus it the most efficient mode of transport in NYC. With an efficiency score of 9.17, this translates into the Rapid Bus carrying over 9 passengers per mile traveled.\nNot surprisingly, the Demand Response as the lowest efficiency at 0.11. Demand Response modes of transportation are typically used for specialized transportation needs and are available upon request."
  },
  {
    "objectID": "MP01_files/mp01.html#table-summarization",
    "href": "MP01_files/mp01.html#table-summarization",
    "title": "mp01",
    "section": "",
    "text": "In the task below, a comprehensive dataset is created via merging annual transit usage and financial data for 2022.\nFirst, the USAGE table is filtered on the year 2022, then aggregated each UPT and VRM for each transit agency and mode of transportation. This summarized data is stored in the new table titled USAGE_2022_ANNUAL.\nThen, the FINANCIALS table is updated to match the Mode labels in USAGE. Once again, the abbreviations are changed to their full names. This standardizes the Mode naming convention.\nLastly, the USAGE_2022_ANNUAL table is merged with the updated FINANCIALS table, joined on NTD ID and Mode. The new table is titled USAGE_AND_FINANCIALS. This table becomes very useful when evaluating farebox recovery in the following section.\n\n# ============================================================\n# Task 5 - Table Summarization\n# ============================================================\n#creating a new table from USAGE that has annual total (sum) UPT and VRM for 2022\nnames(USAGE)\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == 2022) |&gt;\n  group_by(`NTD ID`, Agency, metro_area, Mode) |&gt;\n  summarize(\n    UPT = sum(UPT, na.rm=TRUE),\n    VRM = sum(VRM, na.rm=TRUE)\n  ) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"SR\" ~ \"Streetcar\", \n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"RB\" ~ \"Rapid Bus\",\n    Mode == \"MG\" ~ \"Monorail\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))  # Default for any unrecognized codes\n\n\n#Merging USAGE_2022_ANNUAL table to the FINANCIALS table \nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n                                  FINANCIALS, \n                                  join_by(`NTD ID`, Mode)) |&gt;\n  drop_na()\n#Viewing first few rows of merged table \nhead(USAGE_AND_FINANCIALS)"
  },
  {
    "objectID": "MP01_files/mp01.html#farebox-recovery-among-major-systems",
    "href": "MP01_files/mp01.html#farebox-recovery-among-major-systems",
    "title": "mp01",
    "section": "",
    "text": "For analysis, first the total UPT for each combination of agency and mode is summarized. The USAGE_AND_FINANCIALS table is used for this analysis.\n\n# 1. Which transit system (agency and mode) had the most UPT in 2022?\n#table summarizing the total UPT for each agency and mode \nUPT_summary_agency_mode &lt;- USAGE_AND_FINANCIALS |&gt; \n  group_by(Agency, Mode) |&gt;\n  summarize(Total_UPT = sum(UPT, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_UPT)) \n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\n#printing summary table \nprint(UPT_summary_agency_mode)\n\n# A tibble: 1,129 × 3\n# Groups:   Agency [525]\n   Agency                                                   Mode       Total_UPT\n   &lt;chr&gt;                                                    &lt;chr&gt;          &lt;dbl&gt;\n 1 MTA New York City Transit                                Heavy Rail    1.79e9\n 2 MTA New York City Transit                                Bus           4.59e8\n 3 Los Angeles County Metropolitan Transportation Authority Bus           1.94e8\n 4 Chicago Transit Authority                                Bus           1.40e8\n 5 New Jersey Transit Corporation                           Bus           1.13e8\n 6 Chicago Transit Authority                                Heavy Rail    1.04e8\n 7 MTA Bus Company                                          Bus           1.00e8\n 8 Washington Metropolitan Area Transit Authority           Heavy Rail    9.84e7\n 9 Southeastern Pennsylvania Transportation Authority       Bus           9.66e7\n10 Washington Metropolitan Area Transit Authority           Bus           8.99e7\n# ℹ 1,119 more rows\n\n#max UPT from table \nmax_UPT &lt;- UPT_summary_agency_mode |&gt; \n  filter(Total_UPT == max(Total_UPT))\nmax_UPT_system &lt;- head(max_UPT,1)\nprint(max_UPT_system)\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                    Mode        Total_UPT\n  &lt;chr&gt;                     &lt;chr&gt;           &lt;dbl&gt;\n1 MTA New York City Transit Heavy Rail 1793073801\n\nmost_UPT_agency &lt;- max_UPT_system$Agency #grabs the value from the column after the $ sign\n#print(most_UPT_agency)\nmost_UPT_mode &lt;- max_UPT_system$Mode\n#print(most_UPT_mode)\nprint(paste(\"The Agency with the highest UPT is \", most_UPT_agency, \" with a mode of \", most_UPT_mode))\n\n[1] \"The Agency with the highest UPT is  MTA New York City Transit  with a mode of  Heavy Rail\"\n\n\nThe top performing agency and its most with the highest UPT is displayed above. We find the MTA NYC Transit with its Heavy Rail mode to have the highest UPT, with ~1.8B unlinked passenger trips in 2022.\n\n\n\nTo reiterate, the Farebox Recovery Ratio is the proportion of operating expenses coverd by fare revenue, and is calculated as the ratio of Total Fares to Expenses. The higher the Farebox Recovery Ratio, the greater financial stability through passenger fare revenue.\n\n# 2. Which transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nhighest_farebox_recovery &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Farebox_Recovery = `Total Fares` / Expenses) |&gt;\n  filter(Farebox_Recovery == max(Farebox_Recovery, na.rm = TRUE)) |&gt;\n  arrange(desc(Farebox_Recovery)) |&gt;\n  head(1) #returns top row\n#print(highest_farebox_recovery)\nmost_farebox_agency &lt;- highest_farebox_recovery$Agency\nmost_farebox_mode &lt;- highest_farebox_recovery$Mode\ntop_recovery &lt;- highest_farebox_recovery$Farebox_Recovery\n\nprint(paste(\"The Agency with the highest Farebox Recovery is\", most_farebox_agency, \"and its mode is\", most_farebox_mode, \"with a farebox recovery ratio of\", round(top_recovery,2)))\n\n[1] \"The Agency with the highest Farebox Recovery is Transit Authority of Central Kentucky and its mode is Vanpool with a farebox recovery ratio of 2.38\"\n\n\nThe Vanpool mode of transportation within the Transit Authority of Central Kentucky performed with the highest farebox recovery ratio. One caution to keep in mind is this report did not filter on only the major systems, as the programmer chose to include all transportation systems (Major and Minor).\n\n\n\nThe next analysis looks at the lowest operating expenses per UPT, which is an indicator of cost efficiency. The lower the expenses per UPT, the more cost-effective a transportation system is in transporting passengers.\n\n# 3. Which transit system (agency and mode) has the lowest expenses per UPT?\nlowest_expenses_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Expenses_per_UPT = Expenses /UPT) |&gt;\n  arrange(Expenses_per_UPT) |&gt; #arranges in ascending order \n  filter(Expenses_per_UPT == min(Expenses_per_UPT, na.rm = TRUE)) |&gt;\n  head(1) #selects top row, which will be lowest, since in ascending order\n\nlowest_exp_agency &lt;- lowest_expenses_per_UPT$Agency\nlowest_exp_mode &lt;- lowest_expenses_per_UPT$Mode\nlowest_exp_per_UPT &lt;- lowest_expenses_per_UPT$Expenses_per_UPT\n\nprint(paste(\"The transit system with the lowest expenses per UPT is\", lowest_exp_agency, \"and its mode is\", lowest_exp_mode, \"with an expenses per UPT ratio of\", round(lowest_exp_per_UPT,2)))\n\n[1] \"The transit system with the lowest expenses per UPT is North Carolina State University and its mode is Bus with an expenses per UPT ratio of 1.18\"\n\n\n\n\n\n\n# 4. Which transit system (agency and mode) has the highest total fares per UPT?\nhighest_fares_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Fares_per_UPT = `Total Fares` / UPT) |&gt;\n  arrange(desc(Fares_per_UPT)) |&gt;\n  head(1)\n\nhighest_fares_agency &lt;- highest_fares_per_UPT$Agency\nhighest_fares_mode &lt;- highest_fares_per_UPT$Mode\nhighest_fares_per_UPT_amt &lt;- highest_fares_per_UPT$Fares_per_UPT\n\nprint(paste(\"The transit system with the highest total fares per UPT is\", highest_fares_agency,\"and its mode is\", highest_fares_mode, \"with a fares per UPT ratio of\", round(highest_fares_per_UPT_amt,2)))\n\n[1] \"The transit system with the highest total fares per UPT is Altoona Metro Transit and its mode is Demand Response with a fares per UPT ratio of 660.12\"\n\n\n\n\n\nThis study focuses on identifying the transit system with the lowest operating expenses per Vehicle Revenue Mile (VRM). This is another important metric in evaluating cost efficiency. In this case, we look at cost efficiency in relation to the distance covered by transit services.\n\n# 5. Which transit system (agency and mode) has the lowest expenses per VRM?\n#very similar to code for #3 \nlowest_expenses_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Expenses_per_VRM = Expenses / VRM) |&gt;\n  arrange(Expenses_per_VRM) |&gt;\n  head(1)\n\nlowest_exp_VRM_agency &lt;- lowest_expenses_per_VRM$Agency\nlowest_exp_VRM_mode &lt;- lowest_expenses_per_VRM$Mode\nlowest_exp_per_VRM_amt &lt;- lowest_expenses_per_VRM$Expenses_per_VRM\n\nprint(paste(\"The transit system with the lowest expenses per VRM is\", lowest_exp_VRM_agency, \"and its mode is\", lowest_exp_VRM_mode, \"with an expenses per VRM ratio of\", round(lowest_exp_per_VRM_amt,2)))\n\n[1] \"The transit system with the lowest expenses per VRM is New Mexico Department of Transportation and its mode is Vanpool with an expenses per VRM ratio of 0.34\"\n\n\nThe output above tells us the New Mexico Department of Transportation, when operating the Vanpool mode, has the lowest expenses per VRM, at $0.34 per mile. This portrays the efficient use of resources within this mode and agency.\n\n\n\n\n# 6. Which transit system (agency and mode) has the highest total fares per VRM?\n# very similar to code for #4\nhighest_fares_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  mutate(Fares_per_VRM = `Total Fares` / VRM) |&gt;\n  arrange(desc(Fares_per_VRM)) |&gt;\n  head(1)\n\nhighest_fares_agency &lt;- highest_fares_per_VRM$Agency\nhighest_fares_mode &lt;- highest_fares_per_VRM$Mode\nhighest_fares_per_VRM_amt &lt;- highest_fares_per_VRM$Fares_per_VRM\n\nprint(paste(\"The transit system with the highest total fares per VRM is\", highest_fares_agency,\"and its mode is\", highest_fares_mode, \"with a fares per VRM ratio of\", round(highest_fares_per_VRM_amt,2)))\n\n[1] \"The transit system with the highest total fares per VRM is Chicago Water Taxi (Wendella) and its mode is Ferry Boat with a fares per VRM ratio of 237.46\"\n\n\nWe find the Chicago Water Taxi has the highest fares per VRM, at ~$237. This translates to for every mile traveled by the ferry, the system generates over $237 in revenue.\n\n\n\nTo conclude, the MTA New York City Transit Heavy Rail Mode is the most efficient transportation system in the country. The subway had $1.8B unlinked trips in 2022 alone. With the highest ridership and UPT, surpassing all other transportation systems in the country, the NYC subway provides the best service efficiency. The scale and service of the MTA NYC Transit system in unparalleled across the country."
  },
  {
    "objectID": "mp02.html#examining-success-by-genre-and-decade",
    "href": "mp02.html#examining-success-by-genre-and-decade",
    "title": "mp02",
    "section": "",
    "text": "The analysis may be broken down by decades and genres.\nWe start with categorizing movies by decade. The code below creates a new column that groups movies by the decade they were released, through the use of the startYear column. We add a decade column through the use of the mutate function.\n\n\nCode\nall_movies_filtered &lt;- all_movies_filtered |&gt; \n  mutate(decade = floor(startYear / 10) * 10)\n\n\nMoving on to grouping by genres. The code below splits the genres column into separate rows so movies can be individually analyzed by each genre.\n\n\nCode\nall_movies_filtered &lt;- all_movies_filtered |&gt;\n  separate_rows(genres, sep=\",\")\n\n\nWe then filter the movies that are considered successful. We define our success_threshold as the Q3 value, or 62.05, found in the previous task.\n\n\nCode\nsuccess_threshold &lt;- 62.05 \nsuccessful_movies &lt;- all_movies_filtered |&gt;\n  filter(success_metric &gt;= success_threshold)\n\n\nThen group by genre and decade, and count the number of successes.\n\n\nCode\nsuccess_by_genre_decade &lt;- successful_movies |&gt;\n  group_by(genres, decade) |&gt;\n  summarize(success_count=n(), .groups=\"drop\")\n\n\nLet’s take a look at few different views of our trends.\nLine Plot showing Success Over Time by Genre:\nEach line in the plot below represents the number of successful movies for a given genre in each decade, as defined by the custom success metric, filtered above the 62.05 threshold, showing only “successful” movies.\n\n\nCode\nggplot(success_by_genre_decade, aes(x = decade, y = success_count, color = genres)) +\n  geom_line(size = 1) +\n  labs(title = \"Trends in Movie Success by Genre and Decade\",\n       x = \"Decade\", y = \"Number of Successful Movies\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nObservations on the above:\n\nDocumentaries appear to dominate the successful movie space, while seeing a sharp rise from the 1980s to early 2000’s, and then a significant decline to follow\nNotable increase in biography popularity, with a peak in the early 2000’s\nSuccess in the early 2000’s, as we see most of the genres peak around 2010\n\nBar Chart showing Success by Genre since 2010:\n\n\nCode\nsuccess_2010s &lt;- success_by_genre_decade |&gt; \n  filter(decade &gt;= 2010)\n\nggplot(success_2010s, aes(x = reorder(genres, success_count), y = success_count, fill = genres)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Successful Movies by Genre (Post-2010)\",\n       x = \"Genre\", y = \"Number of Successful Movies\") +\n  coord_flip() +  # Flip the axis to make it easier to read\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhen we take a look at genre success following the year 2010, we see drama dominating the field, followed by comedy, action, then crime.\nWe find that documentaries, while extremely popular before 2010, are quite low on the list in comparison to other genres.\nWith these results, the chosen genre for the next project, which will be used in the following task, is Action."
  },
  {
    "objectID": "mp02.html#successful-personnel-in-the-genre",
    "href": "mp02.html#successful-personnel-in-the-genre",
    "title": "mp02",
    "section": "",
    "text": "The chosen genre is Action. we start with filtering only on Action movies. Rather than filtering from the TITLE_BASICS table, we filter from our cleaned dataset of movies, all_movies_filtered.\nWe join our dataset of cleaned movies data, filtered to only action movies, with the TITLE_PRINCIPALS table to get the principals, which will be actors and directors, in our case.\nThis dataset is then joined to the NAME_BASICS table, so we can later access the names of the actors and directors.\n\n\nCode\naction_movies_filtered &lt;- successful_movies |&gt;\n  separate_rows(genres, sep = \",\") |&gt;\n  filter(genres == \"Action\")\n\naction_principals &lt;- TITLE_PRINCIPALS |&gt;\n  inner_join(action_movies_filtered, by = \"tconst\")\n\naction_personnel &lt;- action_principals |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\")\n\n\nThe code below accesses directors and leads to creating and displaying a table of the top 5 directors, who have worked on action movies, in our dataset.\n\nCode\ndirectors &lt;- action_personnel |&gt;\n  filter(category == \"director\")\n\ndirector_success &lt;- directors |&gt;\n  group_by(nconst, primaryName) |&gt;\n  summarize(\n    success_count = n(),\n    avg_success_metric = round(mean(success_metric, na.rm = TRUE),2),\n    known_for_titles = first(knownForTitles)\n  ) |&gt;\n  arrange(desc(avg_success_metric)) |&gt;\n  ungroup() |&gt;\n  slice_head(n=5)\n\ndirectors_known_for &lt;- director_success |&gt;\n  separate_rows(known_for_titles, sep = \",\") |&gt;\n  inner_join(TITLE_BASICS, by = c(\"known_for_titles\" = \"tconst\")) |&gt;\n  group_by(primaryName) |&gt;\n  summarize(\n    success_count = first(success_count),\n    avg_success_metric = first(avg_success_metric),\n    known_for_titles = paste(primaryTitle, collapse = \", \")\n  )\n\ndirector_table &lt;- directors_known_for |&gt;\n  slice_head(n = 5) |&gt;\n  select(primaryName, success_count, avg_success_metric, known_for_titles) |&gt;\n  gt() |&gt;\n  tab_header(title = \"Top 5 Action Directors\") |&gt;\n  cols_label(\n    primaryName = \"Director\",\n    success_count = \"Number of Successful Movies\",\n    avg_success_metric = \"Average Success Metric\",\n    known_for_titles = \"Known For Titles\"\n  )\n\nprint(director_table)\n\n\n\n\n\n\n\nTop 5 Action Directors\n\n\n\n\nDirector\n\n\nNumber of Successful Movies\n\n\nAverage Success Metric\n\n\nKnown For Titles\n\n\n\n\n\n\nBob Persichetti\n\n\n1\n\n\n112.97\n\n\nSpider-Man: Into the Spider-Verse, Spider-Man: Across the Spider-Verse, The Little Prince, Puss in Boots: The Last Wish\n\n\n\n\nChristopher Nolan\n\n\n6\n\n\n117.52\n\n\nTenet, Interstellar, Inception, The Prestige\n\n\n\n\nPeter Jackson\n\n\n4\n\n\n120.14\n\n\nThe Lord of the Rings: The Fellowship of the Ring, Bad Taste, The Lord of the Rings: The Return of the King, The Lord of the Rings: The Two Towers\n\n\n\n\nRichard Marquand\n\n\n1\n\n\n115.74\n\n\nStar Wars: Episode VI - Return of the Jedi, Nowhere to Run, Jagged Edge\n\n\n\n\nRodney Rothman\n\n\n1\n\n\n112.97\n\n\nSpider-Man: Into the Spider-Verse, Spider-Man: Across the Spider-Verse, Forgetting Sarah Marshall, 22 Jump Street\n\n\n\n\n\nThe chosen director is Peter Jackson. We see he’s had 4 successful movies and one of the highest success metrics.\nThe code below accesses actors and leads to creating and displaying a table of the top 5 actors, who have worked on action movies, in our dataset.\n\nCode\nactors &lt;- action_personnel |&gt;\n  filter(category == \"actor\" | category == \"actress\") |&gt;\n  group_by(nconst, primaryName) |&gt;\n  summarize(success_count=n(), avg_success_metric = round(mean(success_metric),2), known_for_titles = first(knownForTitles)) |&gt;\n  arrange(desc(avg_success_metric)) |&gt;\n  ungroup() |&gt;\n  slice_head(n=5)\n\nactors_known_for &lt;- actors |&gt;\n  separate_rows(known_for_titles, sep = \",\") |&gt;\n  inner_join(TITLE_BASICS, by = c(\"known_for_titles\" = \"tconst\")) |&gt;\n  group_by(primaryName) |&gt;\n  summarize(\n    success_count = first(success_count),\n    avg_success_metric = first(avg_success_metric),\n    known_for_titles = paste(primaryTitle, collapse = \", \")\n  )\n\nactor_table &lt;- actors_known_for |&gt;\n  slice_head(n = 5) |&gt;\n  gt() |&gt;\n  tab_header(title = \"Top 5 Action Actors\") |&gt;\n  cols_label(\n    primaryName = \"Actor\",\n    success_count = \"Number of Successful Movies\",\n    avg_success_metric = \"Average Success Metric\",\n    known_for_titles = \"Known For Titles\"\n  )\n\nactor_table\n\n\n\n\n\n\n\nTop 5 Action Actors\n\n\nActor\nNumber of Successful Movies\nAverage Success Metric\nKnown For Titles\n\n\n\n\nAlan Howard\n1\n129.32\nThe Lord of the Rings: The Fellowship of the Ring, The Cook, the Thief, His Wife & Her Lover, The Lord of the Rings: The Return of the King\n\n\nAli Astin\n1\n130.64\nThe Lord of the Rings: The Return of the King, Bad Kids of Crestview Academy\n\n\nDileep Rao\n1\n129.97\nInception, Avatar, Drag Me to Hell, Avatar: The Way of Water\n\n\nMonique Gabriela Curnen\n1\n133.99\nThe Dark Knight, Half Nelson, Contagion, Fast & Furious\n\n\nNoel Appleby\n2\n129.98\nThe Lord of the Rings: The Fellowship of the Ring, The Lord of the Rings: The Return of the King, The Navigator: A Medieval Odyssey\n\n\n\n\n\nThe table above displays the top 4 actors involved in action movies, by highest average success scores.\nThe two chosen actors are Noel Appleby and Alan Howard, given they’ve worked together on a successful film, The Lord of the Rings: The Fellowship of the Ring, and have each seen success in a few other known films throughout their career. They go along well with the chosen director, who was the director of The Lord of the Rings.\nThe team has known success as we can see from their metrics displayed in the corresponding tables and visualized below. The three have worked together in the past, on a movie which was deemed successful, making this team more reliable.\n\n\nCode\nselected_personnel &lt;- bind_rows(\n  actors_known_for |&gt; select(primaryName, avg_success_metric) |&gt; mutate(type = \"Actor\"),\n  directors_known_for |&gt; select(primaryName, avg_success_metric) |&gt; mutate(type = \"Director\")\n)\n\nggplot(selected_personnel, aes(x = reorder(primaryName, avg_success_metric), y = avg_success_metric, fill = type)) +\n  geom_bar(stat = \"identity\", show.legend = TRUE) +\n  coord_flip() +\n  labs(\n    title = \"Success Metrics for Selected Actors and Directors\",\n    x = \"Personnel\",\n    y = \"Average Success Metric\"\n  ) +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"Actor\" = \"skyblue\", \"Director\" = \"orange\")) +\n  geom_text(aes(label = round(avg_success_metric, 2)), hjust = -0.1) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )"
  },
  {
    "objectID": "mp02.html#nostalgia-and-remakes",
    "href": "mp02.html#nostalgia-and-remakes",
    "title": "mp02",
    "section": "",
    "text": "In this task, our goal is to identify a classic action movie to remake. This movie is to have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.\nThe code below focuses on successful action movies released before 1999, sorted by the custom success metric, IMDb rating, and vote count.\n\n\nCode\nclassic_successful_action_movies &lt;- successful_movies |&gt;\n  separate_rows(genres, sep = \",\") |&gt;\n  filter(genres == \"Action\" & startYear &lt; 1999)\n\nclassic_successful_action_movies_sorted &lt;- classic_successful_action_movies |&gt;\n  arrange(desc(success_metric), desc(averageRating), desc(numVotes))\n\nclassic_successful_action_movies_sorted &lt;- classic_successful_action_movies_sorted |&gt;\n  select(primaryTitle, startYear, averageRating, numVotes, success_metric)\n\ntop_classic_successful_action_movies &lt;- classic_successful_action_movies_sorted |&gt; \n  slice_head(n = 10)\n\nprint(top_classic_successful_action_movies)\n\n\n# A tibble: 10 × 5\n   primaryTitle                  startYear averageRating numVotes success_metric\n   &lt;chr&gt;                             &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n 1 Star Wars: Episode V - The E…      1980           8.7  1401591           123.\n 2 Star Wars: Episode IV - A Ne…      1977           8.6  1471222           122.\n 3 Terminator 2: Judgment Day         1991           8.6  1194496           120.\n 4 Léon: The Professional             1994           8.5  1266394           119.\n 5 Raiders of the Lost Ark            1981           8.4  1049518           116.\n 6 Star Wars: Episode VI - Retu…      1983           8.3  1137692           116.\n 7 Aliens                             1986           8.4   784536           114.\n 8 Jurassic Park                      1993           8.2  1086595           114.\n 9 Die Hard                           1988           8.2   958730           113.\n10 Heat                               1995           8.3   731857           112.\n\n\nThe output above displays the top 10 successful classic action movies, as defined above.\nThe chosen classic movie is Aliens. This movie has not been remake in the past 25 years, has an 8.4 average IMDb rating, a 114.01 custom success rating, and over 78,000 votes.\nThese key details on the movie are pulled from the code below, and displayed in the table output.\n\nCode\n# Filter for the movie \"Aliens\"\naliens_info &lt;- top_classic_successful_action_movies |&gt;\n  filter(primaryTitle == \"Aliens\") |&gt;\n  select(primaryTitle, averageRating, numVotes, startYear, success_metric)\n\n# Create a gt table to display the details for \"Aliens\"\naliens_gt_table &lt;- aliens_info |&gt;\n  gt() |&gt;\n  tab_header(title = \"Details for the Movie: Aliens\") |&gt;\n  cols_label(\n    primaryTitle = \"Movie Title\",\n    averageRating = \"IMDb Rating\",\n    numVotes = \"Number of Votes\",\n    startYear = \"Release Year\",\n    success_metric = \"Success Metric\"\n  ) |&gt;\n  fmt_number(\n    columns = c(averageRating, success_metric),\n    decimals = 2\n  ) |&gt;\n  fmt_number(\n    columns = numVotes,\n    use_seps = TRUE,\n    decimals = 0\n  )\n\n# Print the table\nprint(aliens_gt_table)\n\n\n\n\n\n\n\nDetails for the Movie: Aliens\n\n\n\n\nMovie Title\n\n\nIMDb Rating\n\n\nNumber of Votes\n\n\nRelease Year\n\n\nSuccess Metric\n\n\n\n\n\n\nAliens\n\n\n8.40\n\n\n784,536\n\n\n1986\n\n\n114.01"
  },
  {
    "objectID": "mp02.html#putting-it-together",
    "href": "mp02.html#putting-it-together",
    "title": "mp02",
    "section": "",
    "text": "Imagine an action-packed remake of the 1986 sci-fi classic Aliens, a film that has captivated audiences with its 8.4 IMDb rating, 784,536 votes, and a success metric of 114.01. We are breathing new life into this iconic movie with a fresh, bold vision.\nLeading the charge is Peter Jackson, the legendary director behind The Lord of the Rings series. With 4 highly successful films under his belt, and one of the highest success metrics in the industry, Peter Jackson’s vision is the perfect blend of spectacle and storytelling to make this Aliens remake a sure hit.\nWe’ve also assembled a powerhouse cast. Noel Appleby and Alan Howard, both having achieved success in The Lord of the Rings: The Fellowship of the Ring, bring an undeniable chemistry to the screen. Their performances, combined with Jackson’s directorial mastery, promise to elevate this project beyond a mere remake—it’s a revitalized epic that honors the original while pushing the boundaries of modern sci-fi action.\nWith proven success across the board—genre, director, and cast—this project is a guaranteed box office hit and a thrilling adventure for fans, old and new."
  },
  {
    "objectID": "mp03.html#set-up-and-initial-exploration",
    "href": "mp03.html#set-up-and-initial-exploration",
    "title": "mp03",
    "section": "Set-Up and Initial Exploration",
    "text": "Set-Up and Initial Exploration\n\nData I: US House Election Votes from 1976 to 2022\nThis step involved downloading a dataset of the votes from all biennial congressional races in all 50 states and saving as a CSV file using a web browser. The data came from the MIT Election Data Science Lab.\nAnother dataset downloaded was the statewide presidential vote counts from 1976 to 2022, which may be found\nThe code below loads the election data into our environment.\n\n\nCode\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(gt)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(scales)\n\n\n\n\nCode\nhouse_votes &lt;- read_csv(\"~/Downloads/1976-2022-house.csv\")\npresidential_votes &lt;- read_csv(\"~/Downloads/1976-2020-president.csv\")\n\nhead(house_votes)\nhead(presidential_votes)\n\n\n\n\nData II: Congressional Boundary Files 1976 to 2012\nTask 1: Download Congressional Shapefiles 1976-2012\n&\nTask 4: Automate Zip File Extraction\nThe congressional district shapefiles, covering U.S. congressional districts from 1789 to 2012, are provided by Jeffrey B. Lewis, Brandon DeVine, Lincoln Pritcher, and Kenneth C. Martis. These files can be accessed on the Harvard Dataverse here.\nThe code below automates the download of files from the 94th to the 112th Congresses (1976-2012) and extracting the shapefiles from within the zip files.\nThe steps taken involve:\n\nDefining the function get_congressional_shapefile(), which automates the download and import of congressional district shapefiles for the specified range of congressional sessions (94 to 112)\nStoring each shapefile in a zip archive while the function checks if the file already exists locally. If not, file is downloaded. This avoids any redundant downloads.\nUnzipping downloaded zip file, and the shapefile within is read into R using read_sf()\n\nA few additional steps were added to handle the large and complex shapefiles. The code includes a few lines involved in standardizing and simplifying the shapefiles.\nThe steps taken involve:\n\nst_transform() is applied to reproject each shapefile to a common Coordinate Reference System (CRS), which uses latitude and longitude coordinates, which are useful when working with geographic data\nst_simplify() simplifies the geometries of each shapefile. This reduces the number of intricate edges and details, making for improved performance while maintaining accuracy\nsf_use_s2(FALSE) is set to avoid any issues with spherical geometry when simplifying data in a projected CRS\n\nFinally, we consolidate all shapefiles into the spatial object ALL_SHAPES.\n\n\nCode\nsf::sf_use_s2(FALSE)\n\nget_congressional_shapefile &lt;- function(congress_number) {\n  BASE_URL &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n  fname &lt;- paste0(\"districts\", formatC(congress_number, width = 3, format = \"d\", flag = \"0\"), \".zip\")\n  \n  if (!file.exists(fname)) {\n    FILE_URL &lt;- paste0(BASE_URL, fname)\n    download.file(FILE_URL, destfile = fname)\n  }\n  \n  zip_contents &lt;- unzip(fname, exdir = tempdir())\n  shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n  sf_data &lt;- read_sf(shp_file)\n  \n  sf_data &lt;- st_transform(sf_data, crs = 3857)\n  \n  sf_data &lt;- st_simplify(sf_data, dTolerance = 1000)\n  \n  sf_data &lt;- st_transform(sf_data, crs = 4326)\n\n  sf_data$congress_number &lt;- congress_number\n  \n  return(sf_data)\n}\n\nALL_SHAPES &lt;- bind_rows(lapply(94:112, get_congressional_shapefile))\n\n\n\n\nData III: Congressional Boundary Files 2014 to Present\nTask 2: Download Congressional Shapefiles 2014-2022\nOriginal function (not used):\n\n\nCode\nget_census_shapefile &lt;- function(year, congress_number) {\n    BASE_URL &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", year, \"/CD/\")\n    fname &lt;- paste0(\"tl_\", year, \"_us_cd\", congress_number, \".zip\")\n    \n    if (!file.exists(fname)) {\n        FILE_URL &lt;- paste0(BASE_URL, fname)\n        download.file(FILE_URL, destfile = fname)\n    }\n    \n    zip_contents &lt;- unzip(fname, exdir = tempdir())\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n    \n    return(sf_data)\n}\n\nfor (year in 2014:2022) {\n    congress_number &lt;- if (year &lt;= 2015) {\n      114\n    } else if (year &lt;= 2017) {\n      115\n    } else {\n      116\n    }\n    \n    \n    sf_data &lt;- get_census_shapefile(year, congress_number)\n    assign(paste0(\"congress_\", year, \"_sf\"), sf_data)\n}\n\n\nImproved functions, which combines all census congress files and adds CRS standardization and simplification:\n\n\nCode\nsf::sf_use_s2(FALSE)\n\nget_census_shapefile &lt;- function(year, congress_number) {\n    BASE_URL &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", year, \"/CD/\")\n    fname &lt;- paste0(\"tl_\", year, \"_us_cd\", congress_number, \".zip\")\n    \n    if (!file.exists(fname)) {\n        FILE_URL &lt;- paste0(BASE_URL, fname)\n        download.file(FILE_URL, destfile = fname)\n    }\n    \n    zip_contents &lt;- unzip(fname, exdir = tempdir())\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n    \n    sf_data &lt;- st_transform(sf_data, crs = 3857)\n    \n    sf_data &lt;- st_simplify(sf_data, dTolerance = 1000)\n    \n    sf_data &lt;- st_transform(sf_data, crs = 4326)\n    \n    sf_data$year &lt;- year\n    sf_data$congress_number &lt;- congress_number\n    \n    return(sf_data)\n}\n\nALL_CENSUS_SHAPES &lt;- bind_rows(lapply(2014:2022, function(year) {\n    congress_number &lt;- if (year &lt;= 2015) {\n      114\n    } else if (year &lt;= 2017) {\n      115\n    } else {\n      116\n    }\n    \n    sf_data &lt;- get_census_shapefile(year, congress_number)\n    return(sf_data)\n}))\n\n\n\n\nInitial Exploration of Vote Count Data\nTask 3: Exploration of Vote Count Data\n\nStates Gaining and Losing House Seats (1976-2022)\nFor a better understanding to how the number of Representatives per state is determined, refer to this essay for more information.\n\nIn summary,\nThe number of US House seats per state can change every 10 years, following the decennial census conducted by the US Census Bureau. This census determines the population distribution across states, and seats in the House of Representatives are then reapportioned based on these updated population counts.\nThe national total number of House seats is fixed at 435, but individual states may gain or lose seats based on shifts in the population.\n\nFirst attempt: Not showing each state\n\n\nCode\nhouse_seats_by_year &lt;- house_votes |&gt;\n  group_by(year, state_po) |&gt;\n  summarize(num_districts = n_distinct(district), .groups = 'drop')\n\nseat_changes &lt;- house_seats_by_year |&gt;\n  arrange(state_po, year) |&gt;\n  group_by(state_po) |&gt;\n  mutate(seat_change = num_districts - lag(num_districts, default = first(num_districts))) |&gt;\n  ungroup()\n\nlargest_changes &lt;- seat_changes |&gt;\n  filter(!is.na(seat_change)) |&gt;\n  arrange(desc(abs(seat_change))) |&gt;\n  top_n(5, abs(seat_change))  # Selecting the top 5 largest changes\n\nggplot(seat_changes, aes(x = year, y = seat_change, fill = seat_change &gt; 0)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  # Add a prominent line at y = 0\n  geom_hline(yintercept = 0, color = \"black\", size = 1.2, linetype = \"dashed\") +\n  # Add markers for years with the largest changes\n  geom_point(data = largest_changes, aes(x = year, y = seat_change), \n             color = \"darkorange\", size = 3, shape = 21, fill = \"white\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\"), \n                    labels = c(\"Loss\", \"Gain\")) +\n  labs(title = \"Yearly Changes in US House Seats by State\",\n       x = \"Year\",\n       y = \"Seat Change\",\n       fill = \"Seat Change\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\nRecognizing the need for consistent state abbreviations to enhance the clarity of visualizations, I identified that the house_seats_by_year table lacked state abbreviations, which would make presenting net changes by state more straightforward. To address this, I used R’s built-in datasets, specifically state.name and state.abb, which contain full state names and their corresponding abbreviations.\nThe next few chunks of code modify the house_seats_by_year table.\nstate_abbreviations &lt;- data.frame( state_name = state.name, state_abbreviation = state.abb )\nAdjusting title case in house_votes table to match the format in house_seats_by_year table, for the state column.\nWith both tables now in a consistent format, I have house_seats_by_year and house_votes tables where state names are in title case, and house_seats_by_year now includes a state_abbreviation column. These adjustments ensure that visualizations involving state abbreviations will be clean and standardized, with both tables ready for further analysis.\nNow, going back to the original question, the code below analyzes changes in the US House representation by comparing the number of congressional districts in each state between 1976 and 2022.\nThe difference in the number of House seats in 2022 to 1976 is calculated.\nThe resulting data, found in the bar chart below, shows how many House seats each state gained or lost, from 1976 to 2022, excluding any states with no net change.\n\n\nCode\nseats_1976 &lt;- house_seats_by_year |&gt;\n  filter(year == 1976) |&gt;\n  select(state_po, num_districts_1976 = num_districts)\n\nseats_2022 &lt;- house_seats_by_year |&gt;\n  filter(year == 2022) |&gt;\n  select(state_po, num_districts_2022 = num_districts)\n\nseat_differences &lt;- seats_2022 |&gt;\n  inner_join(seats_1976, by = \"state_po\") |&gt;\n  mutate(seat_difference = num_districts_2022 - num_districts_1976) |&gt;\n  filter(seat_difference != 0)  # Exclude states with no difference\n\nggplot(seat_differences, aes(x = reorder(state_po, seat_difference), y = seat_difference, fill = seat_difference &gt; 0)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\"), labels = c(\"Gain\", \"Loss\")) +\n  labs(title = \"Change in US House Seats by State (1976 to 2022)\",\n       x = \"State\",\n       y = \"Seat Difference\",\n       fill = \"Seat Change\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nThe code below highlights the top 5 states with the greatest net difference in number of House seats from 1976 to 2022.\n\n\nCode\n# Filter the top 5 states by absolute seat difference (gain or loss)\ntop_5_seat_differences &lt;- seat_differences |&gt;\n  arrange(desc(abs(seat_difference))) |&gt;\n  slice(1:5) |&gt;\n  select(state_po, seat_difference) |&gt;\n  rename(State = state_po, `Seat Difference` = seat_difference)\n\ntop_5_seat_differences |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 States with Seat Differences (1976 to 2022)\"\n  ) |&gt;\n  fmt_integer(\n    columns = vars(`Seat Difference`)\n  ) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = everything()\n  )\n\n\n\n\n\n\n\n\nTop 5 States with Seat Differences (1976 to 2022)\n\n\nState\nSeat Difference\n\n\n\n\nTX\n14\n\n\nFL\n13\n\n\nNY\n−13\n\n\nCA\n9\n\n\nOH\n−8\n\n\n\n\n\n\n\nAs shown above, we find TX has the greatest difference, with 14 additional seats as of 2022, compared to 1976.\n\nAnalyzing NY’s “fusion” voting system\n\nPlease go here for a further understanding of Fusion Voting\n\n\n\nThe code below involves a few steps to get us to test if there are any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes they received from their “major party line”, instead of their total number of votes across all lines.\nWe limit our data to New York elections, where fusion voting occurs.\nWe create the data frames major_party_votes and total_votes to calculate and compare the election outcomes under different scenarios.\n\nmajor_party_votes (hypothetical outcome): contains the total number of votes each candidate received from their major party line only (Democrat or Republican) and ignores any votes they received from minor parties. 1\ntotal_votes (actual outcome): contains the actual total number of votes each candidate received across all party lines, which is the real vote total used in New York State’s fusion system\n\nFor each district in New York State, we check if the candidate with the highest major_party_votes differs from the candidate with the highest total_votes.\nOur data frame hypothetical_winners lists any elections where the outcome would have changed if only major party line votes were counted, or our hypothetical scenario. 2\n\n\nCode\nny_elections &lt;- house_votes |&gt;\n  filter(state_po == \"NY\" & candidatevotes &gt;= 0 ) |&gt;\n  mutate(major_party_line = if_else(!is.na(party) & (party == \"DEMOCRAT\" | party == \"REPUBLICAN\"), \"major\", \"minor\"))\n\nprint(table(ny_elections$major_party_line))\n\n\n\nmajor minor \n 1419  2560 \n\n\nCode\nmajor_party_votes &lt;- ny_elections |&gt;\n  filter(major_party_line == \"major\") |&gt;\n  group_by(year, district, candidate, party) |&gt;\n  summarize(major_party_votes = sum(candidatevotes), .groups = \"drop\")\n\ntotal_votes &lt;- ny_elections |&gt;\n  group_by(year, district, candidate, party) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\")\n\nelection_outcomes &lt;- major_party_votes |&gt;\n  left_join(total_votes, by = c(\"year\", \"district\", \"candidate\", \"party\"))\n\nhypothetical_winners &lt;- election_outcomes |&gt;\n  group_by(year, district) |&gt;\n  summarize(\n    actual_winner = candidate[which.max(total_votes)],\n    actual_winner_party = party[which.max(total_votes)],\n    hypothetical_winner = candidate[which.max(major_party_votes)],\n    hypothetical_winner_party = party[which.max(major_party_votes)],\n    .groups = \"drop\"\n  ) |&gt;\n  filter(actual_winner != hypothetical_winner)\n\n\n\nPresidential Candidate Performance Relative to Congressional Candidates\n\nGiven this is an in-depth question, we will chunk the code into steps, for better understanding.\n\nStep 1: Aggregating Votes for House and Presidential Races by Year, State, and Party\n\n\nCode\n# Aggregate votes for house races\nhouse_votes_aggregated &lt;- house_votes |&gt;\n  filter(!is.na(party)) |&gt;\n  group_by(year, state_po, party) |&gt;\n  summarize(house_total_votes = sum(candidatevotes), .groups = \"drop\")\n\n# Aggregate votes for presidential races\npresidential_votes_aggregated &lt;- presidential_votes |&gt;\n  filter(!is.na(party_simplified)) |&gt;\n  group_by(year, state_po, party = party_simplified) |&gt;\n  summarize(presidential_total_votes = sum(candidatevotes), .groups = \"drop\")\n\n\nStep 2: Merge Aggregated Tables\n\n\nCode\ncombined_votes &lt;- house_votes_aggregated |&gt;\n  inner_join(presidential_votes_aggregated, by = c(\"year\", \"state_po\", \"party\"))\n\n\nStep 3: Calculate Difference Between Presidential and House Votes\n\nvote_difference = presidential_total_votes - house_total_votes\n\n\n\nCode\ncombined_votes &lt;- combined_votes |&gt;\n  mutate(vote_difference = presidential_total_votes - house_total_votes, ahead_or_behind = if_else(vote_difference &gt;0, \"Ahead\", \"Behind\"))\n\n\nStep 4: Generate Visualization. The code below creates a time series plot showing the trend of average vote differences between presidential and congressional candidates across major parties from 1976 to 2020.\n\n\nCode\naverage_vote_difference &lt;- combined_votes |&gt;\n  group_by(year, party) |&gt;\n  summarize(avg_vote_difference = mean(vote_difference), .groups = \"drop\")\n\nggplot(average_vote_difference |&gt; filter(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")), \n       aes(x = year, y = avg_vote_difference, color = party)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n  labs(\n    title = \"Average Vote Difference Between Presidential and Congressional Candidates\",\n    subtitle = \"Positive values indicate presidential candidates ran ahead of congressional candidates\",\n    x = \"Year\",\n    y = \"Average Vote Difference\",\n    color = \"Party\"\n  ) +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    axis.text = element_text(size = 10),\n    axis.title = element_text(size = 12),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\nFrom analyzing the time series plot above, Republican presidential candidates (in red) typically received more votes than congressional candidates in the 1980’s and early 1990s. Democratic presidential candidates (in blue) tended to receive fewer votes than their congressional counterparts throughout this same time period (1980s- early 1990s).\nThe code below aims at answering the second part to #3 - Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\nThis code identifies the 10 states with the largest vote differences between presidential and congressional candidates from the same party. It then creates a series of line plots, one for each state, showing whether presidential candidates received more or fewer votes than congressional candidates over time. Positive values mean presidential candidates ran ahead, while negative values mean they ran behind, making it easy to see trends for each state and party.\n\n\nCode\navg_vote_diff_states &lt;- combined_votes |&gt;\n  group_by(year,state_po,party) |&gt;\n  summarize(avg_vote_difference = mean(vote_difference, na.rm = TRUE), .groups = \"drop\")\n\n\ntop_states &lt;- avg_vote_diff_states |&gt;\n  group_by(state_po) |&gt;\n  summarize(max_abs_diff = max(abs(avg_vote_difference), na.rm = TRUE)) |&gt;\n  arrange(desc(max_abs_diff)) |&gt;\n  slice_head(n = 10) |&gt;\n  pull(state_po)\n\nggplot(avg_vote_diff_states |&gt;\n         filter(state_po %in% top_states, party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")),\n       aes(x = year, y = avg_vote_difference, color = party, group = party)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n  scale_y_continuous(labels = scales::comma) +  # Format y-axis without scientific notation\n  labs(\n    title = \"Average Vote Difference Between Presidential and Congressional Candidates by Top 10 States\",\n    subtitle = \"Positive values indicate presidential candidates ran ahead of congressional candidates\",\n    x = \"Year\",\n    y = \"Average Vote Difference\",\n    color = \"Party\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    axis.text.x = element_text(size = 8, angle = 45, hjust = 1),\n    axis.text.y = element_text(size = 8),\n    strip.text = element_text(size = 10, face = \"bold\"),\n    plot.margin = margin(10, 10, 10, 10)\n  ) +\n  facet_wrap(~ state_po, scales = \"free_y\", ncol = 5)  # Arrange in 2 rows for readability\n\n\n\n\n\n\n\n\n\n\n\nImporting and Plotting Shape File Data\nTask 4: Automate Zip File Extraction\nPlease note, the code for Task 1, simultaneously completes both tasks 1 and 4.\nThe function get_congressional_shapefile() combines downloading the files, extracitng the shapefile, and reading into R. The function get_congressional_shapefile() serves the same purpose as read_shp_from_zip(), as provided in the sample code for Task 4.\nGiven the function get_congressional_shapefile() handles the downloading, extraction, and reading all in one step, this function is even more streamlined than using the separate read_shp_from_zip() function.\nSee a more detailed explanation for the combined steps taken for Tasks 1 & 4 outlined under Task 1.\nTask 5: Chloropleth Visualization of the 2000 Presidential Election Electoral College Results\nThe below code involves downloading, extracting and loading a US states shapefile from the US Census Bureau.\n\n\nCode\nget_state_shapefile &lt;- function() {\n    BASE_URL &lt;- \"https://www2.census.gov/geo/tiger/TIGER2024/STATE/\"\n    fname &lt;- \"tl_2024_us_state.zip\"\n    \n    if (!file.exists(fname)) {\n        FILE_URL &lt;- paste0(BASE_URL, fname)\n        download.file(FILE_URL, destfile = fname)\n    }\n    \n    zip_contents &lt;- unzip(fname, exdir = tempdir())\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    state_shapefile &lt;- read_sf(shp_file)\n    \n    return(state_shapefile)\n}\n\nstate_shapefile &lt;- get_state_shapefile()\n\nprint(state_shapefile)\n\n\nSimple feature collection with 56 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.2311 ymin: -14.60181 xmax: 179.8597 ymax: 71.43979\nGeodetic CRS:  NAD83\n# A tibble: 56 × 16\n   REGION DIVISION STATEFP STATENS  GEOID GEOIDFQ     STUSPS NAME    LSAD  MTFCC\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n 1 3      5        54      01779805 54    0400000US54 WV     West V… 00    G4000\n 2 3      5        12      00294478 12    0400000US12 FL     Florida 00    G4000\n 3 2      3        17      01779784 17    0400000US17 IL     Illino… 00    G4000\n 4 2      4        27      00662849 27    0400000US27 MN     Minnes… 00    G4000\n 5 3      5        24      01714934 24    0400000US24 MD     Maryla… 00    G4000\n 6 1      1        44      01219835 44    0400000US44 RI     Rhode … 00    G4000\n 7 4      8        16      01779783 16    0400000US16 ID     Idaho   00    G4000\n 8 1      1        33      01779794 33    0400000US33 NH     New Ha… 00    G4000\n 9 3      5        37      01027616 37    0400000US37 NC     North … 00    G4000\n10 1      1        50      01779802 50    0400000US50 VT     Vermont 00    G4000\n# ℹ 46 more rows\n# ℹ 6 more variables: FUNCSTAT &lt;chr&gt;, ALAND &lt;dbl&gt;, AWATER &lt;dbl&gt;,\n#   INTPTLAT &lt;chr&gt;, INTPTLON &lt;chr&gt;, geometry &lt;MULTIPOLYGON [°]&gt;\n\n\nFiltering the state_shapefile to only include the 50 states and District of Columbia.\n\n\nCode\nstate_shapefile_filtered &lt;- state_shapefile |&gt; \n  filter(STATEFP %in% sprintf(\"%02d\", 1:56))\n\n\nWe then prepare the election data for the 2000 presidential election.\n\n\nCode\nelection_2000 &lt;- presidential_votes |&gt;\n  filter(year == 2000, office == \"US PRESIDENT\")\n\nstate_winners_2000 &lt;- election_2000 |&gt;\n  group_by(state_po) |&gt;\n  summarize(total_votes = sum(candidatevotes),\n            winner = candidate[which.max(candidatevotes)],\n            party = party_simplified[which.max(candidatevotes)]) |&gt;\n  ungroup()\n\nelectoral_votes &lt;- data.frame(\n  state_po = c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"),\n  ElectoralVotes = c(9, 3, 8, 6, 54, 8, 8, 3, 25, 13, 4, 4, 22, 12, 7, 6, 8, 9, 4, 10, 12, 18, 10, 7, 11, 3, 5, 4, 4, 15, 5, 33, 14, 3, 21, 8, 7, 23, 4, 8, 3, 11, 32, 5, 3, 13, 11, 5, 11, 3)\n)\n\n\nmap_data_2000 &lt;- state_shapefile_filtered |&gt;\n  left_join(state_winners_2000, by = c(\"STUSPS\" = \"state_po\")) |&gt;\n  rename(Winner = winner, Party = party)\n\nmap_data_2000 &lt;- map_data_2000 |&gt; \n  left_join(electoral_votes, by = c(\"STUSPS\" = \"state_po\"))\n\n\nNext, we join our election_data_2000 with the state shapefile, to create the map of the US with state boundaries.\nAfter some research, found the cowplot package will allow us to place the mainland US map alongside smaller insets for Alaska and Hawaii.\n\n\nCode\nlibrary(cowplot)\n\nmainland_map &lt;- ggplot(map_data_2000) +\n  geom_sf(aes(fill = Winner), color = \"white\") +\n  scale_fill_manual(values = c(\"Bush\" = \"red\", \"Gore\" = \"blue\", \"None\" = \"grey80\")) +  # Added \"None\" for missing values\n  geom_sf_text(aes(label = ElectoralVotes), size = 3, color = \"black\", check_overlap = TRUE) +\n  theme_minimal() +\n  labs(\n    title = \"2000 Presidential Election Electoral College Results\",\n    fill = \"Winner\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  coord_sf(crs = st_crs(4326), xlim = c(-130, -65), ylim = c(24, 50))\n\nalaska_map &lt;- ggplot(map_data_2000 |&gt; filter(STUSPS == \"AK\")) +\n  geom_sf(aes(fill = Winner), color = \"white\") +\n  scale_fill_manual(values = c(\"Bush\" = \"red\", \"Gore\" = \"blue\", \"None\" = \"grey80\")) +  # Added \"None\" for missing values\n  geom_sf_text(aes(label = ElectoralVotes), size = 3, color = \"black\") +\n  coord_sf(crs = st_crs(4326), xlim = c(-180, -130), ylim = c(50, 72)) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\nhawaii_map &lt;- ggplot(map_data_2000 |&gt; filter(STUSPS == \"HI\")) +\n  geom_sf(aes(fill = Winner), color = \"white\") +\n  scale_fill_manual(values = c(\"Bush\" = \"red\", \"Gore\" = \"blue\", \"None\" = \"grey80\")) +  # Added \"None\" for missing values\n  geom_sf_text(aes(label = ElectoralVotes), size = 3, color = \"black\") +\n  coord_sf(crs = st_crs(4326), xlim = c(-161, -154), ylim = c(18, 23)) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\nfinal_map &lt;- ggdraw() +\n  draw_plot(mainland_map, 0, 0, 1, 1) +\n  draw_plot(alaska_map, 0.05, 0.05, 0.2, 0.2) +  # Position and size Alaska inset\n  draw_plot(hawaii_map, 0.3, 0.05, 0.1, 0.1)    # Position and size Hawaii inset\n\nprint(final_map)\n\n\n\n\n\n\n\n\n\nWe find that all 50 states + DC are now properly accounted for, with their number of electoral college votes by state as of the 2000 election.\nWe now try using the tmap package, in the attempts to properly shade each state red or blue, based on the winning candidate from the 2000 election.\n\n\nCode\nlibrary(tmap)\nlibrary(gridExtra)\nlibrary(grid)\n\nwinner_colors &lt;- c(\"Bush\" = \"red\", \"Gore\" = \"blue\", \"None\" = \"grey80\")\n\ntmap_mode(\"plot\")\n\ntitle_size &lt;- 0.8 \n\n# Create the mainland U.S. map without title\nmainland_map &lt;- tm_shape(map_data_2000 |&gt; filter(!STUSPS %in% c(\"AK\", \"HI\"))) +\n  tm_polygons(\"Winner\", palette = winner_colors, title = \"Winner\") +\n  tm_text(\"ElectoralVotes\", size = 0.6, col = \"black\") +\n  tm_layout(legend.position = c(\"left\", \"bottom\"), frame = FALSE)\n\nalaska_map &lt;- tm_shape(map_data_2000 |&gt; filter(STUSPS == \"AK\")) +\n  tm_polygons(\"Winner\", palette = winner_colors) +\n  tm_text(\"ElectoralVotes\", size = 0.6, col = \"black\") +\n  tm_layout(title = \"Alaska\", title.size = title_size, title.position = c(\"center\", \"top\"), legend.show = FALSE, frame = FALSE)\n\nhawaii_map &lt;- tm_shape(map_data_2000 |&gt; filter(STUSPS == \"HI\")) +\n  tm_polygons(\"Winner\", palette = winner_colors) +\n  tm_text(\"ElectoralVotes\", size = 0.6, col = \"black\") +\n  tm_layout(title = \"Hawaii\", title.size = title_size, title.position = c(\"center\", \"top\"), legend.show = FALSE, frame = FALSE)\n\nmainland_grob &lt;- tmap_grob(mainland_map)\n\n\n\n\n\n\n\n\n\nCode\nalaska_grob &lt;- tmap_grob(alaska_map)\n\n\n\n\n\n\n\n\n\nCode\nhawaii_grob &lt;- tmap_grob(hawaii_map)\n\n\n\n\n\n\n\n\n\nCode\ngrid.arrange(\n  arrangeGrob(\n    textGrob(\"2000 Presidential Election Electoral College Results\", gp = gpar(fontsize = 16, fontface = \"bold\")),\n    mainland_grob,\n    heights = c(0.1, 0.9)\n  ),\n  alaska_grob, hawaii_grob,\n  layout_matrix = rbind(c(1, 1), c(2, 3)),\n  heights = c(2, 1), widths = c(2, 1)\n)\n\n\n\n\n\n\n\n\n\nTask 6: Advanced Chloropleth Visualization of Electoral College Results\nThe code below once again makes use of the library tmap. You will find there are some debugging steps included in this code as there were attempts made to adjust the grey space/NAs in the resulting facet plot, before moving on to a different approach.\n\n\nCode\nlibrary(tmap)\n\nstate_winners_all_years &lt;- presidential_votes |&gt;\n  filter(office == \"US PRESIDENT\") |&gt;\n  group_by(year, state_po) |&gt;\n  summarize(\n    total_votes = sum(candidatevotes),\n    Winner = candidate[which.max(candidatevotes)],\n    party = party_simplified[which.max(candidatevotes)]\n  ) |&gt;\n  ungroup()\n\nstate_winners_with_votes &lt;- state_winners_all_years |&gt;\n  left_join(electoral_votes, by = \"state_po\")\n\nmap_data_with_year &lt;- state_shapefile_filtered |&gt;\n  left_join(state_winners_with_votes, by = c(\"STUSPS\" = \"state_po\"))\n\nmissing_data &lt;- map_data_with_year |&gt; filter(is.na(party))\nif (nrow(missing_data) &gt; 0) {\n  print(\"Warning: Missing party data for some states/years.\")\n  print(missing_data)\n}\n\nmap_data_with_year &lt;- map_data_with_year |&gt; filter(!is.na(party))\n\nwinner_colors &lt;- c(\"REPUBLICAN\" = \"red\", \"DEMOCRAT\" = \"blue\")\n\ntmap_mode(\"plot\")\n\ntm_shape(map_data_with_year) +\n  tm_polygons(\"party\", palette = winner_colors, title = \"Winner\") +\n  tm_text(\"ElectoralVotes\", size = 0.6, col = \"black\") +\n  tm_facets(by = \"year\", ncol = 1) +  # Facet by year, single-column layout\n  tm_layout(\n    main.title = \"Electoral College Results Over Time\",\n    legend.position = c(\"left\", \"bottom\"),\n    title.size = 1.2,\n    frame = FALSE\n  )\n\n\n\n\n\n\n\n\n\nThe code below makes use of the libraries tools and gifski to successfully generate an animated chloropleth, displaying presidential election results throughout the 21st century.\n\n\nCode\nlibrary(tools)\nlibrary(gifski)\n\nparties &lt;- presidential_votes |&gt;\n  mutate(state = toTitleCase(tolower(state))) |&gt;\n  filter(state != \"Hawaii\" & state != \"Alaska\" & year &gt;= 2000, office == \"US PRESIDENT\") |&gt;\n  inner_join(state_shapefile_filtered, by = c(\"state\" = \"NAME\")) |&gt;\n  group_by(state, year) |&gt;\n  mutate(state_winner = party_simplified[which.max(candidatevotes)]) |&gt;\n  ungroup()\n\ntmap_mode(\"plot\")\n\nwinner_colors &lt;- c(\"REPUBLICAN\" = \"red\", \"DEMOCRAT\" = \"blue\", \"DEMOCRATIC-FARMER-LABOR\" = \"lightblue\")\n\nanim &lt;- parties |&gt;\n  filter(year &gt;= 2000) |&gt;\n  st_as_sf() |&gt;\n  tm_shape() +\n  tm_polygons(\"state_winner\", title = \"Party\", palette = winner_colors) +\n  tm_facets(along = \"year\", free.coords = FALSE) +\n  tm_text(\"state_po\", size = 0.3, col = \"white\", fontface = \"bold\")\n\ntmap_animation(anim, delay = 50, filename = \"anim_map.gif\")\n\n\nCreating frames\n\n\n===========================\n\n\n=============\n\n\n=============\n\n\n==============\n\n\n=============\n\n\n\nCreating animation\nAnimation saved to /Users/christiecannon/STA9750-2024-FALL/anim_map.gif \n\n\nCode\nknitr::include_graphics(\"anim_map.gif\")\n\n\n\n\n\n\n\n\n\n\n\nComparing the Effects of ECV Allocation Rules\nPlease note the various state Electoral College Vote allocation methods:\n\n\nState-Wide Winner-Take-All\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\nState-Wide Proportional\nNational Proportional\n\n\nState-Wide Winner-Take-All\n\n\nCode\nelection_results &lt;- presidential_votes |&gt;\n  filter(office == \"US PRESIDENT\") |&gt;\n  left_join(electoral_votes, by = \"state_po\")\n\nstate_winner_take_all &lt;- election_results |&gt;\n  group_by(year, state_po) |&gt;\n  slice_max(candidatevotes, with_ties = FALSE) |&gt;\n  summarize(\n    Winner = first(candidate),\n    Party = first(party_simplified),\n    ElectoralVotes = if_else(state_po == \"DC\" & is.na(first(ElectoralVotes)), 3, first(ElectoralVotes)),\n    .groups = \"drop\"\n  )\n\nstate_wide_ecv_allocation &lt;- state_winner_take_all |&gt;\n  group_by(year, Winner, Party) |&gt;\n  summarize(Total_ECVs = sum(ElectoralVotes), .groups = \"drop\")\n\nstate_wide_winner &lt;- state_wide_ecv_allocation |&gt;\n  group_by(year) |&gt;\n  filter(Total_ECVs == max(Total_ECVs)) |&gt;\n  select(year, Winner, Party, Total_ECVs) |&gt;\n  ungroup()\n\nhistorical_winner &lt;- election_results |&gt;\n  group_by(year) |&gt;\n  filter(candidatevotes == max(candidatevotes)) |&gt;\n  summarize(\n    Historical_Winner = first(candidate),\n    Historical_Party = first(party_simplified),\n    .groups = \"drop\"\n  )\n\ncomparison_state_wide &lt;- state_wide_winner |&gt;\n  left_join(historical_winner, by = \"year\") |&gt;\n  mutate(\n    Consistent = if_else(Winner == Historical_Winner, \"Yes\", \"No\")\n  )\n\n\nprint(comparison_state_wide)\n\n\n# A tibble: 12 × 7\n    year Winner   Party Total_ECVs Historical_Winner Historical_Party Consistent\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;            &lt;chr&gt;     \n 1  1976 CARTER,… DEMO…        292 FORD, GERALD      REPUBLICAN       No        \n 2  1980 REAGAN,… REPU…        456 REAGAN, RONALD    REPUBLICAN       Yes       \n 3  1984 REAGAN,… REPU…        525 REAGAN, RONALD    REPUBLICAN       Yes       \n 4  1988 BUSH, G… REPU…        431 BUSH, GEORGE H.W. REPUBLICAN       Yes       \n 5  1992 CLINTON… DEMO…        370 CLINTON, BILL     DEMOCRAT         Yes       \n 6  1996 CLINTON… DEMO…        379 CLINTON, BILL     DEMOCRAT         Yes       \n 7  2000 BUSH, G… REPU…        271 GORE, AL          DEMOCRAT         No        \n 8  2004 BUSH, G… REPU…        279 KERRY, JOHN       DEMOCRAT         No        \n 9  2008 OBAMA, … DEMO…        368 OBAMA, BARACK H.  DEMOCRAT         Yes       \n10  2012 OBAMA, … DEMO…        342 OBAMA, BARACK H.  DEMOCRAT         Yes       \n11  2016 TRUMP, … REPU…        301 CLINTON, HILLARY  DEMOCRAT         No        \n12  2020 BIDEN, … DEMO…        310 BIDEN, JOSEPH R.… DEMOCRAT         Yes       \n\n\nVisualizing the results of State-Wide Winner-Take-All\n\n\nCode\nconsistency_summary &lt;- comparison_state_wide |&gt;\n  summarize(\n    Consistent_Count = sum(Consistent == \"Yes\"),\n    Different_Count = sum(Consistent == \"No\"),\n    Total_Elections = n(),\n    Consistent_Percentage = Consistent_Count / Total_Elections * 100,\n    Different_Percentage = Different_Count / Total_Elections * 100\n  )\n\nconsistency_summary |&gt;\n  gt() |&gt;\n  fmt_number(\n    columns = c(Consistent_Percentage, Different_Percentage),\n    decimals = 2\n  ) |&gt;\n  cols_label(\n    Consistent_Count = \"Consistent with Historical\",\n    Different_Count = \"Different from Historical\",\n    Total_Elections = \"Total Elections\",\n    Consistent_Percentage = \"Consistent (%)\",\n    Different_Percentage = \"Different (%)\"\n  ) |&gt;\n  tab_header(\n    title = \"State-Wide Winner-Take-All Scheme Consistency Summary\"\n  )\n\n\n\n\n\n\n\n\nState-Wide Winner-Take-All Scheme Consistency Summary\n\n\nConsistent with Historical\nDifferent from Historical\nTotal Elections\nConsistent (%)\nDifferent (%)\n\n\n\n\n8\n4\n12\n66.67\n33.33\n\n\n\n\n\n\n\n\n\nCode\nggplot(comparison_state_wide, aes(x = year)) +\n  geom_line(aes(y = Winner, color = \"Scheme Winner\"), size = 1) +\n  geom_point(aes(y = Winner, color = \"Scheme Winner\"), size = 2) +\n  geom_line(aes(y = Historical_Winner, color = \"Historical Winner\"), size = 1, linetype = \"dashed\") +\n  geom_point(aes(y = Historical_Winner, color = \"Historical Winner\"), size = 2, shape = 17) +\n  labs(\n    title = \"Comparison of State-Wide Winner-Take-All Scheme with Historical Winners\",\n    x = \"Election Year\",\n    y = \"Winning Candidate\",\n    color = \"Legend\"\n  ) +\n  scale_color_manual(values = c(\"Scheme Winner\" = \"blue\", \"Historical Winner\" = \"red\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\n\n\nCode\nhistorical_results &lt;- presidential_votes |&gt;\n    filter(office == \"US PRESIDENT\") |&gt;\n    group_by(year, state_po) |&gt;\n    summarize(\n        HistoricalWinner = candidate[which.max(candidatevotes)],\n        HistoricalParty = party_simplified[which.max(candidatevotes)]\n    ) |&gt;\n    ungroup()\n\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\nCode\ndistrict_winner_take_all &lt;- election_results |&gt;\n    group_by(year, state_po) |&gt;\n    reframe(\n        Winner = candidate[which.max(candidatevotes)],\n        Party = party_simplified[which.max(candidatevotes)],\n        ElectoralVotes = max(ElectoralVotes)\n    )\n\n\nconsistency_district &lt;- district_winner_take_all |&gt;\n    left_join(historical_results, by = c(\"year\", \"state_po\")) |&gt;\n    reframe(\n        year,\n        state_po,\n        Winner,\n        HistoricalWinner,\n        Consistency = if_else(Winner == HistoricalWinner, \"Yes\", \"No\")\n    )\n\nconsistency_summary_district &lt;- consistency_district |&gt;\n    count(Consistency) |&gt;\n    mutate(\n        `Total Elections` = sum(n),\n        `Consistent (%)` = round((n / `Total Elections`) * 100, 2),\n        `Different (%)` = 100 - `Consistent (%)`\n    ) |&gt;\n    rename(\n        `Consistent with Historical` = n,\n        `Different from Historical` = `Different (%)`\n    )\n\n\ngt(consistency_summary_district) |&gt;\n    tab_header(\n        title = \"District-Wide Winner-Take-All Scheme Consistency Summary\"\n    )\n\n\n\n\n\n\n\n\nDistrict-Wide Winner-Take-All Scheme Consistency Summary\n\n\nConsistency\nConsistent with Historical\nTotal Elections\nConsistent (%)\nDifferent from Historical\n\n\n\n\nYes\n612\n612\n100\n0\n\n\n\n\n\n\n\nState-Wide Proportional\n\n\nCode\nstate_proportional &lt;- election_results |&gt;\n    group_by(year, state_po) |&gt;\n    mutate(\n        vote_share = candidatevotes / sum(candidatevotes),\n        proportional_ecv = round(vote_share * max(ElectoralVotes))\n    ) |&gt;\n    filter(proportional_ecv &gt; 0) |&gt;  # Only keep candidates with non-zero ECVs\n    reframe(\n        year,\n        state_po,\n        Winner = candidate,\n        Party = party_simplified,\n        ElectoralVotes = proportional_ecv\n    )\n\nconsistency_proportional &lt;- state_proportional |&gt;\n    group_by(year, state_po) |&gt;\n    slice_max(order_by = ElectoralVotes, n = 1, with_ties = FALSE) |&gt;\n    ungroup() |&gt;\n    left_join(historical_results, by = c(\"year\", \"state_po\")) |&gt;\n    reframe(\n        year,\n        state_po,\n        Winner,\n        HistoricalWinner,\n        Consistency = if_else(Winner == HistoricalWinner, \"Yes\", \"No\")\n    )\n\nconsistency_summary_proportional &lt;- consistency_proportional |&gt;\n    count(Consistency) |&gt;\n    mutate(\n        `Total Elections` = sum(n),\n        `Consistent (%)` = round((n / `Total Elections`) * 100, 2),\n        `Different (%)` = 100 - `Consistent (%)`\n    ) |&gt;\n    rename(\n        `Consistent with Historical` = n,\n        `Different from Historical` = `Different (%)`\n    )\n\ngt(consistency_summary_proportional) |&gt;\n    tab_header(\n        title = \"State-Wide Proportional Scheme Consistency Summary\"\n    )\n\n\n\n\n\n\n\n\nState-Wide Proportional Scheme Consistency Summary\n\n\nConsistency\nConsistent with Historical\nTotal Elections\nConsistent (%)\nDifferent from Historical\n\n\n\n\nNo\n3\n600\n0.5\n99.5\n\n\nYes\n597\n600\n99.5\n0.5\n\n\n\n\n\n\n\nNational Proportional\n\n\nCode\nhistorical_results &lt;- presidential_votes |&gt;\n    filter(office == \"US PRESIDENT\") |&gt;\n    group_by(year, state_po) |&gt;\n    summarize(\n        HistoricalWinner = candidate[which.max(candidatevotes)],\n        HistoricalParty = party_simplified[which.max(candidatevotes)]\n    ) |&gt;\n    ungroup()\n\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\nCode\ntotal_electoral_votes &lt;- 538  # Total number of ECVs in the US\n\nnational_proportional &lt;- presidential_votes |&gt;\n    filter(office == \"US PRESIDENT\") |&gt;\n    group_by(year, candidate, party_simplified) |&gt;\n    summarize(\n        national_votes = sum(candidatevotes),\n        total_votes = sum(totalvotes),\n        .groups = \"drop\"\n    ) |&gt;\n    mutate(\n        vote_share = national_votes / total_votes,\n        national_ecv = round(vote_share * total_electoral_votes)\n    ) |&gt;\n    filter(national_ecv &gt; 0) |&gt;\n    select(year, Winner = candidate, Party = party_simplified, ElectoralVotes = national_ecv)\n\nconsistency_national &lt;- national_proportional |&gt;\n    group_by(year) |&gt;\n    slice_max(order_by = ElectoralVotes, n = 1, with_ties = FALSE) |&gt;\n    ungroup() |&gt;\n    left_join(\n        historical_results |&gt;\n            group_by(year) |&gt;\n            summarize(HistoricalWinner = first(HistoricalWinner)),\n        by = \"year\"\n    ) |&gt;\n    reframe(\n        year,\n        Winner,\n        HistoricalWinner,\n        Consistency = if_else(Winner == HistoricalWinner, \"Yes\", \"No\")\n    )\n\nconsistency_summary_national &lt;- consistency_national |&gt;\n    count(Consistency) |&gt;\n    mutate(\n        `Total Elections` = sum(n),\n        `Consistent (%)` = round((n / `Total Elections`) * 100, 2),\n        `Different (%)` = 100 - `Consistent (%)`\n    ) |&gt;\n    rename(\n        `Consistent with Historical` = n,\n        `Different from Historical` = `Different (%)`\n    )\n\ngt(consistency_summary_national) |&gt;\n    tab_header(\n        title = \"National Proportional Scheme Consistency Summary\"\n    )\n\n\n\n\n\n\n\n\nNational Proportional Scheme Consistency Summary\n\n\nConsistency\nConsistent with Historical\nTotal Elections\nConsistent (%)\nDifferent from Historical\n\n\n\n\nNo\n8\n12\n66.67\n33.33\n\n\nYes\n4\n12\n33.33\n66.67\n\n\n\n\n\n\n\n\nTask 7: Evaluating Fairness of ECV Allocation Schemes\nBased on the results from the consistency checks for the four electoral allocation schemes—State-Wide Winner-Take-All, District-Wide Winner-Take-All + State-Wide At Large Votes, State-Wide Proportional, and National Proportional—each approach impacts election outcomes differently in terms of fairness and consistency with historical results.\nI would argue the District-Wide Winner-Take All + State-Wide At Large Votes is the fairest ECV allocation method. It is a blend of proportional and majority-rule elements, allowing individual congressional districts to allocate ECVs to different candidates. I believe this offers a more balanced reflection of voter performance within states. From the results above, one could see the most significant variation compared to historical results in this method compared to the other three. To me, knowing the current electoral college system is regularly challenged, I take this difference as a sign of a good allocation method."
  },
  {
    "objectID": "mp03.html#footnotes",
    "href": "mp03.html#footnotes",
    "title": "mp03",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhere party = NA (no party listed), these values are excluded and considered minor party votes.↩︎\nWe assume a candidate wins if they have the most votes in their district.↩︎"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "mp04",
    "section": "",
    "text": "Abstract\nThis project evaluates two retirement plans offered to CUNY faculty: the Teachers Retirement System (TRS) and the Optional Retirement Plan (ORP). Through the use of historical financial data, bootstrap inference techniques, and actuarial assumptions, the analysis estimates the probability that one plan outperforms the other, under various scenarios. Key considerations include salary growth, market performance, inflation, and individual risk tolerance. The findings aim to support faculty in making an informed and data-driven decision when choosing between the two retirement plans.\nBackground\nCUNY offers tow distinct retirement plans for its employees. The first is the Teachers Retirement System (TRS). This plan is a “defined-benefit” plan pension plan, due to the fixed retirement payments guaranteed by the employer, based on the employee’s final average salary and years of service. Once retired, the retiree receives annual cost of living adjustments, based on inflation, capped at 3%. The contribution rates for TRS vary by salary tier.\nThe second retirement plan option is called the Optional Retirement Plan (ORP). This plan most closely resembles a 401(k) retirement plan, in that is a defined-contribution plan. Employee and employer contributions are invested in mutual funds with age-based asset allocations. Retirement income depends on both market performance and withdrawal strategies. This plan offers the potential for higher returns through compounded growth, but market volatility becomes a risk.\nThe analysis below evaluates these plans in detail, incorporating data from AlphaVantage and FRED to simulate market returns and inflation trends, and uses computational models to project outcomes over an employee’s career.\n\n\nCode\nlibrary(dplyr)\nlibrary(gt)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(reshape2)\n\n\n\n\nFirst, we access AlphaVantage, a commercial stock market data provider. This step involves creating an AlphaVantage free API key. Ir is important to keep this key secure, so the key itself was saved separately in alphavantage_key.txt.\n\n\nCode\n#reading API key in \napi_key_file &lt;- \"alphavantage_key.txt\"\n\nalpha_api_key &lt;- readLines(api_key_file)\n\n\nNext, we downloaded the package and loaded library httr2, which is a tool for making HTTP requests in R, which was designed to simplify interactions with APIs.\n\n\nCode\nlibrary(httr2)\n\n\n\n\n\nWe then access our next data source, FRED: the Federal Reserve Economic Data repository. The FRED api key is accessed in the same fashion as AlphaVantage, the key itself was saved separately in fred_key.txt.\n\n\nCode\n#reading FRED key in \nfred_key_file &lt;- \"fred_key.txt\"\n\nfred_api_key &lt;- readLines(fred_key_file)\n\n\n\n\n\nBefore beginning the Monte Carlo analysis, we identify and download historical data series for each of the following:\n\nWage growth - this metric is necessary as wage growth determines how an individual’s income evolves over time. We first attempted to access the Average Hourly Earnings of State and Local Government Employees (series ID: CES9091000003), given CUNY employees are New York State employees, but it was no longer available in the FRED repository.\nInstead, we accessed the Average Hourly Earnings of Production and Nonsupervisory Employees: Total Private (series ID: CES050000003). This gives us an alternative to understand wage growth patterns over time, within the private sector, given we were unable to find a puclic sector match. The httr2 packages allows us to send the request to the the API and parse the JSON response directly into R. We use a 20 year time period, starting 1/1/2003 and ending 12/31/2003.\n\n\nCode\n#FRED API base URL\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n#FRED API parameters for wage growth data\nfred_params &lt;- list(\n  series_id = \"CES0500000003\",  #average Hourly Earnings (Private Sector)\n  api_key = fred_api_key,       \n  file_type = \"json\",           # Response format\n  observation_start = \"2003-01-01\",  # Start date\n  observation_end = \"2023-12-31\"     # End date\n)\n\n#fetch data\nresp &lt;- request(fred_base_url) |&gt;\n  req_url_query(!!!fred_params) |&gt;\n  req_perform()\n\n#parse the JSON response into a data frame\nwage_growth_data &lt;- resp_body_json(resp)\nwage_growth_df &lt;- wage_growth_data$observations |&gt;\n  as.data.frame() |&gt;\n  transform(date = as.Date(date), value = as.numeric(value))\n\n# Display the first few rows of the data frame\nprint(head(wage_growth_df))\n\n\nAfter completing this step and viewing the dataframe in the R environment, it appeared as a list. The below code converts the wage_growth_df to a proper dataframe.\n\n\nCode\n#extract and convert the observations into a data frame\nwage_growth_df &lt;- do.call(rbind, lapply(wage_growth_data$observations, as.data.frame)) |&gt;\n  as.data.frame() |&gt;\n  transform(\n    date = as.Date(date),            # Convert 'date' column to Date format\n    value = as.numeric(value)        # Convert 'value' column to numeric\n  )\n\n#drop unnecessary columns\nwage_growth_df &lt;- wage_growth_df[, c(\"date\", \"value\")]\n\n#view the structure of the resulting data frame\nstr(wage_growth_df)\n\n\n'data.frame':   214 obs. of  2 variables:\n $ date : Date, format: \"2006-03-01\" \"2006-04-01\" ...\n $ value: num  20.1 20.1 20.1 20.2 20.3 ...\n\n\nCode\n#display the first few rows\nhead(wage_growth_df)\n\n\n        date value\n1 2006-03-01 20.05\n2 2006-04-01 20.15\n3 2006-05-01 20.13\n4 2006-06-01 20.23\n5 2006-07-01 20.29\n6 2006-08-01 20.32\n\n\nInflation - we access the CPI for All Urban Consumers: All Items (series ID: CPIAUCSL) from FRED, which measures the monthly changes in the price of goods and services purchased by urban customers. The resulting dataframe includes monthly observations of the CPI, formatted into a clean data frame for later analysis.\n\n\nCode\n#FRED API base URL\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n#FRED API parameters for CPI data\nfred_params &lt;- list(\n  series_id = \"CPIAUCSL\",            # CPI for All Urban Consumers: All Items\n  api_key = fred_api_key,            # Your API key\n  file_type = \"json\",                # Response format\n  observation_start = \"2003-01-01\",  # Start date\n  observation_end = \"2023-12-31\"     # End date\n)\n\n#fetch CPI data\nresp &lt;- request(fred_base_url) |&gt;\n  req_url_query(!!!fred_params) |&gt;\n  req_perform()\n\n#parse the JSON response into a data frame\ncpi_data &lt;- resp_body_json(resp)\ncpi_df &lt;- do.call(rbind, lapply(cpi_data$observations, as.data.frame)) |&gt;\n  as.data.frame() |&gt;\n  transform(\n    date = as.Date(date),            # Convert 'date' column to Date format\n    value = as.numeric(value)        # Convert 'value' column to numeric\n  )\n\n#drop unnecessary columns \ncpi_df &lt;- cpi_df[, c(\"date\", \"value\")]\n\n#view the structure of the resulting data frame\nstr(cpi_df)\n\n\n'data.frame':   252 obs. of  2 variables:\n $ date : Date, format: \"2003-01-01\" \"2003-02-01\" ...\n $ value: num  183 184 184 183 183 ...\n\n\nCode\n#display the first few rows of the data frame\nhead(cpi_df)\n\n\n        date value\n1 2003-01-01 182.6\n2 2003-02-01 183.6\n3 2003-03-01 183.9\n4 2003-04-01 183.2\n5 2003-05-01 182.9\n6 2003-06-01 183.1\n\n\n\nUS Equity Market total returns - this metric reflects the overall performance of the US stock market, which includes:\n\nCapital Gains: Price appreciation of stocks or an index\nDividends: Payments made to shareholders, reinvested for total returns\n\nWe access AlphaVantage’s TIME_SERIES_DAILY_ADJUSTED function which gives us access to the daily stock prices (open, close, high, low), adjusted close prices (adjusted for dividends and stock splits, representing total returns) , and volume.\nWe use the following code to access S&P 500 Total Returns (SPY) -\n\n\n\nCode\n#base URL for AlphaVantage API\nalpha_base_url &lt;- \"https://www.alphavantage.co/query\"\n\n#API parameters for SPY\ntest_params &lt;- list(\n  `function` = \"TIME_SERIES_DAILY_ADJUSTED\",  \n  symbol = \"SPY\",                          \n  apikey = alpha_api_key                 \n)\n\n#test request\nresp &lt;- request(alpha_base_url) |&gt;\n  req_url_query(!!!test_params) |&gt;\n  req_perform()\n\nprint(resp_body_string(resp))\n\n\n[1] \"{\\n    \\\"Information\\\": \\\"Thank you for using Alpha Vantage! This is a premium endpoint. You may subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly unlock all premium endpoints\\\"\\n}\"\n\n\nGiven the output above, we ran into a premium endpoint, meaning the TIME_SERIES_DAILY_ADJUSTED function is not available using the free version of AlphaVantage. It is important to note this step involved a bit of trial and error.\nWe instead access the TIME_SERIES_DAILY data, which gives the unadjusted daily prices. This is not ideal, but will suffice for approximate analysis.\n\n\nCode\n#define the base URL for AlphaVantage API\nalpha_base_url &lt;- \"https://www.alphavantage.co/query\"\n\n#minimal API parameters for SPY\ntest_params &lt;- list(\n  `function` = \"TIME_SERIES_DAILY\",  # Function to fetch daily prices\n  symbol = \"SPY\",                          # S&P 500 ETF\n  apikey = alpha_api_key                    # Your AlphaVantage API key\n)\n\n#send the test request\nresp &lt;- request(alpha_base_url) |&gt;\n  req_url_query(!!!test_params) |&gt;\n  req_perform()\n\n#parse the JSON response\nspy_data &lt;- resp_body_json(resp)\n\n#extract the daily time series data\nspy_daily &lt;- spy_data[[\"Time Series (Daily)\"]]\n\n#convert to a data frame\nspy_df &lt;- do.call(rbind, lapply(names(spy_daily), function(date) {\n  data.frame(\n    date = as.Date(date),\n    close = as.numeric(spy_daily[[date]][[\"4. close\"]])\n  )\n}))\n\n#sort by date\nspy_df &lt;- spy_df[order(spy_df$date), ]\n\n#view the resulting data frame\nstr(spy_df)\n\n\n'data.frame':   100 obs. of  2 variables:\n $ date : Date, format: \"2024-07-16\" \"2024-07-17\" ...\n $ close: num  565 557 553 549 555 ...\n\n\nCode\nhead(spy_df)\n\n\n          date  close\n100 2024-07-16 564.86\n99  2024-07-17 556.94\n98  2024-07-18 552.66\n97  2024-07-19 548.99\n96  2024-07-22 554.65\n95  2024-07-23 553.78\n\n\nWe also access the S&P 500 Index time seriesfrom FRED, which provides daily index values from 2003 to 2023.\n\n\nCode\n# Define the base URL for the FRED API\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# Define API parameters for US equity market (S&P 500 Index as an alternative to Wilshire 5000)\nfred_params_us &lt;- list(\n  series_id = \"SP500\",              # S&P 500 Index series ID\n  api_key = fred_api_key,           # Your FRED API key\n  file_type = \"json\",               # Response format\n  observation_start = \"2003-01-01\", # Start date\n  observation_end = \"2023-12-31\"    # End date\n)\n\n# Fetch the data from FRED API\nresp &lt;- request(fred_base_url) |&gt;\n  req_url_query(!!!fred_params_us) |&gt;\n  req_perform()\n\n# Parse the JSON response\nus_equity_data &lt;- resp_body_json(resp)\n\n# Convert the observations list into a clean dataframe\nus_equity_df &lt;- do.call(rbind, lapply(us_equity_data$observations, function(x) {\n  data.frame(\n    date = as.Date(x$date),                  # Convert 'date' to Date format\n    us_equity = as.numeric(ifelse(x$value == \".\", NA, x$value)), # Handle \".\" as NA\n    stringsAsFactors = FALSE                # Avoid unnecessary factor conversion\n  )\n}))\n\n# Drop rows with NA values if necessary\nus_equity_df &lt;- us_equity_df[!is.na(us_equity_df$us_equity), ]\n\n# Verify the structure and content of the dataframe\nstr(us_equity_df)\n\n\n'data.frame':   2282 obs. of  2 variables:\n $ date     : Date, format: \"2014-12-05\" \"2014-12-08\" ...\n $ us_equity: num  2075 2060 2060 2026 2035 ...\n\n\nCode\nhead(us_equity_df)\n\n\n        date us_equity\n1 2014-12-05   2075.37\n2 2014-12-08   2060.31\n3 2014-12-09   2059.82\n4 2014-12-10   2026.14\n5 2014-12-11   2035.33\n6 2014-12-12   2002.33\n\n\nInternational Equity Market total returns - this metric provides insight into the benefits of global diversification. Investing outside of the US allows for exposure into different economies and sectors, which reduces dependency on the domestic market. One popular index for markets outside of North America is the MSCI EAFE, which is tracked by the EFA (MSCI EAFE) ETF.\nBy analyzing international equity market returns, we can model how investments in global stocks could impact long-term retirement outcomes, especially when paired with US data.\nWe run the following code to access International Equity Data using AlphaVantage.\n\n\nCode\n# Define the base URL for AlphaVantage API\nalpha_base_url &lt;- \"https://www.alphavantage.co/query\"\n\n# Minimal API parameters for EFA (MSCI EAFE ETF)\nefa_params &lt;- list(\n  `function` = \"TIME_SERIES_DAILY\",  # Function to fetch daily prices\n  symbol = \"EFA\",                   # MSCI EAFE ETF\n  apikey = alpha_api_key             # Your AlphaVantage API key\n)\n\n# Send the request\nresp &lt;- request(alpha_base_url) |&gt;\n  req_url_query(!!!efa_params) |&gt;\n  req_perform()\n\n# Parse the JSON response\nefa_data &lt;- resp_body_json(resp)\n\n# Extract the daily time series data\nefa_daily &lt;- efa_data[[\"Time Series (Daily)\"]]\n\n# Convert to a data frame\nefa_df &lt;- do.call(rbind, lapply(names(efa_daily), function(date) {\n  data.frame(\n    date = as.Date(date),\n    close = as.numeric(efa_daily[[date]][[\"4. close\"]])\n  )\n}))\n\n# Sort by date\nefa_df &lt;- efa_df[order(efa_df$date), ]\n\n# View the resulting data frame\nstr(efa_df)\n\n\n'data.frame':   100 obs. of  2 variables:\n $ date : Date, format: \"2024-07-16\" \"2024-07-17\" ...\n $ close: num  81.2 80.8 80 79.5 80.4 ...\n\n\nCode\nhead(efa_df)\n\n\n          date close\n100 2024-07-16 81.25\n99  2024-07-17 80.82\n98  2024-07-18 80.02\n97  2024-07-19 79.53\n96  2024-07-22 80.37\n95  2024-07-23 79.92\n\n\nThe code above proceeds to use the TIME_SERIES_DAILY endpoint to grab daily prices for the EFA ETF, parameters are specified and the request is sent to the AlphaVantage API, the Time Series (Daily) section of the JSON response is extracted, a data frame is created with the date and closing price columns, sorted by date.\n\nNote: One limitation of using AlphaVantage’s free content is we cannot go back 15-20 years in every sector, as we might hope, as find easier when using FRED.\n\nUS equity data typically calculates logarithmic returns. To transform this data so it may be compared against US equity returns, or incorporated into Monte Carlo simulations, we calculate the logarithmic returns as follows:\n\n\nCode\nefa_df &lt;- efa_df |&gt;\n  transform(return = c(NA, diff(log(close))))\n\nhead(efa_df)\n\n\n          date close       return\n100 2024-07-16 81.25           NA\n99  2024-07-17 80.82 -0.005306362\n98  2024-07-18 80.02 -0.009947856\n97  2024-07-19 79.53 -0.006142294\n96  2024-07-22 80.37  0.010506663\n95  2024-07-23 79.92 -0.005614838\n\n\nWe can also use FRED to access the ICE BofA Emerging Markets Corporate Plus Index Total Return Index Value. This index measures the total returns of emerging market corporate bonds. We use this dataset as a proxy for international equity market performance because it captures returns in emerging markets, which, similar to equity investments, have hgiher risk and growth potential.\n\n\nCode\n# Define FRED API base URL\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# FRED API parameters for the selected international equity index\nfred_params_int &lt;- list(\n  series_id = \"BAMLEMCBPITRIV\",         # International Equity Market Index\n  api_key = fred_api_key,               # Your FRED API key\n  file_type = \"json\",                   # Response format\n  observation_start = \"2003-01-01\",     # Start date\n  observation_end = \"2023-12-31\"        # End date\n)\n\n# Fetch international equity market data\nresp &lt;- request(fred_base_url) |&gt;\n  req_url_query(!!!fred_params_int) |&gt;\n  req_perform()\n\n# Parse the JSON response into a data frame\nint_equity_data &lt;- resp_body_json(resp)\nint_equity_df &lt;- do.call(rbind, lapply(int_equity_data$observations, as.data.frame)) |&gt;\n  as.data.frame() |&gt;\n  transform(\n    date = as.Date(date),             # Convert 'date' column to Date format\n    value = as.numeric(value)        # Convert 'value' column to numeric\n  )\n\n# Drop unnecessary columns and rename\nint_equity_df &lt;- int_equity_df[, c(\"date\", \"value\")] |&gt;\n  rename(int_equity = value)          # Rename 'value' column to 'int_equity'\n\n# View the structure of the resulting data frame\nstr(int_equity_df)\n\n\n'data.frame':   5548 obs. of  2 variables:\n $ date      : Date, format: \"2003-01-01\" \"2003-01-02\" ...\n $ int_equity: num  NA 144 144 145 145 ...\n\n\nCode\n# Display the first few rows of the data frame\nhead(int_equity_df)\n\n\n        date int_equity\n1 2003-01-01         NA\n2 2003-01-02     144.36\n3 2003-01-03     144.43\n4 2003-01-06     144.53\n5 2003-01-07     145.01\n6 2003-01-08     145.50\n\n\nThe code below replaces missing values with NA\n\n\nCode\nint_equity_df$int_equity &lt;- as.numeric(ifelse(int_equity_df$int_equity == \".\", NA, int_equity_df$int_equity))\n\n\nThis code replaces NA with the last valid observation.\n\n\nCode\nlibrary(zoo)\nint_equity_df$int_equity &lt;- na.locf(int_equity_df$int_equity, na.rm = FALSE)\n\n\nAggregating to monthly data -\n\n\nCode\nint_equity_monthly &lt;- int_equity_df |&gt;\n  mutate(month = format(date, \"%Y-%m\")) |&gt;\n  group_by(month) |&gt;\n  summarize(int_equity = mean(int_equity, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  mutate(date = as.Date(paste0(month, \"-01\"), format = \"%Y-%m-%d\")) |&gt;\n  select(-month)  #dropping month - realized later on we did not need it so modifying this step\n\n#checking the structure of the modified dataset\nstr(int_equity_monthly)\n\n\ntibble [252 × 2] (S3: tbl_df/tbl/data.frame)\n $ int_equity: num [1:252] 146 148 149 151 157 ...\n $ date      : Date[1:252], format: \"2003-01-01\" \"2003-02-01\" ...\n\n\nCode\n#previewing the first few rows\nhead(int_equity_monthly)\n\n\n# A tibble: 6 × 2\n  int_equity date      \n       &lt;dbl&gt; &lt;date&gt;    \n1       146. 2003-01-01\n2       148. 2003-02-01\n3       149. 2003-03-01\n4       151. 2003-04-01\n5       157. 2003-05-01\n6       161. 2003-06-01\n\n\nBond market total returns - this financial metric provides insight into fixed-income investments. These are typically less volatile compared to equities, and serve as a method to stabilize a portfolio. Bonds offer a predictable income stream and can hedge against stock market downturns. Bonds serve as a safe and stable portion of a portfolio as one plans for retirement.\nWe access FRED’s 10-Year Treasury Constant Maturity Rate, for 2003-2023.\n\n\nCode\n# Define the base URL for FRED API\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# FRED API parameters for 10-Year Treasury Rate\nfred_params &lt;- list(\n  series_id = \"GS10\",               # 10-Year Treasury Constant Maturity Rate\n  api_key = fred_api_key,           # Your FRED API key\n  file_type = \"json\",               # Response format\n  observation_start = \"2003-01-01\", # Start date\n  observation_end = \"2023-12-31\"    # End date\n)\n\n# Send the request\nresp &lt;- request(fred_base_url) |&gt;\n  req_url_query(!!!fred_params) |&gt;\n  req_perform()\n\n# Parse the JSON response\nbond_data &lt;- resp_body_json(resp)\n\n# Extract and convert the observations into a data frame\nbond_df &lt;- do.call(rbind, lapply(bond_data$observations, as.data.frame)) |&gt;\n  as.data.frame() |&gt;\n  transform(\n    date = as.Date(date),            # Convert 'date' column to Date format\n    value = as.numeric(value)        # Convert 'value' column to numeric\n  )\n\n# Drop unnecessary columns\nbond_df &lt;- bond_df[, c(\"date\", \"value\")]\n\n# View the structure of the resulting data frame\nstr(bond_df)\n\n\n'data.frame':   252 obs. of  2 variables:\n $ date : Date, format: \"2003-01-01\" \"2003-02-01\" ...\n $ value: num  4.05 3.9 3.81 3.96 3.57 3.33 3.98 4.45 4.27 4.29 ...\n\n\nCode\n# Display the first few rows\nhead(bond_df)\n\n\n        date value\n1 2003-01-01  4.05\n2 2003-02-01  3.90\n3 2003-03-01  3.81\n4 2003-04-01  3.96\n5 2003-05-01  3.57\n6 2003-06-01  3.33\n\n\nWith the hope to later analyze returns, we can calculate monthly percentage changes.\n\n\nCode\nbond_df &lt;- bond_df |&gt; \n  transform(return = c(NA, diff(log(value))))\n\n# View the first few rows with returns\nhead(bond_df)\n\n\n        date value      return\n1 2003-01-01  4.05          NA\n2 2003-02-01  3.90 -0.03774033\n3 2003-03-01  3.81 -0.02334736\n4 2003-04-01  3.96  0.03861484\n5 2003-05-01  3.57 -0.10367843\n6 2003-06-01  3.33 -0.06959329\n\n\nShort-term debt returns - reflect the interest earned on safe, short-term fixed-income securities. One commonly used financial instrument here is the Treasury Bill. When planning for retirement, short-term debt can serve as a buffer for market volatility.\nBelow we access FRED’s 3-Month Treasury Bill Rate, with a resulting data frame, t_bill_df, which contains short-term debt rate data from 2003-2023.\n\n\nCode\n# Define the base URL for FRED API\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# FRED API parameters for 3-Month Treasury Bill Rate\nfred_params &lt;- list(\n  series_id = \"TB3MS\",               # 3-Month Treasury Bill Rate\n  api_key = fred_api_key,            # Your FRED API key\n  file_type = \"json\",                # Response format\n  observation_start = \"2003-01-01\",  # Start date\n  observation_end = \"2023-12-31\"     # End date\n)\n\n# Send the request\nresp &lt;- request(fred_base_url) |&gt;\n  req_url_query(!!!fred_params) |&gt;\n  req_perform()\n\n# Parse the JSON response\nt_bill_data &lt;- resp_body_json(resp)\n\n# Extract and convert the observations into a data frame\nt_bill_df &lt;- do.call(rbind, lapply(t_bill_data$observations, as.data.frame)) |&gt;\n  as.data.frame() |&gt;\n  transform(\n    date = as.Date(date),             # Convert 'date' column to Date format\n    value = as.numeric(value)         # Convert 'value' column to numeric\n  )\n\n# Drop unnecessary columns\nt_bill_df &lt;- t_bill_df[, c(\"date\", \"value\")]\n\n# View the structure of the resulting data frame\nstr(t_bill_df)\n\n\n'data.frame':   252 obs. of  2 variables:\n $ date : Date, format: \"2003-01-01\" \"2003-02-01\" ...\n $ value: num  1.17 1.17 1.13 1.13 1.07 0.92 0.9 0.95 0.94 0.92 ...\n\n\nCode\n# Display the first few rows\nhead(t_bill_df)\n\n\n        date value\n1 2003-01-01  1.17\n2 2003-02-01  1.17\n3 2003-03-01  1.13\n4 2003-04-01  1.13\n5 2003-05-01  1.07\n6 2003-06-01  0.92\n\n\n\nWe once again include the monthly logarithmic return rates to our data frame:\n\n\nCode\nt_bill_df &lt;- t_bill_df |&gt; \n  transform(return = c(NA, diff(log(value))))\n\n#view the first few rows with returns\nhead(t_bill_df)\n\n\n        date value      return\n1 2003-01-01  1.17          NA\n2 2003-02-01  1.17  0.00000000\n3 2003-03-01  1.13 -0.03478612\n4 2003-04-01  1.13  0.00000000\n5 2003-05-01  1.07 -0.05455898\n6 2003-06-01  0.92 -0.15104026\n\n\nBefore moving on to the next task, we filter all (applicable) data sets to reflect the time frame of 2007-2023, keeping in mind our wage growth data was only able to go back to mid-2006. This adjustment aligns our data for further analysis.\n\n\nCode\n#defining the date range\nstart_date &lt;- as.Date(\"2007-01-01\")\nend_date &lt;- as.Date(\"2023-12-31\")\n\n#adjusting wage growth dataframe\nwage_growth_df &lt;- subset(wage_growth_df, date &gt;= start_date & date &lt;= end_date)\n\n#adjusting CPI dataframe\ncpi_df &lt;- subset(cpi_df, date &gt;= start_date & date &lt;= end_date)\n\n#adjusting bond dataframe\nbond_df &lt;- subset(bond_df, date &gt;= start_date & date &lt;= end_date)\n\n#adjusting T-bill dataframe\nt_bill_df &lt;- subset(t_bill_df, date &gt;= start_date & date &lt;= end_date)\n\n#adjusting international equity dataframe\nint_equity_monthly &lt;- subset(\n    int_equity_monthly,\n    date &gt;= start_date & date &lt;= end_date\n)\n\n#verify each dataframe\nlist(\n  wage_growth = range(wage_growth_df$date),\n  cpi = range(cpi_df$date),\n  bond = range(bond_df$date),\n  t_bill = range(t_bill_df$date),\n  int_equity = range(int_equity_monthly$date)\n)\n\n\n$wage_growth\n[1] \"2007-01-01\" \"2023-12-01\"\n\n$cpi\n[1] \"2007-01-01\" \"2023-12-01\"\n\n$bond\n[1] \"2007-01-01\" \"2023-12-01\"\n\n$t_bill\n[1] \"2007-01-01\" \"2023-12-01\"\n\n$int_equity\n[1] \"2007-01-01\" \"2023-12-01\"\n\n\n\n\n\nThis step involves conducting exploratory data analysis to identify key properties of the data historical data acquired.\nThe code below calculates the long-run monthly averages and variances for each data series.\nThe table above summarizes the key statistical properties, the long-run monthly averages, and the variability of each series over the 2007-2023 time period.\nThe wage growth shows a mean value of 25.89 and variance of 13.74. This indicates a relatively stable upward trend over time.\nThe CPI exhibits the highest variance at 697.58, indicates there were significant fluctuations in inflation from 2007 to 2023. This impacts our retirement planning, emphasizing the importance of considering inflation-indexed returns (like in our two retirement plan options).\nNext, we generate the correlation matrix table through the code below. We first combine our data into a single data frame, aligned_data.\n\n\nCode\n # Renaming and preparing datasets for alignment\nwage_growth_prepared &lt;- wage_growth_df |&gt; rename(wage_growth = value)\ncpi_prepared &lt;- cpi_df |&gt; rename(cpi = value)\nbond_prepared &lt;- bond_df |&gt; rename(bond_return = return) |&gt; select(date, bond_return)\nt_bill_prepared &lt;- t_bill_df |&gt; rename(t_bill_return = return) |&gt; select(date, t_bill_return)\n\n# Ensure int_equity_monthly has the correct structure\nint_equity_prepared &lt;- int_equity_monthly \n\nprint(head(int_equity_monthly))\n\n\n# A tibble: 6 × 2\n  int_equity date      \n       &lt;dbl&gt; &lt;date&gt;    \n1       204. 2007-01-01\n2       206. 2007-02-01\n3       207. 2007-03-01\n4       208. 2007-04-01\n5       209. 2007-05-01\n6       207. 2007-06-01\n\n\nCode\n# Combine all datasets by date using a full join\naligned_data &lt;- list(\n  wage_growth_prepared,\n  cpi_prepared,\n  bond_prepared,\n  t_bill_prepared,\n  int_equity_prepared\n) |&gt; \n  reduce(full_join, by = \"date\")\n\n# Check structure and preview\nstr(aligned_data)\n\n\n'data.frame':   204 obs. of  6 variables:\n $ date         : Date, format: \"2007-01-01\" \"2007-02-01\" ...\n $ wage_growth  : num  20.6 20.7 20.7 20.8 20.8 ...\n $ cpi          : num  203 204 205 206 207 ...\n $ bond_return  : num  0.04293 -0.00844 -0.03449 0.02811 0.01271 ...\n $ t_bill_return: num  0.02645 0.00999 -0.01805 -0.01427 -0.02917 ...\n $ int_equity   : num  204 206 207 208 209 ...\n\n\nCode\nhead(aligned_data)\n\n\n        date wage_growth     cpi  bond_return t_bill_return int_equity\n1 2007-01-01       20.59 203.437  0.042925045   0.026451186   204.1070\n2 2007-02-01       20.68 204.226 -0.008438869   0.009990093   205.5720\n3 2007-03-01       20.73 205.288 -0.034486176  -0.018054653   207.4839\n4 2007-04-01       20.78 205.904  0.028109959  -0.014271394   208.4286\n5 2007-05-01       20.84 206.755  0.012712036  -0.029168735   209.3670\n6 2007-06-01       20.95 207.234  0.071095922  -0.025697345   207.2132\n\n\n\n\nCode\n# Recompute summary statistics for aligned data\nmean_stats &lt;- aligned_data |&gt;\n  summarize(\n    `Wage Growth` = mean(wage_growth, na.rm = TRUE),\n    `CPI` = mean(cpi, na.rm = TRUE),\n    `Bond Returns` = mean(bond_return, na.rm = TRUE),\n    `T-Bill Returns` = mean(t_bill_return, na.rm = TRUE),\n    `International Equity` = mean(int_equity, na.rm = TRUE)\n  ) |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"Data Metric\",\n    values_to = \"Mean Value\"\n  )\n\nvariance_stats &lt;- aligned_data |&gt;\n  summarize(\n    `Wage Growth` = var(wage_growth, na.rm = TRUE),\n    `CPI` = var(cpi, na.rm = TRUE),\n    `Bond Returns` = var(bond_return, na.rm = TRUE),\n    `T-Bill Returns` = var(t_bill_return, na.rm = TRUE),\n    `International Equity` = var(int_equity, na.rm = TRUE)\n  ) |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"Data Metric\",\n    values_to = \"Variance\"\n  )\n\n# Combine means and variances\nsummary_stats_aligned &lt;- mean_stats |&gt;\n  left_join(variance_stats, by = \"Data Metric\")\n\n# Display the summary statistics using the gt package\nlibrary(gt)\nsummary_stats_aligned |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Updated Summary Statistics for Aligned Data\",\n    subtitle = \"Averages and Variances from 2007 to 2023\"\n  ) |&gt;\n  fmt_number(\n    columns = vars(`Mean Value`, Variance),\n    decimals = 2\n  )\n\n\n\n\n\n\n\n\nUpdated Summary Statistics for Aligned Data\n\n\nAverages and Variances from 2007 to 2023\n\n\nData Metric\nMean Value\nVariance\n\n\n\n\nWage Growth\n25.89\n13.74\n\n\nCPI\n243.26\n697.58\n\n\nBond Returns\n0.00\n0.01\n\n\nT-Bill Returns\n0.00\n0.16\n\n\nInternational Equity\n337.20\n6,754.06\n\n\n\n\n\n\n\nNext, to analyze the relationships between the various economic indicators in our dataset, we compute a correlation matrix and visualize it using a heatmap. The correlation matrix tells us the strength and direction of linear relationship between each variable pair.\n\n\nCode\n#compute correlation matrix\ncor_matrix &lt;- cor(aligned_data[, -1], use = \"complete.obs\")\n\n#reshape correlation matrix into long format for ggplot\ncor_long &lt;- melt(cor_matrix)\n\n# creating heatmap \nggplot(cor_long, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile(color = \"white\") + # Add gridlines for better separation\n  scale_fill_gradient2(\n    low = \"blue\", high = \"red\", mid = \"white\",\n    midpoint = 0, limit = c(-1, 1), space = \"Lab\",\n    name = \"Correlation\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank()\n  ) +\n  ggtitle(\n    \"Correlation Matrix Heatmap\",\n    subtitle = \"Relationships Between Economic Indicators\"\n  )\n\n\n\n\n\n\n\n\n\nThe heatmap visualizes the correlations among the economic indicators. We see a strong positive correlation between wage growth and CIP, indicating that as wages increase, consumer prices tend to rise, which aligns with inflationary trends. Differently, we find a weak positive correlation between T-bill returns and other indicators like bond returns and international equities. This highlights the more stable and less market-sensitive nature of these economic indicators."
  },
  {
    "objectID": "mp04.html#task-1-register-for-alphavantage-api-key",
    "href": "mp04.html#task-1-register-for-alphavantage-api-key",
    "title": "mp04",
    "section": "",
    "text": "First, we access AlphaVantage, a commercial stock market data provider. This step involves creating an AlphaVantage free API key. Ir is important to keep this key secure, so the key itself was saved separately in alphavantage_key.txt.\n\n\nCode\n#reading API key in \napi_key_file &lt;- \"alphavantage_key.txt\"\n\nalpha_api_key &lt;- readLines(api_key_file)\n\n\nNext, we downloaded the package and loaded library httr2, which is a tool for making HTTP requests in R, which was designed to simplify interactions with APIs.\n\n\nCode\nlibrary(httr2)"
  },
  {
    "objectID": "mp04.html#task-2-register-for-fred-api-key",
    "href": "mp04.html#task-2-register-for-fred-api-key",
    "title": "mp04",
    "section": "",
    "text": "We then access our next data source, FRED: the Federal Reserve Economic Data repository. The FRED api key is accessed in the same fashion as AlphaVantage, the key itself was saved separately in fred_key.txt.\n\n\nCode\n#reading FRED key in \nfred_key_file &lt;- \"fred_key.txt\"\n\nfred_api_key &lt;- readLines(fred_key_file)"
  },
  {
    "objectID": "mp04.html#task-3-data-acquisition",
    "href": "mp04.html#task-3-data-acquisition",
    "title": "mp04",
    "section": "",
    "text": "Before beginning the Monte Carlo analysis, we identify and download historical data series for each of the following:\n\nWage growth - this metric is necessary as wage growth determines how an individual’s income evolves over time. We first attempted to access the Average Hourly Earnings of State and Local Government Employees (series ID: CES9091000003), given CUNY employees are New York State employees, but it was no longer available in the FRED repository.\nInstead, we accessed the Average Hourly Earnings of Production and Nonsupervisory Employees: Total Private (series ID: CES050000003). This gives us an alternative to understand wage growth patterns over time, within the private sector, given we were unable to find a puclic sector match. The httr2 packages allows us to send the request to the the API and parse the JSON response directly into R. We use a 20 year time period, starting 1/1/2003 and ending 12/31/2003.\n\n\nCode\n#FRED API base URL\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n#FRED API parameters for wage growth data\nfred_params &lt;- list(\n  series_id = \"CES0500000003\",  #average Hourly Earnings (Private Sector)\n  api_key = fred_api_key,       \n  file_type = \"json\",           # Response format\n  observation_start = \"2003-01-01\",  # Start date\n  observation_end = \"2023-12-31\"     # End date\n)\n\n#fetch data\nresp &lt;- request(fred_base_url) |&gt;\n  req_url_query(!!!fred_params) |&gt;\n  req_perform()\n\n#parse the JSON response into a data frame\nwage_growth_data &lt;- resp_body_json(resp)\nwage_growth_df &lt;- wage_growth_data$observations |&gt;\n  as.data.frame() |&gt;\n  transform(date = as.Date(date), value = as.numeric(value))\n\n# Display the first few rows of the data frame\nprint(head(wage_growth_df))\n\n\nAfter completing this step and viewing the dataframe in the R environment, it appeared as a list. The below code converts the wage_growth_df to a proper dataframe.\n\n\nCode\n#extract and convert the observations into a data frame\nwage_growth_df &lt;- do.call(rbind, lapply(wage_growth_data$observations, as.data.frame)) |&gt;\n  as.data.frame() |&gt;\n  transform(\n    date = as.Date(date),            # Convert 'date' column to Date format\n    value = as.numeric(value)        # Convert 'value' column to numeric\n  )\n\n#drop unnecessary columns\nwage_growth_df &lt;- wage_growth_df[, c(\"date\", \"value\")]\n\n#view the structure of the resulting data frame\nstr(wage_growth_df)\n\n\n'data.frame':   214 obs. of  2 variables:\n $ date : Date, format: \"2006-03-01\" \"2006-04-01\" ...\n $ value: num  20.1 20.1 20.1 20.2 20.3 ...\n\n\nCode\n#display the first few rows\nhead(wage_growth_df)\n\n\n        date value\n1 2006-03-01 20.05\n2 2006-04-01 20.15\n3 2006-05-01 20.13\n4 2006-06-01 20.23\n5 2006-07-01 20.29\n6 2006-08-01 20.32\n\n\nInflation - we access the CPI for All Urban Consumers: All Items (series ID: CPIAUCSL) from FRED, which measures the monthly changes in the price of goods and services purchased by urban customers. The resulting dataframe includes monthly observations of the CPI, formatted into a clean data frame for later analysis.\n\n\nCode\n#FRED API base URL\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n#FRED API parameters for CPI data\nfred_params &lt;- list(\n  series_id = \"CPIAUCSL\",            # CPI for All Urban Consumers: All Items\n  api_key = fred_api_key,            # Your API key\n  file_type = \"json\",                # Response format\n  observation_start = \"2003-01-01\",  # Start date\n  observation_end = \"2023-12-31\"     # End date\n)\n\n#fetch CPI data\nresp &lt;- request(fred_base_url) |&gt;\n  req_url_query(!!!fred_params) |&gt;\n  req_perform()\n\n#parse the JSON response into a data frame\ncpi_data &lt;- resp_body_json(resp)\ncpi_df &lt;- do.call(rbind, lapply(cpi_data$observations, as.data.frame)) |&gt;\n  as.data.frame() |&gt;\n  transform(\n    date = as.Date(date),            # Convert 'date' column to Date format\n    value = as.numeric(value)        # Convert 'value' column to numeric\n  )\n\n#drop unnecessary columns \ncpi_df &lt;- cpi_df[, c(\"date\", \"value\")]\n\n#view the structure of the resulting data frame\nstr(cpi_df)\n\n\n'data.frame':   252 obs. of  2 variables:\n $ date : Date, format: \"2003-01-01\" \"2003-02-01\" ...\n $ value: num  183 184 184 183 183 ...\n\n\nCode\n#display the first few rows of the data frame\nhead(cpi_df)\n\n\n        date value\n1 2003-01-01 182.6\n2 2003-02-01 183.6\n3 2003-03-01 183.9\n4 2003-04-01 183.2\n5 2003-05-01 182.9\n6 2003-06-01 183.1\n\n\n\nUS Equity Market total returns - this metric reflects the overall performance of the US stock market, which includes:\n\nCapital Gains: Price appreciation of stocks or an index\nDividends: Payments made to shareholders, reinvested for total returns\n\nWe access AlphaVantage’s TIME_SERIES_DAILY_ADJUSTED function which gives us access to the daily stock prices (open, close, high, low), adjusted close prices (adjusted for dividends and stock splits, representing total returns) , and volume.\nWe use the following code to access S&P 500 Total Returns (SPY) -\n\n\n\nCode\n#base URL for AlphaVantage API\nalpha_base_url &lt;- \"https://www.alphavantage.co/query\"\n\n#API parameters for SPY\ntest_params &lt;- list(\n  `function` = \"TIME_SERIES_DAILY_ADJUSTED\",  \n  symbol = \"SPY\",                          \n  apikey = alpha_api_key                 \n)\n\n#test request\nresp &lt;- request(alpha_base_url) |&gt;\n  req_url_query(!!!test_params) |&gt;\n  req_perform()\n\nprint(resp_body_string(resp))\n\n\n[1] \"{\\n    \\\"Information\\\": \\\"Thank you for using Alpha Vantage! This is a premium endpoint. You may subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly unlock all premium endpoints\\\"\\n}\"\n\n\nGiven the output above, we ran into a premium endpoint, meaning the TIME_SERIES_DAILY_ADJUSTED function is not available using the free version of AlphaVantage. It is important to note this step involved a bit of trial and error.\nWe instead access the TIME_SERIES_DAILY data, which gives the unadjusted daily prices. This is not ideal, but will suffice for approximate analysis.\n\n\nCode\n#define the base URL for AlphaVantage API\nalpha_base_url &lt;- \"https://www.alphavantage.co/query\"\n\n#minimal API parameters for SPY\ntest_params &lt;- list(\n  `function` = \"TIME_SERIES_DAILY\",  # Function to fetch daily prices\n  symbol = \"SPY\",                          # S&P 500 ETF\n  apikey = alpha_api_key                    # Your AlphaVantage API key\n)\n\n#send the test request\nresp &lt;- request(alpha_base_url) |&gt;\n  req_url_query(!!!test_params) |&gt;\n  req_perform()\n\n#parse the JSON response\nspy_data &lt;- resp_body_json(resp)\n\n#extract the daily time series data\nspy_daily &lt;- spy_data[[\"Time Series (Daily)\"]]\n\n#convert to a data frame\nspy_df &lt;- do.call(rbind, lapply(names(spy_daily), function(date) {\n  data.frame(\n    date = as.Date(date),\n    close = as.numeric(spy_daily[[date]][[\"4. close\"]])\n  )\n}))\n\n#sort by date\nspy_df &lt;- spy_df[order(spy_df$date), ]\n\n#view the resulting data frame\nstr(spy_df)\n\n\n'data.frame':   100 obs. of  2 variables:\n $ date : Date, format: \"2024-07-16\" \"2024-07-17\" ...\n $ close: num  565 557 553 549 555 ...\n\n\nCode\nhead(spy_df)\n\n\n          date  close\n100 2024-07-16 564.86\n99  2024-07-17 556.94\n98  2024-07-18 552.66\n97  2024-07-19 548.99\n96  2024-07-22 554.65\n95  2024-07-23 553.78\n\n\nWe also access the S&P 500 Index time seriesfrom FRED, which provides daily index values from 2003 to 2023.\n\n\nCode\n# Define the base URL for the FRED API\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# Define API parameters for US equity market (S&P 500 Index as an alternative to Wilshire 5000)\nfred_params_us &lt;- list(\n  series_id = \"SP500\",              # S&P 500 Index series ID\n  api_key = fred_api_key,           # Your FRED API key\n  file_type = \"json\",               # Response format\n  observation_start = \"2003-01-01\", # Start date\n  observation_end = \"2023-12-31\"    # End date\n)\n\n# Fetch the data from FRED API\nresp &lt;- request(fred_base_url) |&gt;\n  req_url_query(!!!fred_params_us) |&gt;\n  req_perform()\n\n# Parse the JSON response\nus_equity_data &lt;- resp_body_json(resp)\n\n# Convert the observations list into a clean dataframe\nus_equity_df &lt;- do.call(rbind, lapply(us_equity_data$observations, function(x) {\n  data.frame(\n    date = as.Date(x$date),                  # Convert 'date' to Date format\n    us_equity = as.numeric(ifelse(x$value == \".\", NA, x$value)), # Handle \".\" as NA\n    stringsAsFactors = FALSE                # Avoid unnecessary factor conversion\n  )\n}))\n\n# Drop rows with NA values if necessary\nus_equity_df &lt;- us_equity_df[!is.na(us_equity_df$us_equity), ]\n\n# Verify the structure and content of the dataframe\nstr(us_equity_df)\n\n\n'data.frame':   2282 obs. of  2 variables:\n $ date     : Date, format: \"2014-12-05\" \"2014-12-08\" ...\n $ us_equity: num  2075 2060 2060 2026 2035 ...\n\n\nCode\nhead(us_equity_df)\n\n\n        date us_equity\n1 2014-12-05   2075.37\n2 2014-12-08   2060.31\n3 2014-12-09   2059.82\n4 2014-12-10   2026.14\n5 2014-12-11   2035.33\n6 2014-12-12   2002.33\n\n\nInternational Equity Market total returns - this metric provides insight into the benefits of global diversification. Investing outside of the US allows for exposure into different economies and sectors, which reduces dependency on the domestic market. One popular index for markets outside of North America is the MSCI EAFE, which is tracked by the EFA (MSCI EAFE) ETF.\nBy analyzing international equity market returns, we can model how investments in global stocks could impact long-term retirement outcomes, especially when paired with US data.\nWe run the following code to access International Equity Data using AlphaVantage.\n\n\nCode\n# Define the base URL for AlphaVantage API\nalpha_base_url &lt;- \"https://www.alphavantage.co/query\"\n\n# Minimal API parameters for EFA (MSCI EAFE ETF)\nefa_params &lt;- list(\n  `function` = \"TIME_SERIES_DAILY\",  # Function to fetch daily prices\n  symbol = \"EFA\",                   # MSCI EAFE ETF\n  apikey = alpha_api_key             # Your AlphaVantage API key\n)\n\n# Send the request\nresp &lt;- request(alpha_base_url) |&gt;\n  req_url_query(!!!efa_params) |&gt;\n  req_perform()\n\n# Parse the JSON response\nefa_data &lt;- resp_body_json(resp)\n\n# Extract the daily time series data\nefa_daily &lt;- efa_data[[\"Time Series (Daily)\"]]\n\n# Convert to a data frame\nefa_df &lt;- do.call(rbind, lapply(names(efa_daily), function(date) {\n  data.frame(\n    date = as.Date(date),\n    close = as.numeric(efa_daily[[date]][[\"4. close\"]])\n  )\n}))\n\n# Sort by date\nefa_df &lt;- efa_df[order(efa_df$date), ]\n\n# View the resulting data frame\nstr(efa_df)\n\n\n'data.frame':   100 obs. of  2 variables:\n $ date : Date, format: \"2024-07-16\" \"2024-07-17\" ...\n $ close: num  81.2 80.8 80 79.5 80.4 ...\n\n\nCode\nhead(efa_df)\n\n\n          date close\n100 2024-07-16 81.25\n99  2024-07-17 80.82\n98  2024-07-18 80.02\n97  2024-07-19 79.53\n96  2024-07-22 80.37\n95  2024-07-23 79.92\n\n\nThe code above proceeds to use the TIME_SERIES_DAILY endpoint to grab daily prices for the EFA ETF, parameters are specified and the request is sent to the AlphaVantage API, the Time Series (Daily) section of the JSON response is extracted, a data frame is created with the date and closing price columns, sorted by date.\n\nNote: One limitation of using AlphaVantage’s free content is we cannot go back 15-20 years in every sector, as we might hope, as find easier when using FRED.\n\nUS equity data typically calculates logarithmic returns. To transform this data so it may be compared against US equity returns, or incorporated into Monte Carlo simulations, we calculate the logarithmic returns as follows:\n\n\nCode\nefa_df &lt;- efa_df |&gt;\n  transform(return = c(NA, diff(log(close))))\n\nhead(efa_df)\n\n\n          date close       return\n100 2024-07-16 81.25           NA\n99  2024-07-17 80.82 -0.005306362\n98  2024-07-18 80.02 -0.009947856\n97  2024-07-19 79.53 -0.006142294\n96  2024-07-22 80.37  0.010506663\n95  2024-07-23 79.92 -0.005614838\n\n\nWe can also use FRED to access the ICE BofA Emerging Markets Corporate Plus Index Total Return Index Value. This index measures the total returns of emerging market corporate bonds. We use this dataset as a proxy for international equity market performance because it captures returns in emerging markets, which, similar to equity investments, have hgiher risk and growth potential.\n\n\nCode\n# Define FRED API base URL\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# FRED API parameters for the selected international equity index\nfred_params_int &lt;- list(\n  series_id = \"BAMLEMCBPITRIV\",         # International Equity Market Index\n  api_key = fred_api_key,               # Your FRED API key\n  file_type = \"json\",                   # Response format\n  observation_start = \"2003-01-01\",     # Start date\n  observation_end = \"2023-12-31\"        # End date\n)\n\n# Fetch international equity market data\nresp &lt;- request(fred_base_url) |&gt;\n  req_url_query(!!!fred_params_int) |&gt;\n  req_perform()\n\n# Parse the JSON response into a data frame\nint_equity_data &lt;- resp_body_json(resp)\nint_equity_df &lt;- do.call(rbind, lapply(int_equity_data$observations, as.data.frame)) |&gt;\n  as.data.frame() |&gt;\n  transform(\n    date = as.Date(date),             # Convert 'date' column to Date format\n    value = as.numeric(value)        # Convert 'value' column to numeric\n  )\n\n# Drop unnecessary columns and rename\nint_equity_df &lt;- int_equity_df[, c(\"date\", \"value\")] |&gt;\n  rename(int_equity = value)          # Rename 'value' column to 'int_equity'\n\n# View the structure of the resulting data frame\nstr(int_equity_df)\n\n\n'data.frame':   5548 obs. of  2 variables:\n $ date      : Date, format: \"2003-01-01\" \"2003-01-02\" ...\n $ int_equity: num  NA 144 144 145 145 ...\n\n\nCode\n# Display the first few rows of the data frame\nhead(int_equity_df)\n\n\n        date int_equity\n1 2003-01-01         NA\n2 2003-01-02     144.36\n3 2003-01-03     144.43\n4 2003-01-06     144.53\n5 2003-01-07     145.01\n6 2003-01-08     145.50\n\n\nThe code below replaces missing values with NA\n\n\nCode\nint_equity_df$int_equity &lt;- as.numeric(ifelse(int_equity_df$int_equity == \".\", NA, int_equity_df$int_equity))\n\n\nThis code replaces NA with the last valid observation.\n\n\nCode\nlibrary(zoo)\nint_equity_df$int_equity &lt;- na.locf(int_equity_df$int_equity, na.rm = FALSE)\n\n\nAggregating to monthly data -\n\n\nCode\nint_equity_monthly &lt;- int_equity_df |&gt;\n  mutate(month = format(date, \"%Y-%m\")) |&gt;\n  group_by(month) |&gt;\n  summarize(int_equity = mean(int_equity, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  mutate(date = as.Date(paste0(month, \"-01\"), format = \"%Y-%m-%d\")) |&gt;\n  select(-month)  #dropping month - realized later on we did not need it so modifying this step\n\n#checking the structure of the modified dataset\nstr(int_equity_monthly)\n\n\ntibble [252 × 2] (S3: tbl_df/tbl/data.frame)\n $ int_equity: num [1:252] 146 148 149 151 157 ...\n $ date      : Date[1:252], format: \"2003-01-01\" \"2003-02-01\" ...\n\n\nCode\n#previewing the first few rows\nhead(int_equity_monthly)\n\n\n# A tibble: 6 × 2\n  int_equity date      \n       &lt;dbl&gt; &lt;date&gt;    \n1       146. 2003-01-01\n2       148. 2003-02-01\n3       149. 2003-03-01\n4       151. 2003-04-01\n5       157. 2003-05-01\n6       161. 2003-06-01\n\n\nBond market total returns - this financial metric provides insight into fixed-income investments. These are typically less volatile compared to equities, and serve as a method to stabilize a portfolio. Bonds offer a predictable income stream and can hedge against stock market downturns. Bonds serve as a safe and stable portion of a portfolio as one plans for retirement.\nWe access FRED’s 10-Year Treasury Constant Maturity Rate, for 2003-2023.\n\n\nCode\n# Define the base URL for FRED API\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# FRED API parameters for 10-Year Treasury Rate\nfred_params &lt;- list(\n  series_id = \"GS10\",               # 10-Year Treasury Constant Maturity Rate\n  api_key = fred_api_key,           # Your FRED API key\n  file_type = \"json\",               # Response format\n  observation_start = \"2003-01-01\", # Start date\n  observation_end = \"2023-12-31\"    # End date\n)\n\n# Send the request\nresp &lt;- request(fred_base_url) |&gt;\n  req_url_query(!!!fred_params) |&gt;\n  req_perform()\n\n# Parse the JSON response\nbond_data &lt;- resp_body_json(resp)\n\n# Extract and convert the observations into a data frame\nbond_df &lt;- do.call(rbind, lapply(bond_data$observations, as.data.frame)) |&gt;\n  as.data.frame() |&gt;\n  transform(\n    date = as.Date(date),            # Convert 'date' column to Date format\n    value = as.numeric(value)        # Convert 'value' column to numeric\n  )\n\n# Drop unnecessary columns\nbond_df &lt;- bond_df[, c(\"date\", \"value\")]\n\n# View the structure of the resulting data frame\nstr(bond_df)\n\n\n'data.frame':   252 obs. of  2 variables:\n $ date : Date, format: \"2003-01-01\" \"2003-02-01\" ...\n $ value: num  4.05 3.9 3.81 3.96 3.57 3.33 3.98 4.45 4.27 4.29 ...\n\n\nCode\n# Display the first few rows\nhead(bond_df)\n\n\n        date value\n1 2003-01-01  4.05\n2 2003-02-01  3.90\n3 2003-03-01  3.81\n4 2003-04-01  3.96\n5 2003-05-01  3.57\n6 2003-06-01  3.33\n\n\nWith the hope to later analyze returns, we can calculate monthly percentage changes.\n\n\nCode\nbond_df &lt;- bond_df |&gt; \n  transform(return = c(NA, diff(log(value))))\n\n# View the first few rows with returns\nhead(bond_df)\n\n\n        date value      return\n1 2003-01-01  4.05          NA\n2 2003-02-01  3.90 -0.03774033\n3 2003-03-01  3.81 -0.02334736\n4 2003-04-01  3.96  0.03861484\n5 2003-05-01  3.57 -0.10367843\n6 2003-06-01  3.33 -0.06959329\n\n\nShort-term debt returns - reflect the interest earned on safe, short-term fixed-income securities. One commonly used financial instrument here is the Treasury Bill. When planning for retirement, short-term debt can serve as a buffer for market volatility.\nBelow we access FRED’s 3-Month Treasury Bill Rate, with a resulting data frame, t_bill_df, which contains short-term debt rate data from 2003-2023.\n\n\nCode\n# Define the base URL for FRED API\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# FRED API parameters for 3-Month Treasury Bill Rate\nfred_params &lt;- list(\n  series_id = \"TB3MS\",               # 3-Month Treasury Bill Rate\n  api_key = fred_api_key,            # Your FRED API key\n  file_type = \"json\",                # Response format\n  observation_start = \"2003-01-01\",  # Start date\n  observation_end = \"2023-12-31\"     # End date\n)\n\n# Send the request\nresp &lt;- request(fred_base_url) |&gt;\n  req_url_query(!!!fred_params) |&gt;\n  req_perform()\n\n# Parse the JSON response\nt_bill_data &lt;- resp_body_json(resp)\n\n# Extract and convert the observations into a data frame\nt_bill_df &lt;- do.call(rbind, lapply(t_bill_data$observations, as.data.frame)) |&gt;\n  as.data.frame() |&gt;\n  transform(\n    date = as.Date(date),             # Convert 'date' column to Date format\n    value = as.numeric(value)         # Convert 'value' column to numeric\n  )\n\n# Drop unnecessary columns\nt_bill_df &lt;- t_bill_df[, c(\"date\", \"value\")]\n\n# View the structure of the resulting data frame\nstr(t_bill_df)\n\n\n'data.frame':   252 obs. of  2 variables:\n $ date : Date, format: \"2003-01-01\" \"2003-02-01\" ...\n $ value: num  1.17 1.17 1.13 1.13 1.07 0.92 0.9 0.95 0.94 0.92 ...\n\n\nCode\n# Display the first few rows\nhead(t_bill_df)\n\n\n        date value\n1 2003-01-01  1.17\n2 2003-02-01  1.17\n3 2003-03-01  1.13\n4 2003-04-01  1.13\n5 2003-05-01  1.07\n6 2003-06-01  0.92\n\n\n\nWe once again include the monthly logarithmic return rates to our data frame:\n\n\nCode\nt_bill_df &lt;- t_bill_df |&gt; \n  transform(return = c(NA, diff(log(value))))\n\n#view the first few rows with returns\nhead(t_bill_df)\n\n\n        date value      return\n1 2003-01-01  1.17          NA\n2 2003-02-01  1.17  0.00000000\n3 2003-03-01  1.13 -0.03478612\n4 2003-04-01  1.13  0.00000000\n5 2003-05-01  1.07 -0.05455898\n6 2003-06-01  0.92 -0.15104026\n\n\nBefore moving on to the next task, we filter all (applicable) data sets to reflect the time frame of 2007-2023, keeping in mind our wage growth data was only able to go back to mid-2006. This adjustment aligns our data for further analysis.\n\n\nCode\n#defining the date range\nstart_date &lt;- as.Date(\"2007-01-01\")\nend_date &lt;- as.Date(\"2023-12-31\")\n\n#adjusting wage growth dataframe\nwage_growth_df &lt;- subset(wage_growth_df, date &gt;= start_date & date &lt;= end_date)\n\n#adjusting CPI dataframe\ncpi_df &lt;- subset(cpi_df, date &gt;= start_date & date &lt;= end_date)\n\n#adjusting bond dataframe\nbond_df &lt;- subset(bond_df, date &gt;= start_date & date &lt;= end_date)\n\n#adjusting T-bill dataframe\nt_bill_df &lt;- subset(t_bill_df, date &gt;= start_date & date &lt;= end_date)\n\n#adjusting international equity dataframe\nint_equity_monthly &lt;- subset(\n    int_equity_monthly,\n    date &gt;= start_date & date &lt;= end_date\n)\n\n#verify each dataframe\nlist(\n  wage_growth = range(wage_growth_df$date),\n  cpi = range(cpi_df$date),\n  bond = range(bond_df$date),\n  t_bill = range(t_bill_df$date),\n  int_equity = range(int_equity_monthly$date)\n)\n\n\n$wage_growth\n[1] \"2007-01-01\" \"2023-12-01\"\n\n$cpi\n[1] \"2007-01-01\" \"2023-12-01\"\n\n$bond\n[1] \"2007-01-01\" \"2023-12-01\"\n\n$t_bill\n[1] \"2007-01-01\" \"2023-12-01\"\n\n$int_equity\n[1] \"2007-01-01\" \"2023-12-01\""
  },
  {
    "objectID": "mp04.html#task-4-initial-analysis",
    "href": "mp04.html#task-4-initial-analysis",
    "title": "mp04",
    "section": "",
    "text": "This step involves conducting exploratory data analysis to identify key properties of the data historical data acquired.\nThe code below calculates the long-run monthly averages and variances for each data series.\nThe table above summarizes the key statistical properties, the long-run monthly averages, and the variability of each series over the 2007-2023 time period.\nThe wage growth shows a mean value of 25.89 and variance of 13.74. This indicates a relatively stable upward trend over time.\nThe CPI exhibits the highest variance at 697.58, indicates there were significant fluctuations in inflation from 2007 to 2023. This impacts our retirement planning, emphasizing the importance of considering inflation-indexed returns (like in our two retirement plan options).\nNext, we generate the correlation matrix table through the code below. We first combine our data into a single data frame, aligned_data.\n\n\nCode\n # Renaming and preparing datasets for alignment\nwage_growth_prepared &lt;- wage_growth_df |&gt; rename(wage_growth = value)\ncpi_prepared &lt;- cpi_df |&gt; rename(cpi = value)\nbond_prepared &lt;- bond_df |&gt; rename(bond_return = return) |&gt; select(date, bond_return)\nt_bill_prepared &lt;- t_bill_df |&gt; rename(t_bill_return = return) |&gt; select(date, t_bill_return)\n\n# Ensure int_equity_monthly has the correct structure\nint_equity_prepared &lt;- int_equity_monthly \n\nprint(head(int_equity_monthly))\n\n\n# A tibble: 6 × 2\n  int_equity date      \n       &lt;dbl&gt; &lt;date&gt;    \n1       204. 2007-01-01\n2       206. 2007-02-01\n3       207. 2007-03-01\n4       208. 2007-04-01\n5       209. 2007-05-01\n6       207. 2007-06-01\n\n\nCode\n# Combine all datasets by date using a full join\naligned_data &lt;- list(\n  wage_growth_prepared,\n  cpi_prepared,\n  bond_prepared,\n  t_bill_prepared,\n  int_equity_prepared\n) |&gt; \n  reduce(full_join, by = \"date\")\n\n# Check structure and preview\nstr(aligned_data)\n\n\n'data.frame':   204 obs. of  6 variables:\n $ date         : Date, format: \"2007-01-01\" \"2007-02-01\" ...\n $ wage_growth  : num  20.6 20.7 20.7 20.8 20.8 ...\n $ cpi          : num  203 204 205 206 207 ...\n $ bond_return  : num  0.04293 -0.00844 -0.03449 0.02811 0.01271 ...\n $ t_bill_return: num  0.02645 0.00999 -0.01805 -0.01427 -0.02917 ...\n $ int_equity   : num  204 206 207 208 209 ...\n\n\nCode\nhead(aligned_data)\n\n\n        date wage_growth     cpi  bond_return t_bill_return int_equity\n1 2007-01-01       20.59 203.437  0.042925045   0.026451186   204.1070\n2 2007-02-01       20.68 204.226 -0.008438869   0.009990093   205.5720\n3 2007-03-01       20.73 205.288 -0.034486176  -0.018054653   207.4839\n4 2007-04-01       20.78 205.904  0.028109959  -0.014271394   208.4286\n5 2007-05-01       20.84 206.755  0.012712036  -0.029168735   209.3670\n6 2007-06-01       20.95 207.234  0.071095922  -0.025697345   207.2132\n\n\n\n\nCode\n# Recompute summary statistics for aligned data\nmean_stats &lt;- aligned_data |&gt;\n  summarize(\n    `Wage Growth` = mean(wage_growth, na.rm = TRUE),\n    `CPI` = mean(cpi, na.rm = TRUE),\n    `Bond Returns` = mean(bond_return, na.rm = TRUE),\n    `T-Bill Returns` = mean(t_bill_return, na.rm = TRUE),\n    `International Equity` = mean(int_equity, na.rm = TRUE)\n  ) |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"Data Metric\",\n    values_to = \"Mean Value\"\n  )\n\nvariance_stats &lt;- aligned_data |&gt;\n  summarize(\n    `Wage Growth` = var(wage_growth, na.rm = TRUE),\n    `CPI` = var(cpi, na.rm = TRUE),\n    `Bond Returns` = var(bond_return, na.rm = TRUE),\n    `T-Bill Returns` = var(t_bill_return, na.rm = TRUE),\n    `International Equity` = var(int_equity, na.rm = TRUE)\n  ) |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"Data Metric\",\n    values_to = \"Variance\"\n  )\n\n# Combine means and variances\nsummary_stats_aligned &lt;- mean_stats |&gt;\n  left_join(variance_stats, by = \"Data Metric\")\n\n# Display the summary statistics using the gt package\nlibrary(gt)\nsummary_stats_aligned |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Updated Summary Statistics for Aligned Data\",\n    subtitle = \"Averages and Variances from 2007 to 2023\"\n  ) |&gt;\n  fmt_number(\n    columns = vars(`Mean Value`, Variance),\n    decimals = 2\n  )\n\n\n\n\n\n\n\n\nUpdated Summary Statistics for Aligned Data\n\n\nAverages and Variances from 2007 to 2023\n\n\nData Metric\nMean Value\nVariance\n\n\n\n\nWage Growth\n25.89\n13.74\n\n\nCPI\n243.26\n697.58\n\n\nBond Returns\n0.00\n0.01\n\n\nT-Bill Returns\n0.00\n0.16\n\n\nInternational Equity\n337.20\n6,754.06\n\n\n\n\n\n\n\nNext, to analyze the relationships between the various economic indicators in our dataset, we compute a correlation matrix and visualize it using a heatmap. The correlation matrix tells us the strength and direction of linear relationship between each variable pair.\n\n\nCode\n#compute correlation matrix\ncor_matrix &lt;- cor(aligned_data[, -1], use = \"complete.obs\")\n\n#reshape correlation matrix into long format for ggplot\ncor_long &lt;- melt(cor_matrix)\n\n# creating heatmap \nggplot(cor_long, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile(color = \"white\") + # Add gridlines for better separation\n  scale_fill_gradient2(\n    low = \"blue\", high = \"red\", mid = \"white\",\n    midpoint = 0, limit = c(-1, 1), space = \"Lab\",\n    name = \"Correlation\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank()\n  ) +\n  ggtitle(\n    \"Correlation Matrix Heatmap\",\n    subtitle = \"Relationships Between Economic Indicators\"\n  )\n\n\n\n\n\n\n\n\n\nThe heatmap visualizes the correlations among the economic indicators. We see a strong positive correlation between wage growth and CIP, indicating that as wages increase, consumer prices tend to rise, which aligns with inflationary trends. Differently, we find a weak positive correlation between T-bill returns and other indicators like bond returns and international equities. This highlights the more stable and less market-sensitive nature of these economic indicators."
  }
]